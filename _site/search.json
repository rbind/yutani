[
  {
    "objectID": "post/winit-and-r/index.html",
    "href": "post/winit-and-r/index.html",
    "title": "How To Use Winit With R (Or How To Run Winit On A Non-Main Thread)",
    "section": "",
    "text": "The winit Rust crate is a cross-platform library about creating and managing windows. If you want to create some GUI with Rust, there are many options. Among these, winit is what you are most likely to rely on indirectly or directly.\nFor example, Bevy, the most dominant Rust game engine, uses winit. Tauri, which they say the next Electron, uses a forked-version of winit. It might be less common to use winit directly, but, when you want just window, not versatile GUI toolkits, it’s probably the case (e.g. Learn Wgpu).\nSo, if the urge is to pop up a window and destroy it, winit is the choice. After reading this post, you’ll probably get some sense to do it properly (but displaying useful things on the window will not be covered here)."
  },
  {
    "objectID": "post/winit-and-r/index.html#difference-from-a-standalone-gui",
    "href": "post/winit-and-r/index.html#difference-from-a-standalone-gui",
    "title": "How To Use Winit With R (Or How To Run Winit On A Non-Main Thread)",
    "section": "Difference from a standalone GUI",
    "text": "Difference from a standalone GUI\nCreating an R package and a standalone GUI app are different things. The main difficulty I want to write about today is, whereas a standalone app runs on the main thread, the main thread is for the R session in the case of an R package.\nFor example, the code below is a typical winit application (derived from the official document).\nApp is what actually handles window-related events (user’s click, keyboard input, etc). window_event() implements what to do when which event comes in. For example, this prints a message and stops the event_loop when WindowEvent::CloseRequested is passed.\nEventLoop is what catches such events from OS and window and forwards to App.\n#[derive(Default)]\nstruct App {\n    window: Option&lt;Window&gt;,\n}\n\nimpl ApplicationHandler for App {\n    ...\n\n    fn window_event(&mut self, event_loop: &ActiveEventLoop, id: WindowId, event: WindowEvent) {\n        match event {\n            WindowEvent::CloseRequested =&gt; {\n                println!(\"The close button was pressed; stopping\");\n                self.window = None; // window is automatically closed when dropped\n                event_loop.exit();\n            },\n            ...\n        }\n    }\n}\n\nfn main() {\n    let event_loop = EventLoop::new().unwrap();\n    let mut app = App::default();\n    event_loop.run_app(&mut app);\n}\nevent_loop catches events during run_app(). This blocks. So, if you call this function in your R session, your console is unusable unless you close the window.\nmain()\nYes, this is still useful in some cases. There are many nice R packages that pop up a Shiny window and return some useful value. But, what should we do if we want to use the window concurrently?"
  },
  {
    "objectID": "post/winit-and-r/index.html#stdthreadspwan",
    "href": "post/winit-and-r/index.html#stdthreadspwan",
    "title": "How To Use Winit With R (Or How To Run Winit On A Non-Main Thread)",
    "section": "std::thread::spwan()",
    "text": "std::thread::spwan()\nA naive idea is to run this in a new thread. In Rust, this can be easily done by std::thread::spwan(). The code would be like this:\nfn main() {\n    std::thread::spawn(|| {\n        let event_loop = EventLoop::new().unwrap();\n        let mut app = App::default();\n        event_loop.run_app(&mut app);\n    });\n}\nLooks fine? Actually, this doesn’t raise an error. But, actually, it’s just a panic is not propagated to the top. If we add two unwrap()s below, you’ll find an error message complaining that you ran it in a non-main thread.\nfn main() {\n    std::thread::spawn(|| {\n        ...\n        event_loop.run_app(&mut app).unwrap();\n    })\n    .join()\n    .unwrap();\n}\n\nInitializing the event loop outside of the main thread is a significant cross-platform compatibility hazard. If you absolutely need to create an EventLoop on a different thread, you can use the EventLoopBuilderExtX11::any_thread or EventLoopBuilderExtWayland::any_thread functions.\n\nYou might wonder, if the problem was “Initializing the event loop outside of the main thread,” we can initialize it outside of std::thread::spawn().\nfn main() {\n    let event_loop = EventLoop::new().unwrap();\n    std::thread::spawn(move || {\n        let mut app = App::default();\n        event_loop.run_app(&mut app).unwrap();\n    })\n    .join()\n    .unwrap();\n}\nBut, this doesn’t work either. Since EventLoop is not Send, it cannot be sent to a different thread. You’ll get this compilation error:\nerror[E0277]: `*mut ()` cannot be sent between threads safely\n   --&gt; src/main.rs:49:24\n    |\n49  |       std::thread::spawn(move || {\n    |       ------------------ ^------\n    |       |                  |\n    |  _____|__________________within this `{closure@src/main.rs:49:24: 49:31}`\n    | |     |\n    | |     required by a bound introduced by this call\n50  | |         let mut app = App::default();\n51  | |         event_loop.run_app(&mut app).unwrap();\n52  | |     })\n    | |_____^ `*mut ()` cannot be sent between threads safely\n    |\n    = help: within `{closure@src/main.rs:49:24: 49:31}`, the trait `Send` is not implemented for `*mut ()`, which is required by `{closure@src/main.rs:49:24: 49:31}: Send`\n...\nSo, are there no way to use a thread?"
  },
  {
    "objectID": "post/winit-and-r/index.html#with_any_thread",
    "href": "post/winit-and-r/index.html#with_any_thread",
    "title": "How To Use Winit With R (Or How To Run Winit On A Non-Main Thread)",
    "section": "with_any_thread()",
    "text": "with_any_thread()\nLet’s look at the first panic message again. It says there are some functions.\n\nyou can use the EventLoopBuilderExtX11::any_thread or EventLoopBuilderExtWayland::any_thread functions.\n\nany_thread() is a typo of with_any_thread(). What is this? The document says:\n\nfn with_any_thread(&mut self, any_thread: bool) -&gt; &mut Self\nWhether to allow the event loop to be created off of the main thread.\n\nOh, isn’t this what we wanted?? Yes, this allows us to run the event loop in a non-main thread. you can use this to write such a main() that sleeps 10 seconds as well as running an winit app in a thread.\nuse winit::platform::wayland::EventLoopBuilderExtWayland;\n\nfn main() {\n    std::thread::spawn(|| {\n        let event_loop = EventLoop::builder().with_any_thread(true).build().unwrap();\n        let mut app = App::default();\n        event_loop.run_app(&mut app).unwrap();\n    });\n\n    // sleep instead of waiting for the thread to finish by join().unwrap()\n    std::thread::sleep(std::time::Duration::from_secs(10));\n}\nI use winit::platform::wayland::EventLoopBuilderExtWayland trait because I ran this on my Linux laptop now. You need to use a proper one corresponding to your platform. But, the problem is…\nIf you are on macOS, you are lucky because you probably noticed it faster than those who are on Linux or Windows. Yes, the problem is with_any_thread() is unavailable to macOS!.\nI couldn’t find a reliable reference, but it seems this limitation is made by macOS itself, not winit. So, there’s no hope this will be fixed on winit’s side.\nmultithreading - Why does MacOS/iOS *force* the main thread to be the UI thread, and are there any workarounds? - Stack Overflow\nAnyway, it’s a good news that at least Linux and Windows work fine in this way.\nNext, before thinking about macOS, let’s upgrade the code a bit."
  },
  {
    "objectID": "post/winit-and-r/index.html#eventloopproxy",
    "href": "post/winit-and-r/index.html#eventloopproxy",
    "title": "How To Use Winit With R (Or How To Run Winit On A Non-Main Thread)",
    "section": "EventLoopProxy",
    "text": "EventLoopProxy\nWhile the spawned thread can serve an winit application without problem, it’s a closed world. R cannot communicate with the application. So, we need some channel to send a message to the application from outside of the thread.\nWinit provides EventLoopProxy for such a purpose. Unlike EventLoop, EventLoopProxy is a Send and Sync, so this can be passed between threads. Via this proxy, we can send custom messages to the event loop.\nFirst, modify event_loop to handle custom messages; define an enum and create the event loop with with_user_event().\nenum MyEvent {\n    CloseWindow,\n    ResizeWindow,\n    ...\n}\nlet event_loop = EventLoop::&lt;MyEvent&gt;::with_user_event()\n    .with_any_thread(true)\n    .build()\n    .unwrap()\nAlso, add user_event() implementation to ApplicationHandler.\nimpl ApplicationHandler for App {\n    ...\n    fn user_event(&mut self, event_loop: &ActiveEventLoop, event: MyEvent) {\n        match event {\n            MyEvent::CloseWindow =&gt; {\n                println!(\"Closing window from R session\");\n                self.window = None;\n                event_loop.exit();\n            }\n            ...\n        }\nNow, App is ready to accept messages from R!\nNext, create a proxy so that we can send messages via it. One tricky thing is that the proxy needs to be created in the thread where the EventLoop is created, i.e., the spawned thread. So, we need to pass a channel to the thread to pull the proxy from it (I found this trick in this SO answer).\nThe code would be like this:\nlet (ch_send, ch_recv) = std::sync::mpsc::channel();\n\nstd::thread::spawn(move || {\n    let event_loop = EventLoop::&lt;MyEvent&gt;::with_user_event()\n        .with_any_thread(true)\n        .build()\n        .unwrap();\n\n    // create and pass a proxy to the outside\n    let proxy = event_loop.create_proxy();\n    ch_send.send(proxy).unwrap();\n\n    let mut app = App::default();\n    event_loop.run_app(&mut app).unwrap();\n});\n\n// get the proxy via channel\nlet proxy = ch_recv.recv().unwrap();\n\n// you can send events by calling this from R!!!\nproxy.send(MyEvent::CloseWindow);\nDone!\n(In the real use case, we want a channel to the opposite direction as well, but let’s omit it here for simplicty and go ahead. You can check my actual implementation (code).)"
  },
  {
    "objectID": "post/winit-and-r/index.html#fork",
    "href": "post/winit-and-r/index.html#fork",
    "title": "How To Use Winit With R (Or How To Run Winit On A Non-Main Thread)",
    "section": "Fork?",
    "text": "Fork?\nLet’s think about macOS.\nIf a thread doesn’t work, can we fork the process? Forking can be done easier on R than on Rust. On an R session, we can simply call parallel::mcparallel().\nYes, this probably works. The forked process serves a window without interrupting the R session. But, since it’s a different process, it doesn’t automatically have a communication method with the original process; EventLoopProxy works only on the same process.\nSo, as this anyway requires me to implement some IPC things, I decided to run a winit app server as a separate process."
  },
  {
    "objectID": "post/winit-and-r/index.html#server",
    "href": "post/winit-and-r/index.html#server",
    "title": "How To Use Winit With R (Or How To Run Winit On A Non-Main Thread)",
    "section": "Server",
    "text": "Server\nThis time, since this is a dedicated process for winit, we can just let event_loop.run_app() occupy the main thread.\nAccordingly, the receiver of incoming messages needs to run on a spawned thread. It just forwards the message to the event loop via proxy.\nFor connection, I use ipc-channel crate in this example, but there ara variety of choices (I also tried tonic).\nfn main() {\n    let event_loop = EventLoop::&lt;MyEvent&gt;::with_user_event()\n        .with_any_thread(true)\n        .build()\n        .unwrap();\n\n    let proxy = event_loop.create_proxy();\n\n    let (rx_server, rx_server_name) = IpcOneShotServer::&lt;MyEvent&gt;::new().unwrap();\n\n    // outputs the server name (e.g. socket file) so that a client can connect\n    println!(\"{rx_server_name}\");\n\n    // Wait for the first message (and discard it)\n    let (rx, _event) = rx_server.accept().unwrap();\n\n    std::thread::spawn(move || loop {\n        let event = rx.recv().unwrap();\n        proxy.send_event(event).unwrap();\n    });\n\n    let mut app = App::default();\n    event_loop.run_app(&mut app);\n}\nOn the client side, you can connect to the server by using the server name.\nlet tx: IpcSender&lt;MyEvent&gt; = IpcSender::connect(tx_server_name).unwrap();\nNote that, ipc-channel sends an object by serializing with serde. So, you need to derive Serialize and Deserialize on it. ipc-channel can also send and receive bytes, so if you are not satisfied with serde, you can write your own serialization (or of course use a different crate).\nuse serde::{Deserialize, Serialize};\n\n#[derive(Serialize, Deserialize, Debug)]\nenum MyEvent {\n    CloseWindow,\n    ResizeWindow,\n    ...\n}"
  },
  {
    "objectID": "post/winit-and-r/index.html#caveats",
    "href": "post/winit-and-r/index.html#caveats",
    "title": "How To Use Winit With R (Or How To Run Winit On A Non-Main Thread)",
    "section": "Caveats",
    "text": "Caveats\nConfession: I don’t have macOS, so I’m not sure if this specific implementation works on macOS. However, I believe the idea should be valid. So, please let me know if this doesn’t work for you!\nOne more concern is performance. IPC is probably slow compared to spawned because different processes cannot share memories without using shared memory explicitly. Regarding my use case, this will be a problem to display a large raster image. They say XPC is better in performance, so it might be worth investigating."
  },
  {
    "objectID": "post/winit-and-r/index.html#an-example-r-package",
    "href": "post/winit-and-r/index.html#an-example-r-package",
    "title": "How To Use Winit With R (Or How To Run Winit On A Non-Main Thread)",
    "section": "An example R package",
    "text": "An example R package\nI created an R package to demonstrate the idea I discussed here. Unfortunately, the implementation got a bit complicated due to macOS support (I don’t know why, but it doesn’t compile on macOS when winit is used within an R package…), but I hope the actual code would help you to figure out what I couldn’t explain well here. Feedback is welcome!\nhttps://github.com/yutannihilation/winitRPackage\nThis package can be installed from R-universe, so you can try this without Rust installed.\ninstall.packages(\"winitRPackage\",\n  repos = c('https://yutannihilation.r-universe.dev', 'https://cloud.r-project.org')\n)\n\nUsages\nFirst, please run download_server() to download the server binary. This will be used by ExternalWindowController.\nlibrary(winitRPackage)\n\ndownload_server()\n\n\nUse an external process\nx &lt;- ExternalWindowController$new()\n\n# create a new window titled \"foo\"\nx$open_window(\"foo\")\n\n# get the window size\nx$get_window_size()\n#&gt; [1] 800 600\n\n# close the window\nx$close_window()\n\n\nUse a spawned process\n(As described above, this doesn’t work on macOS)\nx &lt;- SpawnedWindowController$new()\n\n# create a new window titled \"foo\"\nx$open_window(\"foo\")\n\n# get the window size\nx$get_window_size()\n#&gt; [1] 800 600\n\n# close the window\nx$close_window()"
  },
  {
    "objectID": "post/rust-cache-and-r/index.html",
    "href": "post/rust-cache-and-r/index.html",
    "title": "Use rust-cache GitHub Actions For R Package",
    "section": "",
    "text": "This is a quick note about how to use the rust-cache action on a repository of an R package."
  },
  {
    "objectID": "post/rust-cache-and-r/index.html#caveats",
    "href": "post/rust-cache-and-r/index.html#caveats",
    "title": "Use rust-cache GitHub Actions For R Package",
    "section": "Caveats",
    "text": "Caveats\nWhile this works, I see some crates are always recompiled. It seems this happens on Windows more frequently. I tried to investigate, but couldn’t find the root cause… Probably some [rerun-if-changed] settings matters? If you know the answer, I’d really appreciate it.\nAnyway, as this problem exists, please use this technique at your own risk. In some cases, using rust-cache might increase the CI time.\nmacOS:\n\nWindows:"
  },
  {
    "objectID": "post/rust-1.70-and-build-failure-on-windows/index.html",
    "href": "post/rust-1.70-and-build-failure-on-windows/index.html",
    "title": "Rust 1.70 And Build Failures On Windows",
    "section": "",
    "text": "This is a quick notice for developers of R packages using Rust. I posted the same thing on the R-devel-package mailing list, so if you already read it, you don’t need to read this post.\nRust 1.70 was released on June 1st, 2023 1. If you use Rust in your R package, you’ll probably start to see an error and a warning on the final linking step of Windows builds."
  },
  {
    "objectID": "post/rust-1.70-and-build-failure-on-windows/index.html#footnotes",
    "href": "post/rust-1.70-and-build-failure-on-windows/index.html#footnotes",
    "title": "Rust 1.70 And Build Failures On Windows",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://blog.rust-lang.org/2023/06/01/Rust-1.70.0.html↩︎\nhttps://en.wikipedia.org/wiki/Microsoft_Windows_library_files#NTDLL.DLL↩︎\nhttps://github.com/rust-lang/rust/issues/112368#issuecomment-1581917714↩︎\nhttps://github.com/llvm/llvm-project/commit/c5b3de6745c37dd991430b9b88ff97c35b6fc455↩︎"
  },
  {
    "objectID": "post/quote-while-the-promise-is-hot/index.html",
    "href": "post/quote-while-the-promise-is-hot/index.html",
    "title": "Quote While the Promise Is Hot!",
    "section": "",
    "text": "Suppose we want to quote x when x is not NULL. The naive implementation would be like below. Here, y is for comparison. Do you understand why x and y are quoted differently?\nquote_x_and_y &lt;- function(x, y) {\n  if (is.null(x)) {\n    stop(\"x is NULL!\", call. = FALSE)\n  }\n  \n  x &lt;- rlang::enquo(x)\n  y &lt;- rlang::enquo(y)\n  \n  list(x, y)\n}\n\nx &lt;- y &lt;- 1\n\nquote_x_and_y(x, y)\n#&gt; [[1]]\n#&gt; &lt;quosure&gt;\n#&gt;   expr: ^1\n#&gt;   env:  empty\n#&gt; \n#&gt; [[2]]\n#&gt; &lt;quosure&gt;\n#&gt;   expr: ^y\n#&gt;   env:  global\nThis is because x is evaluated when is.null() is called before quoting, whereas y is intact. Lionel Henry, the tidyeval super hero, answered my qustion on RStudio Community:\n\nA forced promise can no longer be captured correctly because it no longer carries an environment.\n\nThis means we must not touch arguments before quoting. Instead, quote first and check the expression inside quosure by rlang::quo_is_*().\nquote_x_and_y2 &lt;- function(x, y) {\n  x &lt;- rlang::enquo(x)\n  y &lt;- rlang::enquo(y)\n  \n  if (rlang::quo_is_null(x)) {\n    stop(\"x is NULL!\", call. = FALSE)\n  }\n  \n  list(x, y)\n}\n\nquote_x_and_y2(x, y)\n#&gt; [[1]]\n#&gt; &lt;quosure&gt;\n#&gt;   expr: ^x\n#&gt;   env:  global\n#&gt; \n#&gt; [[2]]\n#&gt; &lt;quosure&gt;\n#&gt;   expr: ^y\n#&gt;   env:  global\nFor more complex checking, we may need to extract the expression from the quosure by rlang::quo_get_expr().\nquote_x_and_y_wont_stop &lt;- function(x, y) {\n  x &lt;- rlang::enquo(x)\n  y &lt;- rlang::enquo(y)\n\n  x_expr &lt;- rlang::quo_get_expr(x)  \n  if (rlang::call_name(x) %in% \"stop\") {\n    message(\"Nothing can stop me!\\n\")\n  }\n  \n  list(x, y)\n}\n\nquote_x_and_y_wont_stop(stop(\"foo\"), \"bar\")\n#&gt; Nothing can stop me!\n#&gt; [[1]]\n#&gt; &lt;quosure&gt;\n#&gt;   expr: ^stop(\"foo\")\n#&gt;   env:  global\n#&gt; \n#&gt; [[2]]\n#&gt; &lt;quosure&gt;\n#&gt;   expr: ^\"bar\"\n#&gt;   env:  empty\nAnyway, keep in mind to use enquo() (or ensym()) at the very beginning of the function. Quote while the promise is hot."
  },
  {
    "objectID": "post/how-to-convert-a-human-to-waves-by-magick-package/index.html",
    "href": "post/how-to-convert-a-human-to-waves-by-magick-package/index.html",
    "title": "How To Convert A Human To Waves By Magick Package",
    "section": "",
    "text": "I saw this tweet about Mathematica last year, which naturally urged me to write the R version of this code.\nAt that time, I faild because I didn’t know how to zoom images. But, now I know magick package. Let’s try again…"
  },
  {
    "objectID": "post/how-to-convert-a-human-to-waves-by-magick-package/index.html#zoom-images-by-magick",
    "href": "post/how-to-convert-a-human-to-waves-by-magick-package/index.html#zoom-images-by-magick",
    "title": "How To Convert A Human To Waves By Magick Package",
    "section": "Zoom images by magick",
    "text": "Zoom images by magick\nWe can enlarge a image by either image_resize(), image_scale(), or image_sample(). I don’t know about the details of the differences, but it seems image_resize() does better for my purpose.\n\nlibrary(magick)\n\nLinking to ImageMagick 6.9.12.3\nEnabled features: cairo, freetype, fftw, ghostscript, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fontconfig, x11\n\nrose &lt;- image_convert(image_read(\"rose:\"), \"png\")\n\nroses &lt;- c(\n    image_resize(rose, \"400x\"),\n    image_scale(rose, \"400x\"),\n    image_sample(rose, \"400x\")\n)\n\nimage_append(roses, stack = TRUE)\n\n\n\n\n\n\n\n\nBut, zooming is not just about resizing; I want to focus on the center of the image as well. To do this, we can use image_crop(). But, it’s our job to calculate the perper offset to centering the image (IIUC, there’s no equivalent of -gravity center in magick package, right??).\nSuppose we want to zoom by 200%. First of all, extract the original width and height.\n\ninfo &lt;- image_info(rose)\norig_width &lt;- info$width\norig_height &lt;- info$height\n\nThen, let’s calculate the offset; to center the image, the offset should be half of the diffrence between the original size and the size you want.\n\nwidth &lt;- as.integer(orig_width * 2)\nheight &lt;- as.integer(orig_height * 2)\n\noffset_x &lt;- as.integer((width - orig_width) / 2)\noffset_y &lt;- as.integer((height - orig_height) / 2)\n\nNow we have enough information to crop the image. To provide these to Imagemagick, we need to learn a bit about Geometry syntax. It’s as simple as:\n&lt;width&gt;x&lt;height&gt;{+-}&lt;xoffset&gt;{+-}&lt;yoffset&gt;\nWe can construct the geometries by sprintf() (use %+d instead of %d when we need explicit plus sign) as follows:\n\n(g_resize &lt;- sprintf(\"%dx%d\", width, height))\n\n[1] \"140x92\"\n\n(g_crop &lt;- sprintf(\"%dx%d%+d%+d\", orig_width, orig_height, offset_x, offset_y))\n\n[1] \"70x46+35+23\"\n\n\nNow we can zoom\n\nrose_zoomed &lt;- rose %&gt;%\n  image_resize(g_resize) %&gt;% \n  image_crop(g_crop)\n\nimage_append(c(rose, rose_zoomed), stack = TRUE)"
  },
  {
    "objectID": "post/how-to-convert-a-human-to-waves-by-magick-package/index.html#zoom-animatedly",
    "href": "post/how-to-convert-a-human-to-waves-by-magick-package/index.html#zoom-animatedly",
    "title": "How To Convert A Human To Waves By Magick Package",
    "section": "Zoom animatedly",
    "text": "Zoom animatedly\nNow that we know how to zoom, we can create a function to draw rose at the specified zoom level.\n\nzoom_rose &lt;- function(zoom = 1) {\n  width &lt;- as.integer(orig_width * zoom)\n  height &lt;- as.integer(orig_height * zoom)\n  \n  offset_x &lt;- as.integer((width - orig_width) / 2)\n  offset_y &lt;- as.integer((height - orig_height) / 2)\n  \n  g_resize &lt;- sprintf(\"%dx%d\", width, height)\n  g_crop &lt;- sprintf(\"%dx%d%+d%+d\", orig_width, orig_height, offset_x, offset_y)\n\n  rose %&gt;%\n    image_resize(g_resize) %&gt;% \n    image_crop(g_crop)\n}\n\nzoom_rose(1)\n\n\n\n\n\n\n\nzoom_rose(2)\n\n\n\n\n\n\n\n\nThe function can be applied to the vector of zoom levels by lapply(). Note that, to make the zoom speed looks constant, we need to power the steps.\n\nsteps &lt;- 100\nzooms &lt;- 1 + 9 * (0:steps / steps)^2\nimgs &lt;- lapply(zooms, zoom_rose)\n\nThe list of images can be combined by image_join() and then can be converted to an animation by image_animate()\n\nimgs %&gt;%\n  image_join() %&gt;%\n  image_animate(fps = 50)"
  },
  {
    "objectID": "post/how-to-convert-a-human-to-waves-by-magick-package/index.html#result",
    "href": "post/how-to-convert-a-human-to-waves-by-magick-package/index.html#result",
    "title": "How To Convert A Human To Waves By Magick Package",
    "section": "Result",
    "text": "Result\nAparently, there are a lot of things to explain (expecially about involute of a circle), but it would be a bit too long… Let’s jump to the final version of my code and the result :P\n\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(dplyr, warn.conflicts = FALSE)\n\n# the speed of involute increases uniformly; in order to make the lengths between\n# steps equal, we need to calculate the square root\nresolution_of_involute &lt;- 10000L\nt &lt;- sqrt(seq(0, resolution_of_involute) / resolution_of_involute)\n\ncoil_turns &lt;- 17L\nmax_r &lt;- coil_turns * 2 * pi\n\n# waviness of the coil\nwave_frequency &lt;- 100\nwave_height_base &lt;- pi\n\n# download and read the image\nimg_file &lt;- tempfile(fileext = \".png\")\ndownload.file(\"https://hoxo-m.com/img/team/makiyama.png\", destfile = img_file, mode = \"wb\")\nimg_orig &lt;- image_read(img_file)\n\n# convert to grayscale\nimg_bw &lt;- img_orig %&gt;% \n  image_convert(type = \"grayscale\") %&gt;%\n  image_modulate(brightness = 160) %&gt;% \n  image_contrast(10)\n\n# the width and height of the output\nw &lt;- 320\nh &lt;- 320\n\n# the width and height of the original image\ninfo &lt;- image_info(img_bw)\nw_orig &lt;- info$width\nh_orig &lt;- info$height\n\n# the width and height of the zoomed image\nscale &lt;- 30L\nw_big &lt;- w_orig * scale\nh_big &lt;- h_orig * scale\n\n# zoom image\nimg_bw_big &lt;- image_resize(img_bw, sprintf(\"%dx%d\", w_big, h_big))\n\n# place the small image on the center of the big image\nimg &lt;- image_composite(img_bw_big, img_bw,\n                       offset = sprintf(\"%+d%+d\",\n                                        as.integer((w_big - w_orig) / 2),\n                                        as.integer((h_big - h_orig) / 2)))\n\ndraw_hoxom &lt;- function(rotation = 0, zoom = 1) {\n  # unwavy involute\n  d &lt;- tibble(\n    radius = 2 * pi * t * coil_turns,\n    phi = radius - rotation,\n    .x = cos(phi) + radius * sin(phi),\n    .y = sin(phi) - radius * cos(phi)\n  )\n\n  # crop and resize the image at the specified zoom level\n  g &lt;- sprintf(\"%dx%d%+d%+d\",\n               as.integer(w_big / zoom),\n               as.integer(h_big / zoom),\n               as.integer((w_big - w_big / zoom) / 2),\n               as.integer((h_big - h_big / zoom) / 2))\n  \n  blackness &lt;- img %&gt;%\n    image_crop(g) %&gt;%\n    image_resize(sprintf(\"%dx%d\", w, h)) %&gt;%\n    image_data(\"gray\")\n\n  # calculate which pixel each point falls in\n  x_idx &lt;- as.integer(scales::rescale(d$.x, from = c(-max_r, max_r), to = c(1L, dim(blackness)[2])))\n  y_idx &lt;- as.integer(scales::rescale(d$.y, from = c(-max_r, max_r), to = c(dim(blackness)[3], 1L)))\n  \n  # determine the wave height based on the blackness\n  wave_height &lt;- (255 - as.numeric(blackness[cbind(1, x_idx, y_idx)])) / 256 * wave_height_base\n\n  # wavy involute  \n  d_wavy &lt;- d %&gt;% \n    mutate(\n      x = .x + wave_height * sin(phi * wave_frequency) * sin(phi),\n      y = .y - wave_height * sin(phi * wave_frequency) * cos(phi)\n    )\n  \n  p &lt;- ggplot(d_wavy) +\n    geom_path(aes(x, y)) +\n    theme_minimal() +\n    coord_equal(\n      # 0.85 is for zoom\n      xlim = c(-max_r, max_r) * 0.85,\n      ylim = c(-max_r, max_r) * 0.85\n    ) +\n    theme_void()\n  \n  print(p)\n}\n\n\nimgs &lt;- image_graph(w, h, res = 72)\n\nsteps &lt;- 100\nfor (i in seq_len(steps)) {\n  draw_hoxom(2 * pi * i / steps, 1 + (scale - 1) / steps^2 * i^2)\n}\n\ndev.off()\n\npng \n  2 \n\nimage_animate(imgs, fps = 50)\n\n\n\n\n\n\n\n\nI’m grad I’ve finally proven that I can live without Mathematica!"
  },
  {
    "objectID": "post/geom-sf-text-and-geom-sf-label-are-coming/index.html",
    "href": "post/geom-sf-text-and-geom-sf-label-are-coming/index.html",
    "title": "geom_sf_text() and geom_sf_label() Are Coming!",
    "section": "",
    "text": "ggplot2 v3.1.0 will be released soon (hopefully), so let me do a spoiler about a small feature I implemented, geom_sf_label() and geom_sf_text()."
  },
  {
    "objectID": "post/geom-sf-text-and-geom-sf-label-are-coming/index.html#how-can-we-add-labeltext-with-geom_sf",
    "href": "post/geom-sf-text-and-geom-sf-label-are-coming/index.html#how-can-we-add-labeltext-with-geom_sf",
    "title": "geom_sf_text() and geom_sf_label() Are Coming!",
    "section": "How can we add label/text with geom_sf()?",
    "text": "How can we add label/text with geom_sf()?\ngeom_sf() is one of the most exciting features introduced in ggplot2 v3.0.0. It magically allows us to plot sf objects according to their geometries’ shapes (polygons, lines and points).\nBut, for plotting them as some other shapes than the original ones, we cannot rely on geom_sf() so it needs a bit of data transformation beforehand. Suppose we want to add text on each geometry, we need to\n\ncalculate the proper point to add text/labels per geometry by some function like sf::st_centroid() and sf::st_point_on_surface(),\nretrieve the coordinates from the calculated points by sf::st_coordinates(), and\nuse geom_text() or geom_label() with the coordinates\n\nThe code for this would be like below:\n\nlibrary(ggplot2)\n\nnc &lt;- sf::st_read(system.file(\"shape/nc.shp\", package = \"sf\"), quiet = TRUE)\n\n# use only first three elements\nnc3 &lt;- nc[1:3, ]\n\n# choose a point on the surface of each geometry\nnc3_points &lt;- sf::st_point_on_surface(nc3)\n\nWarning in st_point_on_surface.sf(nc3): st_point_on_surface assumes attributes\nare constant over geometries of x\n\n\nWarning in st_point_on_surface.sfc(st_geometry(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n# retrieve the coordinates\nnc3_coords &lt;- as.data.frame(sf::st_coordinates(nc3_points))\nnc3_coords$NAME &lt;- nc3$NAME\n\nnc3_coords\n\n          X        Y      NAME\n1 -81.49496 36.42112      Ashe\n2 -81.13241 36.47396 Alleghany\n3 -80.69280 36.38828     Surry\n\nggplot() +\n  geom_sf(data = nc3, aes(fill = AREA)) +\n  geom_text(data = nc3_coords, aes(X, Y, label = NAME), colour = \"white\")\n\n\n\n\n\n\n\n\nPhew, this seems not so difficult, but I feel the code is a bit too long…"
  },
  {
    "objectID": "post/geom-sf-text-and-geom-sf-label-are-coming/index.html#geom_sf_label-and-geom_sf_text",
    "href": "post/geom-sf-text-and-geom-sf-label-are-coming/index.html#geom_sf_label-and-geom_sf_text",
    "title": "geom_sf_text() and geom_sf_label() Are Coming!",
    "section": "geom_sf_label() and geom_sf_text()",
    "text": "geom_sf_label() and geom_sf_text()\nFor this purpose, upcoming ggplot2 v3.1.0 provides two new geoms, geom_sf_text() and geom_sf_label(). The code equivalent to above can be written as:\n\n# texts and labels\np &lt;- ggplot(nc3) +\n  geom_sf(aes(fill = AREA))\n\np + geom_sf_text(aes(label = NAME), colour = \"white\")\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\n\n\nFor labels, use geom_sf_label():\n\np + geom_sf_label(aes(label = NAME))\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data"
  },
  {
    "objectID": "post/geom-sf-text-and-geom-sf-label-are-coming/index.html#protip-stat_sf_coordinates",
    "href": "post/geom-sf-text-and-geom-sf-label-are-coming/index.html#protip-stat_sf_coordinates",
    "title": "geom_sf_text() and geom_sf_label() Are Coming!",
    "section": "Protip: stat_sf_coordinates()",
    "text": "Protip: stat_sf_coordinates()\nUnder the hood, a Stat called stat_sf_coordinates() does the necessary calculations. If you are an expert of ggplot2, please play with this by combining with other Geoms. As an example, here’s a preliminary version of geom_sf_label_repel() (which I want to implement next…):\n\nggplot(nc) +\n  geom_sf() +\n  ggrepel::geom_label_repel(\n    data = nc[c(1:3, 10:14), ],\n    aes(label = NAME, geometry = geometry),\n    stat = \"sf_coordinates\",\n    min.segment.length = 0,\n    colour = \"magenta\",\n    segment.colour = \"magenta\"\n  )\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\n\n\nFor other cool NEWS of ggplot2 v3.1.0, please read the NEWS.md :)"
  },
  {
    "objectID": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html",
    "href": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html",
    "title": "Enhancing gather() and spread() by Using “Bundled” data.frames",
    "section": "",
    "text": "Last month, I tried to explain gather() and spread() by gt package (https://yutani.rbind.io/post/gather-and-spread-explained-by-gt/). But, after I implemented experimental multi-gather() and multi-spread(), I realized that I need a bit different way of explanation… So, please forget the post, and read this with fresh eyes!"
  },
  {
    "objectID": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#wait-what-is-multi-gather-and-multi-spread",
    "href": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#wait-what-is-multi-gather-and-multi-spread",
    "title": "Enhancing gather() and spread() by Using “Bundled” data.frames",
    "section": "Wait, what is multi-gather() and multi-spread()??",
    "text": "Wait, what is multi-gather() and multi-spread()??\nIn short, the current gather() and spread() have a limitation; they can gather into or spread from only one column at once. So, if we want to handle multiple columns, we need to coerce them to one column before actually gathering or spreading.\nThis is especially problematic when the columns have different types. For example, date column is unexpectedly converted to integers with the following code:\n\nlibrary(tibble)\nlibrary(tidyr)\n\n# a bit different version of https://github.com/tidyverse/tidyr/issues/149#issue-124411755\nd &lt;- tribble(\n  ~place, ~censor,                  ~date, ~status,\n    \"g1\",    \"c1\",  as.Date(\"2019-02-01\"),   \"ok\",\n    \"g1\",    \"c2\",  as.Date(\"2019-02-01\"),  \"bad\",\n    \"g1\",    \"c3\",  as.Date(\"2019-02-01\"),   \"ok\",\n    \"g2\",    \"c1\",  as.Date(\"2019-02-01\"),  \"bad\",\n    \"g2\",    \"c2\",  as.Date(\"2019-02-02\"),   \"ok\"\n)\n\nd %&gt;%\n  gather(key = element, value = value, date, status) %&gt;%\n  unite(thing, place, element, remove = TRUE) %&gt;%\n  spread(thing, value, convert = TRUE)\n\nWarning: attributes are not identical across measure variables;\nthey will be dropped\n\n\n# A tibble: 3 × 5\n  censor g1_date g1_status g2_date g2_status\n  &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;       &lt;int&gt; &lt;chr&gt;    \n1 c1       17928 ok          17928 bad      \n2 c2       17928 bad         17929 ok       \n3 c3       17928 ok             NA &lt;NA&gt;     \n\n\nHere, we need better spread() and gather(), which can handle multiple columns. For more discussions, you can read the following issues:\n\nhttps://github.com/tidyverse/tidyr/issues/149\nhttps://github.com/tidyverse/tidyr/issues/150\n\nIn this post, I’m trying to explain an approach to solve this by using “bundled” data.frames, which is originally proposed by Kirill Müller."
  },
  {
    "objectID": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#bundled-data.frames",
    "href": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#bundled-data.frames",
    "title": "Enhancing gather() and spread() by Using “Bundled” data.frames",
    "section": "“Bundled” data.frames",
    "text": "“Bundled” data.frames\nFor convenience, I use a new term “bundle” for separating some of the columns of a data.frame to another data.frame, and assigning the new data.frame to a column, and “unbundle” for the opposite operation.\nFor example, “bundling X, Y, and Z” means converting this\n\n\n\n\n\n\n\n\nid\nX\nY\nZ\n\n\n\n\n1\n0.1\na\nTRUE\n\n\n2\n0.2\nb\nFALSE\n\n\n3\n0.3\nc\nTRUE\n\n\n\n\n\n\n\nto something like this:\n\n\n\n\n\n\n\n\nid\nfoo\n\n\nX\nY\nZ\n\n\n\n\n1\n0.1\na\nTRUE\n\n\n2\n0.2\nb\nFALSE\n\n\n3\n0.3\nc\nTRUE\n\n\n\n\n\n\n\nYou might wonder if this is really possible without dangerous hacks. But, with tibble package (2D columns are supported now), this is as easy as:\n\ntibble(\n  id = 1:3,\n  foo = tibble(\n    X = 1:3 * 0.1,\n    Y = letters[1:3],\n    Z = c(TRUE, FALSE, TRUE)\n  )\n)\n\n# A tibble: 3 × 2\n     id foo$X $Y    $Z   \n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt;\n1     1   0.1 a     TRUE \n2     2   0.2 b     FALSE\n3     3   0.3 c     TRUE \n\n\nFor more information about data.frame columns, please see Advanced R."
  },
  {
    "objectID": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#an-experimental-package-for-this",
    "href": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#an-experimental-package-for-this",
    "title": "Enhancing gather() and spread() by Using “Bundled” data.frames",
    "section": "An experimental package for this",
    "text": "An experimental package for this\nI created a package for bundling, tiedr. Since this is just an experiment, I don’t seriously introduce this. But, for convenience, let me use this package in this post because, otherwise, the code would be a bit long and hard to read…\nhttps://github.com/yutannihilation/tiedr\nI need four functions from this package, bundle(), unbundle(), gather_bundles(), and spread_bundles(). gather_bundles() and spread_bundles() are some kind of the variants of gather() and spread(), so probably you can guess the usages. Here, I just explain about the first two functions briefly.\n\nbundle()\nbundle() bundles columns. It takes data, and the specifications of bundles in the form of new_col1 = c(col1, col2, ...), new_col2 = c(col3, col4, ...), ....\n\nlibrary(tiedr)\n\nd &lt;- tibble(id = 1:3, X = 1:3 * 0.1, Y = letters[1:3], Z = c(TRUE, FALSE, TRUE))\n\nbundle(d, foo = X:Z)\n\n# A tibble: 3 × 2\n     id foo$X $Y    $Z   \n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt;\n1     1   0.1 a     TRUE \n2     2   0.2 b     FALSE\n3     3   0.3 c     TRUE \n\n\nbundle() also can rename the sub-columns at the same time.\n\nbundle(d, foo = c(x = X, y = Y, z = Z))\n\n# A tibble: 3 × 2\n     id foo$x $y    $z   \n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt;\n1     1   0.1 a     TRUE \n2     2   0.2 b     FALSE\n3     3   0.3 c     TRUE \n\n\n\n\nunbundle()\nunbundle() unbundles columns. This operation is almost the opposite of what bundle() does; one difference is that this adds the names of the bundle as prefixes in order to avoid name collisions. In case the prefix is not needed, we can use sep = NULL.\n\nd %&gt;%\n  bundle(foo = X:Z) %&gt;% \n  unbundle(foo)\n\n# A tibble: 3 × 4\n     id foo_X foo_Y foo_Z\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt;\n1     1   0.1 a     TRUE \n2     2   0.2 b     FALSE\n3     3   0.3 c     TRUE"
  },
  {
    "objectID": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#expose-hidden-structures-in-colnames-as-bundles",
    "href": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#expose-hidden-structures-in-colnames-as-bundles",
    "title": "Enhancing gather() and spread() by Using “Bundled” data.frames",
    "section": "Expose hidden structures in colnames as bundles",
    "text": "Expose hidden structures in colnames as bundles\nOne of the meaningful usage of bundled data.frame is to express the structure of a data. Suppose we have this data (from tidyverse/tidyr#150):\n\nd &lt;- tribble(\n  ~Race,~Female_LoTR,~Male_LoTR,~Female_TT,~Male_TT,~Female_RoTK,~Male_RoTK,\n  \"Elf\",        1229,       971,       331,     513,         183,       510,\n  \"Hobbit\",       14,      3644,         0,    2463,           2,      2673,\n  \"Man\",           0,      1995,       401,    3589,         268,      2459\n)\n\n\n\n\n\n\n\n\n\nRace\nFemale_LoTR\nMale_LoTR\nFemale_TT\nMale_TT\nFemale_RoTK\nMale_RoTK\n\n\n\n\nElf\n1229\n971\n331\n513\n183\n510\n\n\nHobbit\n14\n3644\n0\n2463\n2\n2673\n\n\nMan\n0\n1995\n401\n3589\n268\n2459\n\n\n\n\n\n\n\nIn this data, the prefixes Female_ and Male_ represent the column groups. Thus, as Kirill Müller suggests in the comment, these columns can be bundled (with the sub-columns renamed) to:\n\n\n\n\n\n\n\n\nRace\nFemale\nMale\n\n\nLoTR\nTT\nRoTK\nLoTR\nTT\nRoTK\n\n\n\n\nElf\n1229\n331\n183\n971\n513\n510\n\n\nHobbit\n14\n0\n2\n3644\n2463\n2673\n\n\nMan\n0\n401\n268\n1995\n3589\n2459\n\n\n\n\n\n\n\nWith bundle() we can write this as:\n\nd_bundled &lt;- d %&gt;% \n  bundle(\n    Female = c(LoTR = Female_LoTR, TT = Female_TT, RoTK = Female_RoTK),\n    Male   = c(LoTR = Male_LoTR,   TT = Male_TT,   RoTK = Male_RoTK)\n  )\n\nd_bundled\n\n# A tibble: 3 × 3\n  Race   Female$LoTR   $TT $RoTK Male$LoTR   $TT $RoTK\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Elf           1229   331   183       971   513   510\n2 Hobbit          14     0     2      3644  2463  2673\n3 Man              0   401   268      1995  3589  2459"
  },
  {
    "objectID": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#gather-the-bundles",
    "href": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#gather-the-bundles",
    "title": "Enhancing gather() and spread() by Using “Bundled” data.frames",
    "section": "gather() the bundles",
    "text": "gather() the bundles\nRemember gather() strips colnames and convert it to a column. We can do this operation for bundled data.frames in the same manner. But, unlike gather() for flat data.frames, we don’t need to specify a colname for values, because the contents in bundles already have their colnames.\nLet’s gather Female and Male bundles into key column.\n\nd_gathered &lt;- d_bundled %&gt;%\n  gather_bundles(Female, Male, .key = \"key\")\n\nd_gathered\n\n# A tibble: 6 × 5\n  Race   key     LoTR    TT  RoTK\n  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Elf    Female  1229   331   183\n2 Hobbit Female    14     0     2\n3 Man    Female     0   401   268\n4 Elf    Male     971   513   510\n5 Hobbit Male    3644  2463  2673\n6 Man    Male    1995  3589  2459\n\n\n\n\n\n\n\n\n\n\nRace\nkey\nLoTR\nTT\nRoTK\n\n\n\n\nElf\nFemale\n1229\n331\n183\n\n\nHobbit\nFemale\n14\n0\n2\n\n\nMan\nFemale\n0\n401\n268\n\n\nElf\nMale\n971\n513\n510\n\n\nHobbit\nMale\n3644\n2463\n2673\n\n\nMan\nMale\n1995\n3589\n2459\n\n\n\n\n\n\n\nNow we have all parts for implementing multi-gather(). I did bundling by manual, but we can have a helper function to find the common prefixes and bundle them automatically. So, multi-gather() will be something like:\n\nd %&gt;%\n  auto_bundle(-Race) %&gt;% \n  gather_bundles()\n\n# A tibble: 6 × 5\n  Race   key     LoTR    TT  RoTK\n  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Elf    Female  1229   331   183\n2 Hobbit Female    14     0     2\n3 Man    Female     0   401   268\n4 Elf    Male     971   513   510\n5 Hobbit Male    3644  2463  2673\n6 Man    Male    1995  3589  2459"
  },
  {
    "objectID": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#spread-to-the-bundles",
    "href": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#spread-to-the-bundles",
    "title": "Enhancing gather() and spread() by Using “Bundled” data.frames",
    "section": "spread() to the bundles",
    "text": "spread() to the bundles\nAs we already saw it’s possible to gather() multiple bundles, now it’s obvious that we can spread() multiple columns into multiple bundles vice versa. So, let me skip the details here.\nWe can multi-spread():\n\nd_bundled_again &lt;- d_gathered %&gt;%\n  spread_bundles(key, LoTR:RoTK)\n\nd_bundled_again\n\n# A tibble: 3 × 3\n  Race   Female$LoTR   $TT $RoTK Male$LoTR   $TT $RoTK\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Elf           1229   331   183       971   513   510\n2 Hobbit          14     0     2      3644  2463  2673\n3 Man              0   401   268      1995  3589  2459\n\n\nThen, unbundle() flattens the bundles to prefixes.\n\nd_bundled_again %&gt;%\n  unbundle(-Race)\n\n# A tibble: 3 × 7\n  Race   Female_LoTR Female_TT Female_RoTK Male_LoTR Male_TT Male_RoTK\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 Elf           1229       331         183       971     513       510\n2 Hobbit          14         0           2      3644    2463      2673\n3 Man              0       401         268      1995    3589      2459\n\n\nIt’s done. By combining these two steps, multi-spread() will be something like this:\n\nd_gathered %&gt;%\n  spread_bundles(key, LoTR:RoTK) %&gt;% \n  unbundle(-Race)"
  },
  {
    "objectID": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#considerations",
    "href": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#considerations",
    "title": "Enhancing gather() and spread() by Using “Bundled” data.frames",
    "section": "Considerations",
    "text": "Considerations\nAs I described above, multi-gather() doesn’t need the column name for value. On the other hand, usual gather() needs a new colname. Because, while it needs a name to become a column, an atomic column doesn’t have inner names.\nSimilarly, usual spread() can be considered as a special version of multi-spread(). Consider the case when we multi-spread()ing one column:\n\n# an example in ?tidyr::spread\ndf &lt;- tibble(x = c(\"a\", \"b\"), y = c(3, 4), z = c(5, 6))\n\nspread_bundles(df, key = x, y, simplify = FALSE)\n\n# A tibble: 2 × 3\n      z   a$y   b$y\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     5     3    NA\n2     6    NA     4\n\n\nSince y is the only one column in the data, we can simplify these 1-column data.frames to vectors:\n\nspread_bundles(df, key = x, y, simplify = TRUE)\n\n# A tibble: 2 × 3\n      z     a     b\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     5     3    NA\n2     6    NA     4\n\n\nThis is usual spread().\nI’m yet to see if we can improve the current spread() and gather() to handle these differences transparently…"
  },
  {
    "objectID": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#future-plans",
    "href": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#future-plans",
    "title": "Enhancing gather() and spread() by Using “Bundled” data.frames",
    "section": "Future plans",
    "text": "Future plans\nProbably, this post is too much about the implementational details. I need to think about the interfaces before proposing this on tidyr’s repo.\nAny suggestions or feedbacks are welcome!"
  },
  {
    "objectID": "post/dont-panic-we-can-unwind/index.html",
    "href": "post/dont-panic-we-can-unwind/index.html",
    "title": "Don’t panic!, We Can Unwind On Rust",
    "section": "",
    "text": "Three months ago, I wrote a blog post about R’s protection mechanism and stack unwinding. In the post, I said:\nBut, soon after I published this, Lionel Henry kindly told me that we have another option; it’s true that Rust doesn’t have longjmp, but we can write a C function that does longjmp and call the function from Rust.\nDisclaimer: I still don’t have enough time to test these implementations, so please read carefully. I might be wrong."
  },
  {
    "objectID": "post/dont-panic-we-can-unwind/index.html#what-was-the-problem",
    "href": "post/dont-panic-we-can-unwind/index.html#what-was-the-problem",
    "title": "Don’t panic!, We Can Unwind On Rust",
    "section": "What was the problem?",
    "text": "What was the problem?\nLet’s summarise the last blog post quickly. On error, the resources need to be freed before aborting. But, different languages have different error handling mechanism, and it might not work well to attempt to free resources beyond the language boundaries.\nIn order to solve this problem, R’s C API provices R_UnwindProtect(), which allows us to call a custom cleanup function before aborting. C++ can utilize this to delay R’s stack unwinding by escaping from it by longjmp and then resuming by another API, R_ContinueUnwind(). But, as Rust desn’t support longjmp, Rust cannot implement such a custom cleanup function. So, I concluded that, while it’s not totally safe, we have no choice but to use panic!() to escape from R_UnwindProtect().\nHowever, I was wrong."
  },
  {
    "objectID": "post/dont-panic-we-can-unwind/index.html#overview",
    "href": "post/dont-panic-we-can-unwind/index.html#overview",
    "title": "Don’t panic!, We Can Unwind On Rust",
    "section": "Overview",
    "text": "Overview\nThe basic idea is to write a C function that wraps R_UnwindProtect() and escapes by longjmp, and call it from Rust’s side. Rust then returns normally instead of aborting so that the Rust’s cleanup mechanism works. Returns what to what? The continuous token for R_ContinueUnwind() to the C’s side.\nYou might wonder what’s “the C’s side” mean, especially if you are familiar with extendr framework. Yes, Rust functions can be called directly from R via FFI if compiled so. However, to handle exceptions properly, we must use a C wrapper function to call R_ContinueUnwind().\n\nOne more thing that might be confusing is that we are going to write two C functions:\n\nC function to be called from Rust (i.e., a wrapper around R_UnwindProtect()).\nC function to be called from R session (i.e., a wrapper to inspect and handle the result from the Rust function)"
  },
  {
    "objectID": "post/dont-panic-we-can-unwind/index.html#implementation",
    "href": "post/dont-panic-we-can-unwind/index.html#implementation",
    "title": "Don’t panic!, We Can Unwind On Rust",
    "section": "Implementation",
    "text": "Implementation\n\nC function to be called from Rust\nFirst, write a wrapper function of R_UnwindProtect(). The most of the code is derived from cpp11, so you can read cpp11 for the details.\n#include &lt;setjmp.h&gt;\n#include &lt;stdint.h&gt;\n\n#include &lt;Rinternals.h&gt;\n\nvoid not_so_long_jump(void *jmpbuf, Rboolean jump) {\n    if (jump == TRUE) {\n        longjmp(*(jmp_buf *)jmpbuf, 1);\n    }\n}\n\nSEXP unwind_protect_impl(SEXP (*fun)(void *data), void *data) {\n    SEXP token = R_MakeUnwindCont();\n    R_PreserveObject(token);\n\n    jmp_buf jmpbuf;\n    if (setjmp(jmpbuf)) {\n        // Tag the pointer\n        return (SEXP)((uintptr_t)token | 1);\n    }\n\n    SEXP res = R_UnwindProtect(fun, data, not_so_long_jump, &jmpbuf, token);\n\n    // Note: cpp11 says we need SETCAR(token, R_NilValue) here, but \n    // let's ignore it for now for simplicity.\n\n    return res;\n}\n\nTagged pointer\nYou might wonder what | 1 does. This is a trick called “tagged pointer”, using a few of the least significant bits to store additional information. This time, the implementation uses the lowest bit as an error flag. tagged pointer: https://en.wikipedia.org/wiki/Tagged_pointer\nOf course, tagged pointer is not the only answer. For example, you can probably define a struct or enum to share the definition between Rust and C. But, I use tagged SEXP because it’s simpler in the sense it doesn’t require additional allocation.\nAlso note that I could just return Result here and create the tagged SEXP later. It’s just a matter of preferences.\n\n\nbuild.rs\nThis C function needs to be compiled before compiling the Rust code because it’s needed at the linking stage. We can use cc crate to compile it in the build script like this:\nuse std::path::Path;\n\nfn main() {\n    let r_include_dir =\n        std::env::var(\"R_INCLUDE_DIR\").expect(\"R_INCLUDE_DIR envvar must be provided.\");\n\n    cc::Build::new()\n        .file(\"src/unwind_protect_wrapper.c\")\n        .include(Path::new(&r_include_dir))\n        .compile(\"unwind_protect\");\n\n    println!(\"cargo:rerun-if-changed=src/unwind_protect_wrapper.c\");\n}\n\n\n\nExtern function definiton on Rust’s side\nTo let Rust call the function, we need an extern \"C\" function definiton on Rust’s side. Note that I’m not sure why the signature is this complex, but I just use the result of bindgen as it is.\nuse libR_sys::SEXP;\n\nextern \"C\" {\n    fn unwind_protect_impl(\n        fun: ::std::option::Option&lt;unsafe extern \"C\" fn(data: *mut ::std::os::raw::c_void) -&gt; SEXP&gt;,\n        data: *mut ::std::os::raw::c_void,\n    ) -&gt; SEXP;\n}\n\n\nRust function to call the C function\nTo actually call the above function from Rust code, we need an implementation like below to call unwind_protect(|| ...). This code is derived from extendr’s catch_r_error().\nuse libR_sys::SEXP;\n\n#[derive(Debug)]\npub enum Error {\n    UnexpectedType(String),\n    Aborted(SEXP),\n    Unknown,\n}\n\npub type Result&lt;T&gt; = std::result::Result&lt;T, Error&gt;;\n\npub unsafe fn unwind_protect&lt;F&gt;(f: F) -&gt; Result&lt;SEXP&gt;\nwhere\n    F: FnOnce() -&gt; SEXP + Copy,\n{\n    unsafe extern \"C\" fn do_call&lt;F&gt;(data: *mut std::os::raw::c_void) -&gt; SEXP\n    where\n        F: FnOnce() -&gt; SEXP + Copy,\n    {\n        let data = data as *const ();\n        let f: &F = &*(data as *const F);\n        f()\n    }\n\n    let fun_ptr = do_call::&lt;F&gt; as *const ();\n    let fun = std::mem::transmute(fun_ptr);\n    let data = std::mem::transmute(&f as *const F);\n    let res: SEXP = unwind_protect_impl(fun, data);\n\n    if (res as usize & 1) == 1 {\n        return Err(Error::Aborted(res));\n    }\n\n    Ok(res)\n}\n\n\nRust function to handle the result\nI use this function to handle various types of errors. But, in this post, we only need to handle Aborted error, so this might be a bit overkill. Anyway, the important point is that we must return the same type of result (in this case, SEXP) to C’s side. Even when the actual function doesn’t have any return values, we have to return some value like the one filled with R_NilValue.\nThis is because the final error handling (e.g., raising an error by Rf_error(), and resuming the aborted unwinding by R_ContinueUnwind()) is done on C’s side in this design. No return value means there’s no way to know whether calling the Rust function succeeded or not.\npub fn handle_result(result: Result&lt;SEXP&gt;) -&gt; SEXP {\n    match result {\n        Ok(res) =&gt; res,\n\n        Err(e) =&gt; match e {\n            // The token is already tagged, so pass it as it is.\n            Error::Aborted(token) =&gt; token,\n\n            // In other cases, return the error string with the tag\n            e =&gt; unsafe {\n                let msg = e.to_string();\n                let r_error = Rf_mkCharLenCE(\n                    msg.as_ptr() as *const i8,\n                    msg.len() as i32,\n                    cetype_t_CE_UTF8,\n                );\n\n                // set the error flag\n                (r_error as usize | 1) as SEXP\n            },\n        },\n    }\n}\n\n\nC function to handle the result\nLet’s look at the C’s side then. Again, as I described above, I use a tagged SEXP to indicate the error, but you can use any data structure. The point here is that a wrapper function like this must be implemented to handle unwinding. To handle errors from Rust code, you can raise an error in the R wrapper function instead of C. But, R_ContinueUnwind() cannot be called from R code, so this has to be done at C-level.\nstatic uintptr_t TAGGED_POINTER_MASK = (uintptr_t)1;\n\nSEXP handle_result(SEXP res_) {\n    uintptr_t res = (uintptr_t)res_;\n\n    // An error is indicated by tag.\n    if ((res & TAGGED_POINTER_MASK) == 1) {\n        // Remove tag\n        SEXP res_aligned = (SEXP)(res & ~TAGGED_POINTER_MASK);\n\n        // Currently, there are two types of error cases:\n        //\n        //   1. Error from Rust code\n        //   2. Error from R's C API, which is caught by R_UnwindProtect()\n        //\n        if (TYPEOF(res_aligned) == CHARSXP) {\n            // In case 1, the result is an error message that can be passed to\n            // Rf_error() directly.\n            Rf_error(\"%s\", CHAR(res_aligned));\n        } else {\n            // In case 2, the result is the token to restart the\n            // cleanup process on R's side.\n            R_ContinueUnwind(res_aligned);\n        }\n    }\n\n    return (SEXP)res;\n}\n\n\nExample implementation\nAn example implementation can be found here:\nhttps://github.com/yutannihilation/unextendr"
  },
  {
    "objectID": "post/a-survival-guide-to-install-rlang-from-github-on-windows/index.html",
    "href": "post/a-survival-guide-to-install-rlang-from-github-on-windows/index.html",
    "title": "A Survival Guide To Install rlang From GitHub On Windows",
    "section": "",
    "text": "I don’t have any strong feelings about OSs. They are just tools. I had been a Mac user for 10+ years since I was 10, and now I’m using Windows for no reason. All OSs have their pros and cons. For example, I like Mac, but, in the late 90s, I was very disappointed at Mac because it didn’t have fonts to display Shift_JIS art nicely.\nAnyway, I’m using Windows and I need to survive. Here’s an error I often see when I try to install rlang package from GitHub by devtools::install_github():\nThis is because rlang.dll is used by the current R session (or other session?), so Windows won’t let me overwrite it. What should I do? Here’s some advice."
  },
  {
    "objectID": "post/a-survival-guide-to-install-rlang-from-github-on-windows/index.html#restart-the-r-session",
    "href": "post/a-survival-guide-to-install-rlang-from-github-on-windows/index.html#restart-the-r-session",
    "title": "A Survival Guide To Install rlang From GitHub On Windows",
    "section": "Restart the R session",
    "text": "Restart the R session\nThis is always necessary. Since rlang is very fundamental package, it might be loaded as a dependency of some attached or loaded package (if you are curious about the differences between load and attach, R Packages helps). You need a fresh session with no packages (except for base packages) loaded. On RStudio, Ctrl+Shift+F10, or “Restart R” in “Session” menu.\n\nFor usual packages, this is enough. But, rlang is not the case…"
  },
  {
    "objectID": "post/a-survival-guide-to-install-rlang-from-github-on-windows/index.html#use-remotesinstall_github-instead-of-devtools",
    "href": "post/a-survival-guide-to-install-rlang-from-github-on-windows/index.html#use-remotesinstall_github-instead-of-devtools",
    "title": "A Survival Guide To Install rlang From GitHub On Windows",
    "section": "Use remotes::install_github() instead of devtools",
    "text": "Use remotes::install_github() instead of devtools\ndevtools::install_github() is just re-exported from remotes package. So, are the same one. But, if I use devtools::, devtools’s dependencies are loaded, and, at the moment, rlang is included here. So, the same error will occur.\nOn the other hand, remotes:: doesn’t need rlang directly or indirectly. So, run\nremotes::install_github(\"r-lib/rlang\")\n(You might also need remotes::install_github() for other dependency packages like glue.)\nNote that, pkg package seems aware of this kind of problems, so we’ll be free from this kind of problems when pkg is mature!"
  },
  {
    "objectID": "post/a-survival-guide-to-install-rlang-from-github-on-windows/index.html#extra-steps",
    "href": "post/a-survival-guide-to-install-rlang-from-github-on-windows/index.html#extra-steps",
    "title": "A Survival Guide To Install rlang From GitHub On Windows",
    "section": "Extra steps?",
    "text": "Extra steps?\nUsually, restarting the session + using remotes::install_github() works. But, in the past, I needed some extra steps. I don’t know why, but it seemed RStudio loads rlang in background (c.f. https://github.com/r-lib/remotes/issues/131). So, for future references, I note some. Hope this will never be needed again…\n\nRemove rlang\nRemoving package might help, since removed package cannot be loaded.\nremove.packages(\"rlang\")\n\n\nUse git clone, R CMD build and R CMD INSTALL\nIf you really need to be away from RStudio, or even from any R sessions. In those cases, you can use git and R CMD build and R CMD INSTALL on console.\ngit clone https://github.com/r-lib/rlang\nR.exe CMD build --no-manual --no-build-vignettes rlang/\nR.exe CMD INSTALL [--some-options-i-dont-remember] rlang_*.tar.gz"
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "",
    "text": "I’ve been struggling with configure.win for several days. I think I’ve done, but it seems I’ve come too far from the last post. So, let me try to explain what a configure.win (or configure) would look like.\nLet’s start with this Makevars.win, basically the same one on the last blog post."
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#tweak-makevars.win",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#tweak-makevars.win",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "Tweak Makevars.win",
    "text": "Tweak Makevars.win\nBefore talking about configure scripts, I have to write a bit about Makevars.win (and Makevars) because it has something to be fixed.\n\nSet CARGO_HOME\nCan you see what part is wrong in the above Makevars.win? After the first CRAN submission of this package, I got this reply:\n\nChecking this creates ~/.cargo sized 82MB, in violation of the CRAN Policy. Please fix as necessary and resubmit.\n\nThe CRAN Policy says:\n\n\nPackages should not write in the user’s home filespace (including clipboards), nor anywhere else on the file system apart from the R session’s temporary directory (or during installation in the location pointed to by TMPDIR: and such usage should be cleaned up).\n\n\nBy default, cargo uses ~/.cargo for caching various things like the crates.io index and the dependency crates. Apparently, this is not allowed. To avoid this, we can set CARGO_HOME to the package’s local directory, like gifski package does.\nexport CARGO_HOME=$(PWD)/.cargo\nAlternatively, we can set this on the head of the line of cargo build directly.\nCARGO_HOME=$(PWD)/.cargo\n\n...snip...\n\n  CARGO_HOME=$(PWD)/.cargo cargo build --target=$(TARGET) --lib --release --manifest-path=./rust/Cargo.toml\nIn addition to this, as it “should be cleaned up,” we also need to add these two lines after cargo build:\n    rm -Rf $(CARGO_HOME)\n    rm -Rf $(LIBDIR)/build\nBut, this is a bit painful in terms of development. Why do I need to compile it always from scratch even on my local??\n\n\nNOT_CRAN environmental variable\nFortunately, devtools utilities provide NOT_CRAN envvar to distinguish CRAN and other environment. So, maybe we can determine whether to set CARGO_HOME, depending on NON_CRAN. It would be:\n# An envvar cannot be referred to as $(NOT_CRAN)\nNOT_CRAN_ENVVAR = ${NOT_CRAN}\n\n$(STATLIB):\nifeq ($(NOT_CRAN_ENVVAR),\"true\")\n  cargo build --target=$(TARGET) --lib --release --manifest-path=./rust/Cargo.toml\nelse\n  CARGO_HOME=$(PWD)/.cargo cargo build --target=$(TARGET) --lib --release --manifest-path=./rust/Cargo.toml\n    rm -Rf $(CARGO_HOME)\n    rm -Rf $(LIBDIR)/build\nendif\n\n\nSet PATH\nOne more thing my Makevars (not Makevars.win this time) couldn’t covered was the case when cargo is on PATH but rustc is not. It seems gifski package handles this by including $(HOME)/.cargo/bin in PATH (Makevars).\nI think sourcing \"$(HOME)/.cargo/env\" should also work (this actually just sets PATH), so I’ll try this next time. Note that source is not available on dash, so use . for this.\n. \"$(HOME)/.cargo/env\" && cargo build ...\nOkay, done. Let’s move onto configure scripts."
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#what-is-a-configure-script",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#what-is-a-configure-script",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "What is a configure script?",
    "text": "What is a configure script?\nA configure script is often used for configuring Makefile or Makevars, depending on the user’s setup.\nWriting R Extensions says:\n\nIf your package needs some system-dependent configuration before installation you can include an executable (Bourne shell script configure in your package which (if present) is executed by R CMD INSTALL before any other action is performed.\n\nconfigure is executed on UNIX-alikes, and Windows uses a different file configure.win1. Actually, I got this request from CRAN:\n\nBy the way, ideally string2path would use configure to test for cargo\n\nSo far, I used Makevars.win for testing the existence of cargo, but it seems configure scripts are the better place for this. Moreover, I do want to check the cargo functionality more precisely, for example,\n\nif the Rust version is not too old to support (“MSRV”)\n(Windows only) if the Rust installation has the required toolchain, stable-msvc\n(Windows only) if the Rust installation has the required targets, x86_64-pc-windows-gnu and i686-pc-windows-gnu\n\nbut Makevars.win is a bit too narrow to write a complex shell script."
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#biarch-true",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#biarch-true",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "Biarch: true",
    "text": "Biarch: true\nIf we use configure.win, we have to add the following line to DESCRIPTION.\nBiarch: true\nOtherwise, the 32-bit version won’t get built for unknown reason and it makes CRAN angry. This behaviour is found on R for Windows FAQ, but it doesn’t explain what we should do. I found this on the following post on RStudio Community.\n\nhttps://community.rstudio.com/t/configure-win-and-cran-submission/24684/4"
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#check-cargo",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#check-cargo",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "Check cargo",
    "text": "Check cargo\n\nCheck cargo is installed\nThis is simple.\ncargo version &gt;/dev/null 2&gt;&1\nif [ $? -ne 0 ]; then\n  echo \"cargo command is not available\"\n  exit 1\nfi\nNote that we don’t want to exit here, because the absence of cargo isn’t the end of the world; there might be a precompiled binary for the platform. But, let’s tweak it later.\n\n\nCheck if the Rust version is not too old to support\nThis isn’t always necessary, but we might want to reject the older Rust in the case when we use some feature that is available after the specific version of Rust. You know, comparing versions is tricky. But, this can be archived by sort command with -V option and -C option (c.f. SO answer). -V means version sort. -C means checking if the input is already sorted, and errors when it’s not. In summary, the implementation is the following:\n# c.f. https://github.com/ron-rs/ron/issues/256#issuecomment-657999081\nMIN_RUST_VERSION=\"1.41.0\"\n\nRUST_VERSION=\"`cargo --version | cut -d' ' -f2`\"\nif ! printf '%s\\n' \"${MIN_RUST_VERSION}\" \"${RUST_VERSION}\" | sort -C -V; then\n  echo \"The installed version of cargo (${RUST_VERSION}) is older than the requirement (${MIN_RUST_VERSION})\"\n  exit 1\nfi\n(Btw, did you know we cannot use $(expr) notation in configure because this syntax isn’t available on Solaris? We need to use `expr` instead)\n\n\nCheck if the Rust installation has the required toolchain and targets (Windows only)\nOn Windows, extendr provides support only the specified set of toolchain and target. So, we need to check it.\nChecking toolchain is simple. Use + to specify the toolchain.\nEXPECTED_TOOLCHAIN=\"stable-msvc\"\n\ncargo \"+${EXPECTED_TOOLCHAIN}\" version &gt;/dev/null 2&gt;&1\nif [ $? -ne 0 ]; then\n  echo \"${EXPECTED_TOOLCHAIN} toolchain is not installed\"\n  exit 1\nfi\nInstalled targets can be listed by rustup target list --installed. So, the check would be like\nEXPECTED_TARGET=\"x86_64-pc-windows-gnu\"\n\nif ! rustup target list --installed | grep -q \"${EXPECTED_TARGET}\"; then\n  echo \"target ${EXPECTED_TARGET} is not installed\"\n  exit 1\nfi\nOne thing tricky here is that, unlike Makevars.win, configure.win is executed only once, not per architecture. So, we need to manually enumerate both 64-bit and 32-bit.\nOne more tricky thing at the time of writing this is…, you might not notice, the 32-bit version no longer exists in R-devel, which is supposed to be released as R 4.2! So, we want to check 32-bit only when there’s 32-bit. How? I couldn’t come up with some nice way, but it seems check on ${R_HOME}/bin/i386/ works:\ncheck_cargo_target() {\n  EXPECTED_TARGET=\"$1\"\n  \n  if ! rustup target list --installed | grep -q \"${EXPECTED_TARGET}\"; then\n    echo \"target ${EXPECTED_TARGET} is not installed\"\n    exit 1\n  fi\n}\n\ncheck_cargo_target x86_64-pc-windows-gnu\n\nif [ -d \"${R_HOME}/bin/i386/\" ]; then\n  check_cargo_target i686-pc-windows-gnu\nfi"
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#but-what-can-we-do",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#but-what-can-we-do",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "But what can we do?",
    "text": "But what can we do?\nNow we can check the cargo installation. But, if the check fails, what should we do? Actually, on CRAN, Windows and macOS machines don’t have Rust installed.\nThere are two options.\n\nInstall cargo into the temporary directory\nDownload the precompiled binaries (which means you have to serve the binaries on somewhere beforehand).\n\nFor example, the gifski package uses option 1 for macOS and option 2 for Windows. My string2path package now uses option 2 both for macOS and for Windows."
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#install-cargo-on-the-fly",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#install-cargo-on-the-fly",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "Install cargo on the fly",
    "text": "Install cargo on the fly\nThe implementation for macOS is below. The actual process is written in the downloaded script, but what it does is basically downloading cargo.\n# Try local version on MacOS, otherwise error\n[ `uname` = \"Darwin\" ] && curl \"https://autobrew.github.io/scripts/rust\" -sSf | sh && exit 0\nI don’t write much about this this time, but cargo package should also be useful for this strategy."
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#download-the-precompiled-binaries",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#download-the-precompiled-binaries",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "Download the precompiled binaries",
    "text": "Download the precompiled binaries\nThis is what was done in Makevars.win.\n$(STATLIB):\nifdef CARGO_EXISTS\n    cargo build --target=$(TARGET) --lib --release --manifest-path=./rust/Cargo.toml\nelse\n    mkdir -p $(LIBDIR)\n    curl -L -o $(STATLIB) https://github.com/yutannihilation/$(CRATE)/releases/download/$(BASE_TAG)/$(TARGET)-lib$(CRATE).a\nendif\nTo move this to configure and configure.win. There are several things to consider. For example:\n\nAs this downloads the binary before $(STATLIB) is executed, we need some tweak to ensure it’s not removed by C_clean\n(Windows only) unlike Makevars.win is executed per arch, configure.win is executed only once\n\nWhat’s more, I want to make one addition:\n\nVerify the checksums\n\nSome might have wondered if it’s safe to download an arbitrary binary from GitHub. First of all, I’d argue it’s safe. To say the least, essentially, it’s no unsafer than downloading a cargo binary itself, (or than “curl URL | sh”). If it’s downloaded over HTTPS, the data hardly gets compromised as long as their servers are not compromised. I believe GitHub servers and Rust servers are both very secure.\nThat said, we can improve the security further by “pinning” the binary. So, I recommend verifying the checksum when downloading a binary. For example, the stringi package does this.\n\nMakevars.in and Makevars.win.in\nRemember Makevars.win has these lines ($(STATLIB) is the artifact of the Rust code):\nall: C_clean\nC_clean:\n    rm -Rf $(SHLIB) $(STATLIB) $(OBJECTS)\nRemoving $(STATLIB) is needed to invoke cargo build in the case when cargo is installed. Otherwise, $(STATLIB) keeps existing, which means that target is never executed.\nOn the other hand, we don’t want to execute cargo build if there’s no cargo. So, in this case, we need to prevent the downloaded binary from getting removed.\nIn order to change the logic like this, we can use Makevars.in and Makevars.win.in as a template to generate Makevars and Makevars.win respectively. Generating these files can be done in configure and configure.win. For example,\nMakevars.in:\nC_clean:\n    rm -Rf $(SHLIB) $(OBJECTS) @CLEAN_EXTRA@\nconfigure:\n# cargo installed\nsed -e 's|@CLEAN_EXTRA@|$(STATLIB)|' src/Makevars.in &gt; src/Makevars\n\n# no cargo\nsed -e 's|@CLEAN_EXTRA@||' src/Makevars.in &gt; src/Makevars\nIf we generate Makevars and Makevars.win in configure scripts, we also need to clean up these by the cleanup script (this is required by Writing R Extensions).\nrm -f src/Makevars src/Makevars.win\nOf course, don’t forget to add src/Makevars and src/Makevars.win to .gitignore.\n(Probably, we can generate more sophisticated Makevars, but this time I used only this one replacement.)\n\n\nVerify the checksums\nTo verify the checksum, we can use sha256sum on platforms other than macOS, and shasum -a 256 on macOS. For the example of macOS:\nSHA256SUM_EXPECTED=be65f074cb7ae50e5784e7650f48579fff35f30ff663d1c01eabdc9f35c1f87c\n\n# Verify the checksum\nSHA256SUM_ACTUAL=`shasum -a 256 \"${DST}\" | cut -d' ' -f1`\nif [ -z \"${SHA256SUM_ACTUAL}\" ]; then\n  echo \"Failed to get the checksum\"\n  exit 1\nfi\n\nif [ \"${SHA256SUM_ACTUAL}\" != \"${SHA256SUM_EXPECTED}\" ]; then\n  echo \"Checksum mismatch for the pre-compiled binary\"\n  exit 1\nfi"
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#most-important-ask-cran-maintainers-to-exclude-solaris",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#most-important-ask-cran-maintainers-to-exclude-solaris",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "[MOST IMPORTANT!] Ask CRAN maintainers to exclude Solaris",
    "text": "[MOST IMPORTANT!] Ask CRAN maintainers to exclude Solaris\nOh, sorry, I forgot to enumerate the most important option!\n\nGive up\n\nBecause 32-bit Solaris is not a supported platform by Rust, there’s no option other than giving up. I wrote this on cran-comment and it seems this was accepted:\n\nI would like to request to exclude Solaris from the build targets because Solaris is not a supported platform by Rust. This should be in line with the treatments of other CRAN packages that use Rust; gifski, baseflow, and salso are not built on Solaris. I’m sorry that I didn’t write this in the first submission.\n\nBe sure to add some comment like this when you submit an R package with Rust to CRAN!"
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#example",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#example",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "Example",
    "text": "Example\nFor a real example, please refer to string2path, though your mileage may vary."
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#footnotes",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#footnotes",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere’s also configure.ucrt, but you can forget this for now.↩︎"
  },
  {
    "objectID": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html",
    "href": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html",
    "title": "Unofficial Introduction To extendr (2): Type Conversion Between R and Rust",
    "section": "",
    "text": "extendr is a project that provides an interface between R and Rust. In the last post, I explained about how to create an R package with extendr briefly. This time, we’ll walk though how to handle various R types."
  },
  {
    "objectID": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#vector",
    "href": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#vector",
    "title": "Unofficial Introduction To extendr (2): Type Conversion Between R and Rust",
    "section": "Vector",
    "text": "Vector\nLet’s start with the last example in the last post.\n\n#[extendr]\nfn add(x: i32, y: i32) -&gt; i32 {\n    x + y\n}\n\nWhile this works perfectly fine with a single value, this fails when the length is more than one.\n\nadd(1:2, 2:3)\n\nError in add(1:2, 2:3): Input must be of length 1. Vector of length &gt;1 given.\n\n\nThis is very easy to fix. In Rust, we can use Vec&lt;T&gt; to represent a vector of values of type T.\n\n// I don't explain much about the Rust code this time, but, for now, please don't\n// worry if you can't understand what it does at the moment. Probably it's not\n// very important to understand this post. Move forward.\n\n#[extendr]\nfn add2(x: Vec&lt;i32&gt;, y: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {\n    x.iter().enumerate().map(|(i, x)| x + y[i]).collect()\n}\n\n\nadd2(1:2, 2:3)\n\n[1] 3 5\n\n\nEasy!\n\nWait, didn’t you say we can’t do this…!?\nSome of you might remember, in this post, I wrote\n\nWe cannot simply pass a variable length of vector\n\nfrom R to Rust.\nYeah, it’s true it was too difficult because I was struggling to do it via FFI! There’s no metadata available about the length or the structure of the data by default. But, with extendr, we can seamlessly access these metadata via R’s C API. So, in short, extendr is the game changer.\n\n\n&[T]\nIf you are already familiar with Rust, you might feel using Vec&lt;T&gt; as arguments looks a bit weird. In fact, the document of Vec&lt;T&gt; says:\n\nIn Rust, it’s more common to pass slices as arguments rather than vectors when you just want to provide read access. The same goes for String and &str.\n(https://doc.rust-lang.org/std/vec/struct.Vec.html#slicing)\n\nYes, you can use &[T] instead of Vec&lt;T&gt;, and this seems to matter on the performance slightly. If you are familiar with Rust to the extent that you know the difference between &[T] and Vec&lt;T&gt; (confession: I’m not!), you can should use &[T] instead. Otherwise, Vec&lt;T&gt; just works.\n\n#[extendr]\nfn add2_slice(x: &[i32], y: &[i32]) -&gt; Vec&lt;i32&gt; {\n    x.iter().enumerate().map(|(i, x)| x + y[i]).collect()\n}\n\n\nadd2_slice(1:2, 2:3)\n\n[1] 3 5\n\n\nPlease note that this isn’t the reference to the original R object, just that to the copied values. If you really want no copying, you should use the “proxy” types, which I’ll cover in the next post."
  },
  {
    "objectID": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#na",
    "href": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#na",
    "title": "Unofficial Introduction To extendr (2): Type Conversion Between R and Rust",
    "section": "NA",
    "text": "NA\nOne more caveat about add() is that this cannot handle a missing value, NA.\n\nadd(1L, NA)\n\nError in add(1L, NA): unable to convert R object to primitive\n\n\nIn Rust, we can use Option&lt;T&gt; to represent an optional, or possibly missing, value.\n\n// pattern match is one of the most powerful things in Rust, btw!\n\n#[extendr]\nfn add3(x: Option&lt;i32&gt;, y: Option&lt;i32&gt;) -&gt; Option&lt;i32&gt; {\n    match (x, y) {\n        (Some(x), Some(y)) =&gt; Some(x + y),\n        _ =&gt; NA_INTEGER\n    }\n}\n\nThis function can handle NA.\n\nadd3(1L, 2L)\n\n[1] 3\n\nadd3(1L, NA)\n\n[1] NA\n\n\nIt might be safe to always use Option since there’s always possibility that R value can be NA by nature. But, we might want to choose non-Option version to avoid the overhead (c.f. How much overhead is there with Options and Results? - The Rust Programming Language Forum), so it depends."
  },
  {
    "objectID": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#primitive-types",
    "href": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#primitive-types",
    "title": "Unofficial Introduction To extendr (2): Type Conversion Between R and Rust",
    "section": "Primitive types",
    "text": "Primitive types\nOkay, let’s learn about the primitive types at last. Here’s the corresponding table of R types and Rust types. We don’t have the direct equivalent of factor and complex here, but let’s talk about it later.\n\n\n\nR\nRust\n\n\n\n\ninteger\ni32\n\n\nnumeric\nf64\n\n\nlogical\nbool\n\n\ncharacter\nString &str\n\n\nfactor\n-\n\n\ncomplex\n-\n\n\n\n\ninteger and numeric\ninteger and numeric can mainly be converted into i32 and f64 respectively. I used “mainly” because it’s not that strict. They both can be converted into either of:\n\nu8\nu16\nu32\nu64\ni8\ni16\ni32\ni64\nf32\nf64\n\nSo, in other words, if you don’t want to prevent from numeric values are coerced into integers, you’ll need to check the types by yourself.\n\n\nlogical\nlogical is translated from/into bool. That’s all.\n\n\ncharacter\ncharacter is a bit tricky in that you can convert it to either of String and &str. You’ll probably have to scratch your head to understand the concept of “lifetime” to choose the proper one (confession: I still don’t understand it). But, in short,\n\nString : choose this when you modify the content strings\n&str: choose this (probably with 'static lifetime) when you only reference the strings\n\nIf you are not familiar with Rust yet, I recommend you to start with String. String is copied around so you might have unnecessary overhead, but it’s generally easier to handle because we need to think about the lifetimes less frequently.\n\n\nfactor\nTo put things simpler, until this point, I deliberately chose the cases when we have the corresponding types in Rust’s side. But, factor isn’t the case. It cannot be directly converted into a simple Rust type (at least at the moment). Instead, it can be cast into StrItr. StrItr is a “proxy” to the underlying data on R’s side.\nI’ll try explaining this in another post, but keep in mind that extendr provides that “proxy”-type of interface as well as the simple conversion to Rust’s primitive types."
  },
  {
    "objectID": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#list",
    "href": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#list",
    "title": "Unofficial Introduction To extendr (2): Type Conversion Between R and Rust",
    "section": "list",
    "text": "list\nThe corresponding Rust class for list is List. A List can be converted into HashMap&lt;&str, Robj&gt;. Be careful that R’s list can be a different data structure than Hashmap; it can have duplicated elements and unnamed elements.\n\nuse std::collections::HashMap;\n\n#[extendr]\nfn print_a(x: List) {\n    let x_hashmap: HashMap&lt;&str, Robj&gt; = x.into_hashmap();\n    \n    println!(\"{:?}\", x_hashmap.get(\"a\"));\n}\n\n\nprint_a(list(a = 1, b = 2))\nprint_a(list(b = 2))\n\nr! is a macro to create an R object from a Rust expression, by the way."
  },
  {
    "objectID": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#robj",
    "href": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#robj",
    "title": "Unofficial Introduction To extendr (2): Type Conversion Between R and Rust",
    "section": "Robj?",
    "text": "Robj?\nAs a sneak peak of the next post, let’s take a look at the usage of Robj.\nSo far, I created only functions that accepts just one type. What if we want to create a function that accepts multiple types of arguments? In this case, we can create a function that takes Robj as its argument and convert it by ourselves. Robj has many methods as_XXX() to convert to (or, more precisely, extract and copy the value of R object, and turn it into) a type. Here, let’s use as_integer() to generate Option&lt;i32&gt; .\n\n#[extendr]\nfn int(x: Robj) -&gt; Option&lt;i32&gt; {\n    x.as_integer()\n}\n\n\n# integer\nint(1L)\n\n[1] 1\n\n# not integer-ish\nint(\"foo\")\n\n[1] NA"
  },
  {
    "objectID": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#whats-next",
    "href": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#whats-next",
    "title": "Unofficial Introduction To extendr (2): Type Conversion Between R and Rust",
    "section": "What’s next?",
    "text": "What’s next?\nIn this post, I focused mainly the Rust’s side of the type ecosystem. Next, I probably need to write about more R-ish things like Function or Symbol , which I need some time to understand correctly. Stay tuned…"
  },
  {
    "objectID": "post/2021-06-06-unofficial-introduction-to-extendr-1-your-first-r-package-with-rust/index.html",
    "href": "post/2021-06-06-unofficial-introduction-to-extendr-1-your-first-r-package-with-rust/index.html",
    "title": "Unofficial Introduction To extendr (1): Your First R Package With Rust",
    "section": "",
    "text": "Now the official version of the introduction is available!\nhttps://extendr.github.io/rextendr/articles/package.html"
  },
  {
    "objectID": "post/2018-06-09-plot-osm-tiles/index.html",
    "href": "post/2018-06-09-plot-osm-tiles/index.html",
    "title": "Plot geom_sf() On OpenStreetMap Tiles",
    "section": "",
    "text": "mapview is a very nice package to explore an sf object. It can overlay sf object on the map images:\nnc &lt;- sf::read_sf(system.file(\"shape/nc.shp\", package=\"sf\"))\n\n# mapview::mapview(nc)\nBut, how can I do this with ggplot2? (My friend told me mapview::mapshot() can generate a PNG, but I want to do this with ggplot2!)"
  },
  {
    "objectID": "post/2018-06-09-plot-osm-tiles/index.html#rtfm",
    "href": "post/2018-06-09-plot-osm-tiles/index.html#rtfm",
    "title": "Plot geom_sf() On OpenStreetMap Tiles",
    "section": "RTFM",
    "text": "RTFM\nBefore anything, I need to read Tile Usage Policy to use the OpenStreetMap tiles. For “Requirements” section, this is important:\n\nClearly display license attribution.\n\nAccording to Copyright and License, it’s so simple as just adding this caption to my plots:\n\nlabs(caption = \"\\U00a9 OpenStreetMap contributors\")\n\nFor “Technical Usage Requirements” section, I have to read this more carefully. Let’s look at the requirements one by one.\n\nValid HTTP User-Agent identifying application. Faking another app’s User-Agent WILL get you blocked.\n\nOh, it seems I need to add User-Agent header. OK, let’s invent some nice name… If I use httr::GET(), the code will probably like this:\n\nGET(\n  \"https://...\",\n  add_headers(`User-Agent` = \"Yutani's blog post\")\n)\n\n\nIf known, a valid HTTP Referer.\n\nI don’t have Referers, so I skip this.\n\nDO NOT send no-cache headers. (“Cache-Control: no-cache”, “Pragma: no-cache” etc.)\n\nI do nothing other than swearing I’ll never use this header.\n\nCache Tile downloads locally according to HTTP Expiry Header, alternatively a minimum of 7 days.\n\nAh, this is important. Let’s implement later.\n\nMaximum of 2 download threads. (Unmodified web browsers’ download thread limits are acceptable.)\n\nFortunately, I’m not good at parallelism, so this is fine."
  },
  {
    "objectID": "post/2018-06-09-plot-osm-tiles/index.html#get-tile-urls",
    "href": "post/2018-06-09-plot-osm-tiles/index.html#get-tile-urls",
    "title": "Plot geom_sf() On OpenStreetMap Tiles",
    "section": "Get Tile URLs",
    "text": "Get Tile URLs\nAccording to Slippy map tilenames, the URL of a tile follows this format:\nhttps://[abc].tile.openstreetmap.org/zoom/x/y.png \nI have to fill these four parts:\n\n[abc]\nzoom\nx\ny\n\nLet’s look at these one by one.\n\n[abc]\n[abc] means there are three domains; a.tile.openstreetmap.org, b.tile.openstreetmap.org, c.tile.openstreetmap.org. But, why? It says:\n\nBrowser-based applications can thus request multiple tiles from multiple subdomains faster than from one subdomain.\n\nSo, as I’m not browser-based, I can choose arbitrary one.\n\n\nzoom\nZoom parameter is an integer number from 0 to 19. If zoom is 0, there’s only one tile. Likewise 2 x 2 tiles for zoom 1, 4 x 4 tiles for zoom 2, and so on. Then, which one should I choose? This can be roughly determined based on the size of the bbox (boundary box) of the sf object.\n\n# get the bbox\nb &lt;- sf::st_bbox(nc)\nb\n\n     xmin      ymin      xmax      ymax \n-84.32385  33.88199 -75.45698  36.58965 \n\n# calculate the lengths of x and y of the bbox\nx_len &lt;- b[\"xmax\"] - b[\"xmin\"]\ny_len &lt;- b[\"ymax\"] - b[\"ymin\"]\n\n# calculate the minimum zoom level that is smaller than the lengths\nx_zoom &lt;- sum(x_len &lt; 360 / 2^(0:19)) - 1\ny_zoom &lt;- sum(y_len &lt; 170.1022 / 2^(0:19)) - 1\n\nzoom &lt;- min(x_zoom, y_zoom)\nzoom\n\n[1] 5\n\n\nBut, since the tile is so small as 256 × 256 pixel, it’s often better to zoom more.\n\nzoom &lt;- zoom + 2\n\n(I’m not sure how to do this correctly, I guess the zoom level should be determined by the size of the plot canvas, not the bbox.)\n\n\nx and y\nA pair of x and y represents the location of a tile. x corresponds to longitude, y to latitude. If the zoom is given, I can convert longitudes and latitudes to x and y according to the pseudo code on OpenStreetMap’s wiki:\n\nsec &lt;- function(x) {\n  1 / cos(x)\n}\n\nlonlat2xy &lt;- function(lat_deg, lon_deg, zoom) {\n  n &lt;- 2^zoom\n\n  x &lt;- (n * (lat_deg + 180)) %/% 360\n  lon_rad &lt;- lon_deg * pi / 180\n  y &lt;- (n * (1 - log(tan(lon_rad) + sec(lon_rad)) / pi)) %/% 2\n\n  list(x = x, y = y)\n}\n\nBut, how can I find the set of tiles which covers the whole bbox?\n\nlibrary(ggplot2)\n\np &lt;- ggplot(nc) +\n  geom_sf() +\n  annotate(\"rect\", xmin = b[\"xmin\"], xmax = b[\"xmax\"], ymin = b[\"ymin\"], ymax = b[\"ymax\"],\n           colour = alpha(\"red\", 0.4), fill = \"transparent\", linetype = \"dashed\", size = 1.2)\n\nsize aesthetic has been deprecated for use with lines as of ggplot2 3.4.0\nℹ Please use linewidth aesthetic instead\nThis message is displayed once every 8 hours.\n\np\n\n\n\n\n\n\n\n\nThis problem can be simplified; I can focus only two corners, the north-west and the south-east. If I calculate which tiles of x and y those two points fall in, I can find the rest of the tiles by filling the sequences of x and y between these two tiles.\n\ncorners &lt;- expand.grid(x = b[c(1, 3)], y = b[c(2, 4)])\n\np +\n  geom_point(aes(x, y), corners[2:3,], colour = \"red\", size = 5)\n\n\n\n\n\n\n\n\nHere’s the tiles:\n\nxy &lt;- lonlat2xy(b[c(\"xmin\", \"xmax\")], b[c(\"ymin\", \"ymax\")], zoom)\n\ntiles &lt;- expand.grid(x = seq(xy$x[\"xmin\"], xy$x[\"xmax\"]),\n                     y = seq(xy$y[\"ymin\"], xy$y[\"ymax\"]))\n\ntiles\n\n   x  y\n1 34 51\n2 35 51\n3 36 51\n4 37 51\n5 34 50\n6 35 50\n7 36 50\n8 37 50\n\n\n\n\nTile URLs\nFrom the results above, I can yield the URLs of the tiles:\n\nurls &lt;- sprintf(\"https://a.tile.openstreetmap.org/%d/%d/%d.png\", zoom, tiles$x, tiles$y)\nurls\n\n[1] \"https://a.tile.openstreetmap.org/7/34/51.png\"\n[2] \"https://a.tile.openstreetmap.org/7/35/51.png\"\n[3] \"https://a.tile.openstreetmap.org/7/36/51.png\"\n[4] \"https://a.tile.openstreetmap.org/7/37/51.png\"\n[5] \"https://a.tile.openstreetmap.org/7/34/50.png\"\n[6] \"https://a.tile.openstreetmap.org/7/35/50.png\"\n[7] \"https://a.tile.openstreetmap.org/7/36/50.png\"\n[8] \"https://a.tile.openstreetmap.org/7/37/50.png\"\n\n\nOK, now I can download them. But, before that, let’s confirm that the tiles really cover the expected area.\n\n\nTile positions\nTo plot these tiles, I calculate the north-west corner of a tile by the following code (the pseudo code for this is also found on the wiki):\n\nxy2lonlat &lt;- function(x, y, zoom) {\n  n &lt;- 2^zoom\n\n  lon_deg &lt;- x / n * 360.0 - 180.0\n  lat_rad &lt;- atan(sinh(pi * (1 - 2 * y / n)))\n  lat_deg &lt;- lat_rad * 180.0 / pi\n\n  list(lon_deg = lon_deg, lat_deg = lat_deg)\n}\n\nThen, south-east corners can be also calculated easily. Let’s calculate the both corners and bind them to a data.frame.\n\nlibrary(purrr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nnw_corners &lt;- pmap_dfr(tiles, xy2lonlat, zoom = zoom)\n# add 1 to x and y to get the south-east corners\nse_corners &lt;- pmap_dfr(mutate_all(tiles, `+`, 1), xy2lonlat, zoom = zoom)\n\nnames(nw_corners) &lt;- c(\"xmin\", \"ymax\")\nnames(se_corners) &lt;- c(\"xmax\", \"ymin\")\n\ntile_positions &lt;- bind_cols(nw_corners, se_corners)\ntile_positions\n\n# A tibble: 8 × 4\n   xmin  ymax  xmax  ymin\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -84.4  34.3 -81.6  32.0\n2 -81.6  34.3 -78.8  32.0\n3 -78.8  34.3 -75.9  32.0\n4 -75.9  34.3 -73.1  32.0\n5 -84.4  36.6 -81.6  34.3\n6 -81.6  36.6 -78.8  34.3\n7 -78.8  36.6 -75.9  34.3\n8 -75.9  36.6 -73.1  34.3\n\n\nNow I can plot the empty tiles as below:\n\np +\n  geom_point(aes(x, y), corners[2:3,], colour = \"red\", size = 5) +\n  geom_rect(data = tile_positions,\n            aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),\n            colour = \"blue\", fill = \"transparent\")\n\n\n\n\n\n\n\n\nYay, confirmed! Let’s proceed to the next step."
  },
  {
    "objectID": "post/2018-06-09-plot-osm-tiles/index.html#get-tile-data",
    "href": "post/2018-06-09-plot-osm-tiles/index.html#get-tile-data",
    "title": "Plot geom_sf() On OpenStreetMap Tiles",
    "section": "Get tile data",
    "text": "Get tile data\nIf I just get the response from a URL, httr::GET() is handy. But, this time, for the requirement of caching, I have to save the responses to disk first. So, I use curl::curl_download() here.\nNote that PNG data can be read into R session by png::readPNG().\n\nget_tile &lt;- function(url) {\n  # build a local path\n  path &lt;- stringr::str_extract(url, \"/\\\\d+/\\\\d+/\\\\d+.png\")\n  local_png &lt;- here::here(file.path(\"data\", \"osm-tiles\", path))\n\n  if (!file.exists(local_png)) {\n    dir.create(dirname(local_png), showWarnings = FALSE, recursive = TRUE)\n    \n    # add header\n    h &lt;- curl::new_handle()\n    curl::handle_setheaders(h, `User-Agent` = \"Yutani's blog post\")\n    \n    curl::curl_download(url, destfile = local_png)\n  }\n\n  png::readPNG(local_png)\n}\n\nThen, let’s get all tiles.\n\npngs &lt;- map(urls, get_tile)"
  },
  {
    "objectID": "post/2018-06-09-plot-osm-tiles/index.html#plot-tiles",
    "href": "post/2018-06-09-plot-osm-tiles/index.html#plot-tiles",
    "title": "Plot geom_sf() On OpenStreetMap Tiles",
    "section": "Plot tiles",
    "text": "Plot tiles\nTo plot tiles, I use annotation_raster(), whose necessary arguments are:\n\nraster\nxmin\nxmax\nymin\nymax\n\nThe first one is pngs and the others are contained in tile_positions. So, let’s combine them so that I can use pmap().\n\nargs &lt;- tile_positions %&gt;%\n  mutate(raster = pngs)\n\nargs\n\n# A tibble: 8 × 5\n   xmin  ymax  xmax  ymin raster               \n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;               \n1 -84.4  34.3 -81.6  32.0 &lt;dbl [256 × 256 × 3]&gt;\n2 -81.6  34.3 -78.8  32.0 &lt;dbl [256 × 256 × 3]&gt;\n3 -78.8  34.3 -75.9  32.0 &lt;dbl [256 × 256 × 3]&gt;\n4 -75.9  34.3 -73.1  32.0 &lt;dbl [256 × 256 × 3]&gt;\n5 -84.4  36.6 -81.6  34.3 &lt;dbl [256 × 256 × 3]&gt;\n6 -81.6  36.6 -78.8  34.3 &lt;dbl [256 × 256 × 3]&gt;\n7 -78.8  36.6 -75.9  34.3 &lt;dbl [256 × 256 × 3]&gt;\n8 -75.9  36.6 -73.1  34.3 &lt;dbl [256 × 256 × 3]&gt;\n\n\nNow I can plot tiles at last.\n\nggplot(nc) +\n  pmap(args, annotation_raster, interpolate = TRUE) +\n  geom_sf(fill = alpha(\"red\", 0.3)) +\n  # don't forget the license notice!\n  labs(caption = \"\\U00a9 OpenStreetMap contributors\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nDone!\nBut, I didn’t expect the code would be this long… Maybe I need to create a package for this."
  },
  {
    "objectID": "post/2018-06-09-plot-osm-tiles/index.html#caveats",
    "href": "post/2018-06-09-plot-osm-tiles/index.html#caveats",
    "title": "Plot geom_sf() On OpenStreetMap Tiles",
    "section": "Caveats",
    "text": "Caveats\nNote that, I didn’t care about the CRS because nc’s CRS is fortunately EPSG 3857, which OpenStreetMap uses. If the sf object I want to plot has the different CRS, there may be a bit more to consider (and I don’t understand the CRS well…).\nUpdate:\nSorry, I was wrong… nc’s CRS is EPSG 4267 and OpenStreetMap tiles use EPSG 4326. Thanks Edzer for pointing this out!\n\nsf::st_crs(nc)\n\nCoordinate Reference System:\n  User input: NAD27 \n  wkt:\nGEOGCRS[\"NAD27\",\n    DATUM[\"North American Datum 1927\",\n        ELLIPSOID[\"Clarke 1866\",6378206.4,294.978698213898,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4267]]\n\n\nSo, I should have converted nc to the CRS first.\n\nnc_4326 &lt;- sf::st_transform(nc, 4326)\n\nFortunately, the difference between EPSG 4267 and EPSG 4326 is rather negligible for this scale, so the result map should look almost same if I used nc_4326 instead of nc. Here’s the difference (can you see there’s a little red colors?):\n\nnc_4326_not_transformed &lt;- sf::`st_crs&lt;-`(nc, 4326)\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\nggplot() +\n  geom_sf(data = nc_4326_not_transformed, fill = \"transparent\", colour = \"red\") +\n  geom_sf(data = nc_4326, fill = \"transparent\", colour = \"blue\") +\n  theme_minimal()"
  },
  {
    "objectID": "post/2017-11-07-ggplot-add/index.html",
    "href": "post/2017-11-07-ggplot-add/index.html",
    "title": "An Example Usage of ggplot_add()",
    "section": "",
    "text": "A generic function ggplot_add() was added to ggplot2 by this PR:\nAllow addition of custom objects by thomasp85 · Pull Request #2309 · tidyverse/ggplot2\nI think creating a custom Geom or Stat and its constructor (geom_*() or stat_*()) is enough for the most of the extension packages of ggplot2, but some people, including me, need this."
  },
  {
    "objectID": "post/2017-11-07-ggplot-add/index.html#why-are-there-no-geom_highlight",
    "href": "post/2017-11-07-ggplot-add/index.html#why-are-there-no-geom_highlight",
    "title": "An Example Usage of ggplot_add()",
    "section": "Why are there no geom_highlight()?",
    "text": "Why are there no geom_highlight()?\nHere is an example code of my package gghighlight:\ngghighlight_point(d, aes(foo, bar), predicate = bar &gt; 20 & baz == \"A\")\nYou may wonder why this can’t be written like this:\nggplot(d, aes(foo, bar)) +\n  geom_highlight_point(bar &gt; 20 & baz == \"A\")\nLet me explain a bit."
  },
  {
    "objectID": "post/2017-11-07-ggplot-add/index.html#geom_stat_-doesnt-know-about-the-other-layers",
    "href": "post/2017-11-07-ggplot-add/index.html#geom_stat_-doesnt-know-about-the-other-layers",
    "title": "An Example Usage of ggplot_add()",
    "section": "geom_*()/stat_*() doesn’t know about the other layers",
    "text": "geom_*()/stat_*() doesn’t know about the other layers\ngeom_highlight_point(bar &gt; 20 & baz == \"A\") is passed bar &gt; 20 & baz == \"A\" without the data d, with which the expression should be evaluated. It needs d specified in ggplot(...).\nBut, considering the structure of the above code, geom_highlight_point(...) cannot access to the result of ggplot(...) in any usual way:\n`+`(ggplot(...), geom_highlight_point(...))\nIf ggplot2 were designed pipe-friendly, this\n`%&gt;%`(ggplot(...), geom_highlight_point(...))\nwould be evaluated as this, which means geom_highlight_point(...) could take d from ggplot(...)…\ngeom_highlight_point(ggplot(...), ...)\nAnyway, let’s give up here. All I have to do is set this expression as an attribute of a custom Geom and pray that it will be evaluated with the proper data in the phase of building a plot."
  },
  {
    "objectID": "post/2017-11-07-ggplot-add/index.html#geomstat-doesnt-know-about-the-original-data",
    "href": "post/2017-11-07-ggplot-add/index.html#geomstat-doesnt-know-about-the-original-data",
    "title": "An Example Usage of ggplot_add()",
    "section": "Geom*/Stat* doesn’t know about the original data",
    "text": "Geom*/Stat* doesn’t know about the original data\nTake a look at the simplest example in the vignette “Extending ggplot2”:\nStatChull &lt;- ggproto(\"StatChull\", Stat,\n  compute_group = function(data, scales) {\n    data[chull(data$x, data$y), , drop = FALSE]\n  },\n\n  required_aes = c(\"x\", \"y\")\n)\nYou may notice that compute_group() expects data has the fixed column x and y. Actually, data is not the original data but the one the mapping is already applied. So, there is no column bar and baz anymore; bar is renamed to y and baz is dropped. Oh no, I cannot evaluate bar &gt; 20 & baz == \"A\" here, too…"
  },
  {
    "objectID": "post/2017-11-07-ggplot-add/index.html#ggplot_add",
    "href": "post/2017-11-07-ggplot-add/index.html#ggplot_add",
    "title": "An Example Usage of ggplot_add()",
    "section": "ggplot_add()",
    "text": "ggplot_add()\nLet’s remember this one:\n`+`(ggplot(...), geom_highlight_point(...))\nOnly + (or +.gg) can access to both ggplot(...) and geom_highlight_point(...). This means that, inside +.gg, bar &gt; 20 & baz == \"A\" can be evaluated.\nSo…, should I implement my custom +.gg by myself? No, because I will be able to use ggplot_add()!\nggplot_add() is called in +.gg() via add_ggplot() (very confusing..) and is passed both the original plot and the new object. The current implementation is this:\n\"+.gg\" &lt;- function(e1, e2) {\n  # Get the name of what was passed in as e2, and pass along so that it\n  # can be displayed in error messages\n  e2name &lt;- deparse(substitute(e2))\n\n  if      (is.theme(e1))  add_theme(e1, e2, e2name)\n  else if (is.ggplot(e1)) add_ggplot(e1, e2, e2name)\n  else if (is.ggproto(e1)) {\n    stop(\"Cannot add ggproto objects together.\",\n         \" Did you forget to add this object to a ggplot object?\",\n         call. = FALSE)\n  }\n}\n(https://github.com/tidyverse/ggplot2/blob/7d0549a03e5ea08c27c768e88d5717f18cb4a5ce/R/plot-construction.r#L40-L52)\nadd_ggplot &lt;- function(p, object, objectname) {\n  if (is.null(object)) return(p)\n\n  p &lt;- plot_clone(p)\n  p &lt;- ggplot_add(object, p, objectname)\n  set_last_plot(p)\n  p\n}\n(https://github.com/tidyverse/ggplot2/blob/7d0549a03e5ea08c27c768e88d5717f18cb4a5ce/R/plot-construction.r#L59-L66)\nBy using this, I can implement the proof-of-concept version of geom_highlight_point() as bellow:\ngeom_highlight_point &lt;- function(expr) {\n  structure(list(expr = rlang::enquo(expr)), class = \"highlight\")\n}\n\nggplot_add.highlight &lt;- function(object, plot, object_name) {\n  new_data &lt;- dplyr::filter(plot$data, !! object$expr)\n  new_layer &lt;- geom_point(data = new_data,\n                          mapping = plot$mapping,\n                          colour = alpha(\"red\", 0.5),\n                          size = 5)\n  plot$layers &lt;- append(plot$layers, new_layer)\n  plot\n}\nlibrary(ggplot2)\n\nd &lt;- data.frame(foo = 11:30, bar = 11:30, baz = rep(c(\"A\", \"B\", \"C\"), length.out = 20),\n                stringsAsFactors = FALSE)\n\nggplot(d, aes(foo, bar)) +\n  geom_highlight_point(bar &gt; 20 & baz == \"A\") +\n  geom_point()  # for comparison\n\nI’m not sure if this is the intended usage of ggplot_add(), but this seems very nice. Looking forward to the next release of ggplot2!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m Hiroaki Yutani. You can call me either Yutani or Hiroaki (I guess Yutani is easier).\nThis blog is where I struggle to learn about R (and improve my poor English)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wannabe Rstats-fu",
    "section": "",
    "text": "Use rust-cache GitHub Actions For R Package\n\n\n\n\n\n\nRust\n\n\nGitHub Actions\n\n\n\n\n\n\n\n\n\nOct 16, 2024\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nHow To Use Winit With R (Or How To Run Winit On A Non-Main Thread)\n\n\n\n\n\n\nRust\n\n\nsavvy\n\n\n\n\n\n\n\n\n\nOct 11, 2024\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction of Savvy, (Not Really) an Alternative to Extendr: Part 1\n\n\n\n\n\n\nRust\n\n\nsavvy\n\n\nextendr\n\n\n\n\n\n\n\n\n\nSep 17, 2023\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t panic!, We Can Unwind On Rust\n\n\n\n\n\n\nRust\n\n\nextendr\n\n\n\n\n\n\n\n\n\nAug 6, 2023\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nA Quick Note About How To Bundle Rust Crates For An R Package\n\n\n\n\n\n\nRust\n\n\nextendr\n\n\n\n\n\n\n\n\n\nJul 16, 2023\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nRust 1.70 And Build Failures On Windows\n\n\n\n\n\n\nRust\n\n\nextendr\n\n\n\n\n\n\n\n\n\nJun 10, 2023\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nR, Rust, Protect, And Unwinding\n\n\n\n\n\n\nRust\n\n\nextendr\n\n\n\n\n\n\n\n\n\nMay 20, 2023\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nRust and the CRAN Repository Policy\n\n\n\n\n\n\nRust\n\n\nextendr\n\n\n\n\n\n\n\n\n\nAug 14, 2022\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nWriting A Configure Script For An R Package Using Rust\n\n\n\n\n\n\nRust\n\n\nextendr\n\n\n\nconfigure.win and configure \n\n\n\n\n\nSep 21, 2021\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nUnofficial Introduction To extendr (Appendix I): Setup GitHub Actions CI\n\n\n\n\n\n\nRust\n\n\nextendr\n\n\n\nIntegrate R and Rust with extendr \n\n\n\n\n\nAug 1, 2021\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nUnofficial Introduction To extendr (2): Type Conversion Between R and Rust\n\n\n\n\n\n\nRust\n\n\nextendr\n\n\n\nIntegrate R and Rust with extendr \n\n\n\n\n\nJun 14, 2021\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\ngghighlight 0.3.2\n\n\n\n\n\n\ngghighlight\n\n\nggplot2\n\n\nR package\n\n\n\n\n\n\n\n\n\nJun 7, 2021\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nUnofficial Introduction To extendr (1): Your First R Package With Rust\n\n\n\n\n\n\nRust\n\n\nextendr\n\n\n\nIntegrate R and Rust with extendr. \n\n\n\n\n\nJun 6, 2021\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nTips To Turn R Markdown Into Slidev Presentation\n\n\n\n\n\n\nR Markdown\n\n\nSlidev\n\n\n\nSlidev is a tool to create slides from Markdown. This post is about some tips to use it in combination with R Markdown. \n\n\n\n\n\nJun 5, 2021\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nSome more notes about using Rust code in R packages\n\n\n\n\n\n\nRust\n\n\n\n\n\n\n\n\n\nSep 15, 2020\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\ngghighlight 0.2.0\n\n\n\n\n\n\ngghighlight\n\n\nggplot2\n\n\nR package\n\n\n\ngghighlight 0.2.0 is released! \n\n\n\n\n\nFeb 17, 2020\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nEnhancing gather() and spread() by Using “Bundled” data.frames\n\n\n\n\n\n\ntidyr\n\n\ntidy data\n\n\n\n\n\n\n\n\n\nFeb 3, 2019\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nA Survival Guide To Install rlang From GitHub On Windows\n\n\n\n\n\n\nrlang\n\n\nWindows\n\n\n\n\n\n\n\n\n\nJan 25, 2019\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\ngather() and spread() Explained By gt\n\n\n\n\n\n\ntidyr\n\n\ntidy data\n\n\ngt\n\n\n\n\n\n\n\n\n\nJan 24, 2019\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nA Tip to Debug ggplot2\n\n\n\n\n\n\npackage development\n\n\n\n\n\n\n\n\n\nJan 11, 2019\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nHow To Convert A Human To Waves By Magick Package\n\n\n\n\n\n\nggplot2\n\n\nmagick\n\n\n\n\n\n\n\n\n\nNov 23, 2018\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nQuote While the Promise Is Hot!\n\n\n\n\n\n\ntidyeval\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\nOct 18, 2018\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\ngeom_sf_text() and geom_sf_label() Are Coming!\n\n\n\n\n\n\ntidyverse\n\n\nggplot2\n\n\n\n\n\n\n\n\n\nOct 10, 2018\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nDouble dispatch of S3 method\n\n\n\n\n\n\nR internal\n\n\n\n\n\n\n\n\n\nSep 17, 2018\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nPlot geom_sf() On OpenStreetMap Tiles\n\n\n\n\n\n\nggplot2\n\n\nGIS\n\n\n\n\n\n\n\n\n\nJun 9, 2018\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nAnatomy of gghighlight\n\n\n\n\n\n\ngghighlight\n\n\nggplot2\n\n\n\n\n\n\n\n\n\nJun 3, 2018\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\nAn Example Usage of ggplot_add()\n\n\n\n\n\n\ngghighlight\n\n\nggplot2\n\n\nR package\n\n\n\n\n\n\n\n\n\nNov 7, 2017\n\n\nHiroaki Yutani\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/2018-06-03-anatomy-of-gghighlight/index.html",
    "href": "post/2018-06-03-anatomy-of-gghighlight/index.html",
    "title": "Anatomy of gghighlight",
    "section": "",
    "text": "I’m overhauling my gghighlight package toward the upcoming release of ggplot2 2.3.0. I think I will introduce about the new gghighlight() soon, but before that, I want to write out the ideas behind gghighlight.\nNote that what I’ll write here contains few new things, as the basic idea is already covered by this great post:\nMy post is mainly for organizing my thought, yet I hope someone find this useful :)\n# tweak for plotting\nknit_print.ggplot &lt;- function(x, ...) {\n  x &lt;- x  +\n    theme_minimal() +\n    scale_x_continuous(expand = expand_scale(mult = 0.5)) +\n    scale_y_continuous(expand = expand_scale(mult = 0.2))\n  ggplot2:::print.ggplot(x, ...)\n}"
  },
  {
    "objectID": "post/2018-06-03-anatomy-of-gghighlight/index.html#data",
    "href": "post/2018-06-03-anatomy-of-gghighlight/index.html#data",
    "title": "Anatomy of gghighlight",
    "section": "Data",
    "text": "Data\nSuppose we have this data:\n\n\n\n\n\nx\ny\ntype\nvalue\n\n\n\n\n3\n3\na\n0\n\n\n8\n3\na\n1\n\n\n13\n3\na\n0\n\n\n2\n2\nb\n0\n\n\n7\n2\nb\n10\n\n\n12\n2\nb\n10\n\n\n1\n1\nc\n10\n\n\n6\n1\nc\n20\n\n\n11\n1\nc\n0"
  },
  {
    "objectID": "post/2018-06-03-anatomy-of-gghighlight/index.html#simple-plot",
    "href": "post/2018-06-03-anatomy-of-gghighlight/index.html#simple-plot",
    "title": "Anatomy of gghighlight",
    "section": "Simple plot",
    "text": "Simple plot\nIf we plot the data very simply, the code would be like this:\n\nlibrary(tidyverse)\n\nggplot(d, aes(x, y, colour = type)) +\n  geom_point(size = 10)\n\nWarning: `expand_scale()` was deprecated in ggplot2 3.3.0.\nPlease use `expansion()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated."
  },
  {
    "objectID": "post/2018-06-03-anatomy-of-gghighlight/index.html#highlighted-plot",
    "href": "post/2018-06-03-anatomy-of-gghighlight/index.html#highlighted-plot",
    "title": "Anatomy of gghighlight",
    "section": "Highlighted plot",
    "text": "Highlighted plot\nNow, what if we want to highlight only the points of records whose type are \"b\"?\nWe need two layers:\n\nunhighlighted layer\nhighlighted layer\n\n\nCreate an unhighlighted layer\nAn unhighlighted layer is the colorless version of the above points with the same data. To create this, we can simply remove colour from aes() and specify a static colour \"grey\". I call this operation as bleach.\n\nbleached_layer &lt;- geom_point(data = d, aes(x, y),\n                             size = 10, colour = \"grey\")\n\nIf we plot this, the result would be below:\n\nggplot() +\n  bleached_layer\n\n\n\n\n\n\n\n\n\n\nCreate a highlighted layer\nA highlighted layer is the fewer-data version of the above points with (not necessarily the same) colors. To create this, we need some data manipulation. Let’s filter the data.\n\nd_sieved &lt;- filter(d, type == \"b\")\n\nThen the layer we want can be created like below. I call this operation as sieve (filter might be a better word, but I wanted to choose another word than dplyr’s verbs to avoid confusion).\n\nsieved_layer &lt;- geom_point(data = d_sieved, aes(x, y, colour = type),\n                           size = 10)\n\nIf we plot this, the result would be below:\n\nggplot() +\n  sieved_layer\n\n\n\n\n\n\n\n\n\n\nJoin the two layers\nNow we can draw the highlighted version of the plot as below:\n\nggplot() +\n  bleached_layer +\n  sieved_layer"
  },
  {
    "objectID": "post/2018-06-03-anatomy-of-gghighlight/index.html#by-point-vs-by-group",
    "href": "post/2018-06-03-anatomy-of-gghighlight/index.html#by-point-vs-by-group",
    "title": "Anatomy of gghighlight",
    "section": "“by point” vs “by group”",
    "text": "“by point” vs “by group”\nSo far, so good. Then, let’s consider a bit about the case when the geom is not point, but line.\nWhile points can be plotted one by one, lines cannot be drawn without the relationship between points. For example, haven’t you experienced an unexpected zigzag line?\n\nggplot(d, aes(x, y)) +\n  geom_line(size = 3)\n\nsize aesthetic has been deprecated for use with lines as of ggplot2 3.4.0\nℹ Please use linewidth aesthetic instead\nThis message is displayed once every 8 hours.\n\n\n\n\n\n\n\n\n\nLines need group variable, which indicates the series of data points.\n\nggplot(d, aes(x, y, group = type)) +\n  geom_line(size = 3)\n\n\n\n\n\n\n\n\nNote that group doesn’t need to be declared explicitly, as ggplot2 infers the groups from the specified variables. More precisely, it calculates group IDs based on the combination of discrete variables here. So, usually, specifying a discrete variable on colour or fill is enough.\n\nggplot(d, aes(x, y, colour = type)) +\n  geom_line(size = 3)\n\n\n\n\n\n\n\n\nAnyway, lines need groups. Accordingly, we need to consider the group when we sieve the data. Otherwise, the lines will be incomplete as this example:\n\n# data whose values are &gt;=10\nd_sieved2 &lt;- filter(d, value &gt;= 10)\n\nggplot() +\n  geom_line(data = d, aes(x, y, group = type), size = 3, colour = \"grey\") +\n  geom_line(data = d_sieved2, aes(x, y, colour = type), size = 3)\n\n\n\n\n\n\n\n\nSo, the correct way of doing this is to use group_by() and some aggregate functions like max() so that the calculations are done by group.\n\n# data series whose max values are &gt;=10\nd_sieved3 &lt;- d %&gt;%\n  group_by(type) %&gt;% \n  filter(max(value) &gt;= 10)\n\nggplot() +\n  geom_line(data = d, aes(x, y, group = type), size = 3, colour = \"grey\") +\n  geom_line(data = d_sieved3, aes(x, y, colour = type), size = 3)"
  },
  {
    "objectID": "post/2018-06-03-anatomy-of-gghighlight/index.html#prevent-unhighlighted-layer-from-facetted",
    "href": "post/2018-06-03-anatomy-of-gghighlight/index.html#prevent-unhighlighted-layer-from-facetted",
    "title": "Anatomy of gghighlight",
    "section": "Prevent unhighlighted layer from facetted",
    "text": "Prevent unhighlighted layer from facetted\nNext topic is facetting. Let’s naively facet the plot above.\n\nggplot() +\n  geom_line(data = d, aes(x, y, group = type), size = 3, colour = \"grey\") +\n  geom_line(data = d_sieved3, aes(x, y, colour = type), size = 3) +\n  facet_wrap(~ type)\n\n\n\n\n\n\n\n\nHmm…, unhighlighted lines are facetted. But, maybe we want the grey ones exists in all of the panels.\nfacet_*() facets all layers if the data contains the specified variable, in this case type. In other words, if the layer’s data doesn’t have the variable, it won’t get facetted. Let’s rename it.\n\nd_bleached &lt;- d\nnames(d_bleached)[3] &lt;- \"GROUP\"\n\nggplot() +\n  geom_line(data = d_bleached, aes(x, y, group = GROUP), size = 3, colour = \"grey\") +\n  geom_line(data = d_sieved3, aes(x, y, colour = type), size = 3) +\n  facet_wrap(~ type)\n\n\n\n\n\n\n\n\nYou may notice about one more good thing; the panel for \"a\" disappeared. This is because now d_sieved3 is the only data that contains type and it has only records of \"b\" and \"c\"."
  },
  {
    "objectID": "post/2018-06-03-anatomy-of-gghighlight/index.html#some-spoilers",
    "href": "post/2018-06-03-anatomy-of-gghighlight/index.html#some-spoilers",
    "title": "Anatomy of gghighlight",
    "section": "Some spoilers",
    "text": "Some spoilers\nThe next version of gghighlight will do the above things almost automatically. All you have to do is just adding gghighlight().\n\nlibrary(gghighlight)\n\nggplot(d, aes(x, y, colour = type)) +\n  geom_point(size = 10) +\n  gghighlight(type == \"b\")\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\n\nlabel_key: type\n\n\n\n\n\n\n\n\n\n\nggplot(d, aes(x, y, colour = type)) +\n  geom_line(size = 3) +\n  gghighlight(max(value) &gt;= 10)\n\nlabel_key: type\n\n\n\n\n\n\n\n\n\nStay tuned!"
  },
  {
    "objectID": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html",
    "href": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html",
    "title": "Tips To Turn R Markdown Into Slidev Presentation",
    "section": "",
    "text": "Slidev is a tool to create slides from Markdown. Recently, I used this with R Markdown and it was generally comfortable to use.\nThat said, there were some points I needed to google around to find solutions, so let me share these in this post.\n(You might wonder what this presentation talks about R and Rust, but the contents are all in Japanese, sorry. I’m preparing another blog post for this, so stay tuned!)"
  },
  {
    "objectID": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#getting-started",
    "href": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#getting-started",
    "title": "Tips To Turn R Markdown Into Slidev Presentation",
    "section": "Getting started",
    "text": "Getting started\nFirst of all, we need to create a project that has Slidev installed. This is done by:\nnpm init slidev\nThen, you’ll be asked several questions:\n❯ npm init slidev\nnpx: installed 22 in 3.269s\n\n  ●■▲\n  Slidev Creator  v0.19.6\n\n✔ Project name: … slidev-rmarkdown-test\n  Scaffolding project in slidev-rmarkdown-test ...\n  Done.\n\n✔ Install and start it now? … yes\n✔ Choose the agent › npm\n[ .................] / fetchMetadata: sill pacote version manifest for @nodelib/fs.scandir@2.1.5 fetched in 2883ms\nProject name will be the directory name of the new project, so choose a name that doesn’t exist yet.\nAfter finishing the installation, a web browser is launched and displays the template slides. These slides are generated from slides.md by a local Node.js server. slides.md is located in the top directory of the project created right now.\n❯ tree -L 1 slidev-rmarkdown-test\nslidev-rmarkdown-test\n|-- README.md\n|-- components\n|-- netlify.toml\n|-- node_modules\n|-- package-lock.json\n|-- package.json\n|-- slides.md\n`-- vercel.json\n\n2 directories, 6 files\nSo, what we should do next is obvious; let’s place slides.Rmd in the same directory so that we can render it to overwrite slides.md. The Node.js server detects the change on slides.md and regenerates slides from it on the fly.\nWe can stop the server with Ctrl+C on console. To launch again, execute this:\nnpm run dev"
  },
  {
    "objectID": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#choose-md_document",
    "href": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#choose-md_document",
    "title": "Tips To Turn R Markdown Into Slidev Presentation",
    "section": "Choose md_document",
    "text": "Choose md_document\nFirst of all, the target we want to generate is a Markdown file, so let’s specify md_document as the output.\n---\n# R Markdown metadata\ntitle: R Markdown to Slidev\noutput:\n  md_document:\n    variant: \"markdown_github\"\n---"
  },
  {
    "objectID": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#preserve-yaml-front-matter",
    "href": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#preserve-yaml-front-matter",
    "title": "Tips To Turn R Markdown Into Slidev Presentation",
    "section": "Preserve YAML front-matter",
    "text": "Preserve YAML front-matter\nWhat’s a bit tricky here is that Slidev also uses YAML front-matter for defining metadata. So, we need to put both items for R Markdown and those for Slidev together, and preserve it after rendering. Let’s specify preserve_yaml: true.\n---\n...\n  md_document:\n    variant: \"markdown_github\"\n    preserve_yaml: true\n---\nThen, put settings for Slidev. There might be some name collision between R Markdown and Slidev, but I don’t find it yet, fortunately. The full YAML would be like this:\n---\n# R Markdown metadata\ntitle: R Markdown to Slidev\noutput:\n  md_document:\n    variant: \"markdown_github\"\n    preserve_yaml: true\n\n# Slidev metadata\ntheme: seriph\nbackground: ./images/top.gif\nclass: 'text-center'\nhighlighter: shiki\ninfo: |\n  ## Use Slidev with R Markdown\n  \n  Source code can be found on &lt;https://github.com/yutannihilation/slidev-rmarkdown&gt;.\n---"
  },
  {
    "objectID": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#per-slide-yaml-front-matter",
    "href": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#per-slide-yaml-front-matter",
    "title": "Tips To Turn R Markdown Into Slidev Presentation",
    "section": "Per-slide YAML Front-matter",
    "text": "Per-slide YAML Front-matter\nOne of the great things with Slidev is that it’s very customizable. Per-slide YAML Front-matter is a good example of this; for example, we can specify layout per slide like this (image is a specific parameter for image-right layout):\n---\nlayout: image-right\nimage: './images/image.png'\n---\nThis will generate a slide like this:\n\n\n\nslides\n\n\nBut, the problem is, R Markdown (or probably underlying Pandoc?) seems to allow only one YAML front-matter. So, if we simply write\n---\nlayout: image-right\nimage: './images/image.png'\n---\nit will just disappear, alas… What can we do??\nWell, we can use Pandoc’s raw attribute to bypass the unnecessary conversion.\n```{=html}\n---\nlayout: image-right\nimage: './images/image.png'\n---\n```"
  },
  {
    "objectID": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#setting-base-url",
    "href": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#setting-base-url",
    "title": "Tips To Turn R Markdown Into Slidev Presentation",
    "section": "Setting Base URL",
    "text": "Setting Base URL\nThe base.url chunk option must be specified, otherwise the generated images will be broken. If the slides will be served on the root path (/), the setting should be like this:\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_knit$set(base.url = \"/\")\n```\n:::\nFor another example, if the presentation is served under some path like /slides/presentation1, then the base.url should be like this:\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_knit$set(base.url = \"/slides/presentation1/\")\n```\n:::"
  },
  {
    "objectID": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#add-blank-lines-between-custom-tags",
    "href": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#add-blank-lines-between-custom-tags",
    "title": "Tips To Turn R Markdown Into Slidev Presentation",
    "section": "Add blank lines between custom tags",
    "text": "Add blank lines between custom tags\nSlidev provides some custome tags. For example, &lt;v-click&gt; is a tag to apply annimations (c.f. https://sli.dev/guide/animations.html). But, for unknown reason, this won’t work (I don’t find the reason yet, but I think something is happening in conversions by Pandoc):\n&lt;v-click&gt;\n* item1\n&lt;/v-click&gt;\n&lt;v-click&gt;\n* item2\n&lt;/v-click&gt;\nIt seems we need to insert blank lines between the tags.\n&lt;v-click&gt;\n\n* item1\n\n&lt;/v-click&gt;\n\n&lt;v-click&gt;\n\n* item2\n\n&lt;/v-click&gt;"
  },
  {
    "objectID": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#deploy-to-github-pages",
    "href": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#deploy-to-github-pages",
    "title": "Tips To Turn R Markdown Into Slidev Presentation",
    "section": "Deploy to GitHub Pages",
    "text": "Deploy to GitHub Pages\nSlidev needs Node.js server to serve the slides, but it can also be exported as an standalone single-page application by the following command:\nnpm run build\nThe generated result goes to dist/, which can be deployed to GitHub Pages.\nSlidev also provides PDF export. For more details, please refer to the official document."
  },
  {
    "objectID": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#limitations",
    "href": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#limitations",
    "title": "Tips To Turn R Markdown Into Slidev Presentation",
    "section": "Limitations",
    "text": "Limitations\nWhile Slidev has many cool features, I doubt it can be an R package like revealjs or xaringan, as Slidev requires Node.js to build or run, which is not very portable. So, I’m not sure how useful this post is to R users, but hope you enjoy!"
  },
  {
    "objectID": "post/2021-06-07-gghighlight-032/index.html",
    "href": "post/2021-06-07-gghighlight-032/index.html",
    "title": "gghighlight 0.3.2",
    "section": "",
    "text": "gghighlight 0.3.2 is on CRAN now!\nThis release is mainly for fixing the potential test failures with upcoming version of ggplot2, but this version contains two new features."
  },
  {
    "objectID": "post/2021-06-07-gghighlight-032/index.html#n",
    "href": "post/2021-06-07-gghighlight-032/index.html#n",
    "title": "gghighlight 0.3.2",
    "section": "n()",
    "text": "n()\nSince gghighlight uses dplyr inside, you can now use dplyr’s expression, n(). This is useful to highlight based on the size of the group.\nSuppose we have this data:\n\nlibrary(gghighlight)\n\nLoading required package: ggplot2\n\nlibrary(dplyr, warn.conflicts = FALSE)\n\nset.seed(1098)\ncenters &lt;- tibble(\n  id = sample(letters, 11),\n  x = c(-1, -2, -3,   0,  1,  4,  2,  5,  7,  1, -1),\n  y = c( 4, -3, -7, -10, -8, -3,  9,  5, -1,  1, -1),\n  n = c(50, 50, 100, 50, 50, 120, 40, 10, 20, 5, 8)\n)\n\nd &lt;- centers %&gt;% \n  rowwise() %&gt;% \n  summarise(id = id, x = x + rnorm(n, sd = 3), y = y + rnorm(n, sd = 3))\n\np &lt;- ggplot(d, aes(x, y, colour = id)) + geom_point()\np\n\n\n\n\n\n\n\n\nBy using n(), we can focus on the large groups.\n\np +\n  gghighlight(n() &gt;= 100, use_direct_label = FALSE)\n\n\n\n\n\n\n\n\nOr small groups.\n\np +\n  gghighlight(n() &lt; 10, use_direct_label = FALSE)\n\n\n\n\n\n\n\n\nYou can also use n() as a non-logical predicate, whose values are used for sorting data and the top max_highlight of rows/groups are highlighted.\n\n# Same result as above\np +\n  gghighlight(-n(), max_highlight = 2, use_direct_label = FALSE)"
  },
  {
    "objectID": "post/2021-06-07-gghighlight-032/index.html#to-unhighlight-or-not-to-unhighlight",
    "href": "post/2021-06-07-gghighlight-032/index.html#to-unhighlight-or-not-to-unhighlight",
    "title": "gghighlight 0.3.2",
    "section": "To unhighlight or not to unhighlight…",
    "text": "To unhighlight or not to unhighlight…\nBy default, unhighlighted data are grayed out. unhighlighted_params is the option to override this. Now, you can even choose not to unhighlight at all by specifying explicit NULL to colour or fill!\n\np +\n  gghighlight(n() &lt; 10, use_direct_label = FALSE,\n              unhighlighted_params = list(colour = NULL))\n\n\n\n\n\n\n\n\nHmm…, but this is the very same plot as the original one. How can this be useful? Well, remember we still can tweak other parameters like alpha.\n\np +\n  gghighlight(n() &lt; 10, use_direct_label = FALSE,\n              unhighlighted_params = list(colour = NULL, alpha = 0.2))\n\n\n\n\n\n\n\n\nThis plot doesn’t look very nice in that the colors are a bit difficult to distinguish. This is mainly because I didn’t come up with some nice data, but it’s generally a tough job to tweak colors by alpha properly, so I don’t recommend this much. But, hope you can find some good use case for this!"
  },
  {
    "objectID": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html",
    "href": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html",
    "title": "Unofficial Introduction To extendr (Appendix I): Setup GitHub Actions CI",
    "section": "",
    "text": "extendr is a project that provides an interface between R and Rust. In the series of posts, I explained how to use extendr, but, this time, let me pick a complementary topic. The CI setup is important to develop a package and it’s not difficult to tweak the existing GitHub Actions (GHA) settings to compile Rust code. By using GHA, you can even provide precompiled binaries via GitHub releases."
  },
  {
    "objectID": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html#setup-rust-toolchain",
    "href": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html#setup-rust-toolchain",
    "title": "Unofficial Introduction To extendr (Appendix I): Setup GitHub Actions CI",
    "section": "Setup Rust toolchain",
    "text": "Setup Rust toolchain\nA GitHub repository for R package development typically has such a YAML for testing:\nhttps://github.com/r-lib/actions/blob/master/examples/check-standard.yaml\nTo test a package using extendr, it’s as easy as to just add the steps to setup Rust toolchain. Note that the runners might already have Rust toolchain installed, but these steps ensure the intended toolchain is used.\n- name: Set up Rust\n  uses: actions-rs/toolchain@v1\n  with:\n    toolchain: stable\n    default: true\n\n- name: Additional Rust set up for Windows\n  if: runner.os == 'Windows'\n  run: |\n    rustup target add i686-pc-windows-gnu\n    rustup target add x86_64-pc-windows-gnu\nIf you want to run tests also on the nightly toolchain, you can include the channel in the build matrix like this1:\n- {os: windows-latest, r: 'release', rust: 'stable'}\n- {os: macOS-latest,   r: 'release', rust: 'stable'}\n- {os: ubuntu-20.04,   r: 'release', rust: 'stable',  rspm: \"...\"}\n- {os: ubuntu-20.04,   r: 'devel',   rust: 'stable',  rspm: \"...\"}\n- {os: ubuntu-20.04,   r: 'release', rust: 'nightly', rspm: \"...\"}\nand specify the channel in the actions-rs/toolchain@v1 step:\n- name: Set up Rust\n  uses: actions-rs/toolchain@v1\n  with:\n    toolchain: ${{ matrix.config.rust }}\n    default: true\nYou might need more setups depending on the crates you use, but basically that’s all you need to do."
  },
  {
    "objectID": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html#provide-precompiled-binaries-for-windows",
    "href": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html#provide-precompiled-binaries-for-windows",
    "title": "Unofficial Introduction To extendr (Appendix I): Setup GitHub Actions CI",
    "section": "Provide precompiled binaries for Windows",
    "text": "Provide precompiled binaries for Windows\nAs you might have already noticed, the setup instruction for Windows is a bit complex compared to other OSes (i.e., Linux and macOS). So, it might be worth considering providing the precompiled static libraries, just like rwinlib does for many C++ libraries.\nOther motivation is that some of CRAN machines don’t have Rust toolchain. If the package author wants to submit their package to CRAN, such a mechanism is needed.\n(edit: it seems macOS is also the case, but I don’t find what’s the best way to solve this. I’ll probably write another post for this.)\nThere probably isn’t a single standard way to achieve this, but let me share what I did for my package, string2path here. YMMV, of course.\nI used softprops/action-gh-release action to publish the binaries as a GitHub release. The setting would be like this:\n# rename the binaries before uploading so that we can distinguish them easily.\n- name: Tweak staticlib\n  if: runner.os == 'Windows'\n  run: |\n    mv ./check/string2path.Rcheck/00_pkg_src/string2path/src-i386/rust/target/i686-pc-windows-gnu/release/libstring2path.a \\\n      i686-pc-windows-gnu-libstring2path.a\n    mv ./check/string2path.Rcheck/00_pkg_src/string2path/src-x64/rust/target/x86_64-pc-windows-gnu/release/libstring2path.a \\\n      x86_64-pc-windows-gnu-libstring2path.a\n\n- name: Release\n  uses: softprops/action-gh-release@v1\n  # only run this on a tag event\n  if: runner.os == 'Windows' && startsWith(github.ref, 'refs/tags/')\n  with:\n    fail_on_unmatched_files: true\n    files: |\n      i686-pc-windows-gnu-libstring2path.a\n      x86_64-pc-windows-gnu-libstring2path.a\nWith this setup, you can publish the binaries by pushing tags. For example, let’s create windows_20210801-3 tag and push it.\ngit tag windows_20210801-3\ngit push origin windows_20210801-3\nThen, the GHA will publish the corresponding release like this:\n\nNext, tweak src/Makefile.win as follows to allow users to download the binaries when cargo is not available. There might be more nicer code to choose the latest release automatically, but I think it’s safe to specify a fixed tag name, though it’s a bit tiresome to update this manually every time you update Rust code.\n(edit: I found this violates the CRAN policy. A package is not allowed to write “anywhere else on the file system apart from the R session’s temporary directory. You need to set CARGO_HOME envvar to some temporary directory to avoid this.)\nCRATE = foo   # your crate name\nBASE_TAG = windows_20210801-3  # the tag you want to use\n\n# c.f. https://stackoverflow.com/a/34756868\n# Note that this assignment (`:=`) is not available on Solaris, so you need to\n# add \"GNU make\" to SystemRequirements field on DESCRIPTION, even though this\n# can never compile on Solaris anyway...\nCARGO_EXISTS := $(shell cargo --version 2&gt; /dev/null)\n\n# ..snip...\n\n$(STATLIB):\nifdef CARGO_EXISTS\n    cargo build --target=$(TARGET) --lib --release --manifest-path=./rust/Cargo.toml\nelse\n    mkdir -p $(LIBDIR)\n    curl -L -o $(STATLIB) https://github.com/yutannihilation/$(CRATE)/releases/download/$(BASE_TAG)/$(TARGET)-lib$(CRATE).a\nendif\nOne caveat is that this won’t work when cargo is installed but with the GNU toolchain (extendr requires the MSVC toolchain on Windows). I guess some friendlier check can be done in configure.win, but this post won’t look into the details."
  },
  {
    "objectID": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html#other-topics-i-couldnt-cover",
    "href": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html#other-topics-i-couldnt-cover",
    "title": "Unofficial Introduction To extendr (Appendix I): Setup GitHub Actions CI",
    "section": "Other topics I couldn’t cover",
    "text": "Other topics I couldn’t cover\n\nsccache: Builds can be faster by using sccache, a ccache-like compiler caching tool for Rust. A blog post describes how to use this on GHA, but I don’t think I understand it to the extent where I can explain it here in clear words, sorry…\nHow to run tests on Rust’s side?: This post doesn’t explain how to run Rust tests (i.e., cargo test) or lints (i.e., cargo fmt, and cargo clippy). I even don’t figure out what tests should live in R or in Rust."
  },
  {
    "objectID": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html#example",
    "href": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html#example",
    "title": "Unofficial Introduction To extendr (Appendix I): Setup GitHub Actions CI",
    "section": "Example",
    "text": "Example\nHere’s the real example of the settings on my repo:\nhttps://github.com/yutannihilation/string2path/blob/main/.github/workflows/check-pak.yaml"
  },
  {
    "objectID": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html#footnotes",
    "href": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html#footnotes",
    "title": "Unofficial Introduction To extendr (Appendix I): Setup GitHub Actions CI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOn Windows, you need to use 'stalble-msvc', not 'stable-gnu', but msvc should be the default so you can just specify 'stable'↩︎"
  },
  {
    "objectID": "post/a-quick-note-about-how-to-bundle-rust/index.html",
    "href": "post/a-quick-note-about-how-to-bundle-rust/index.html",
    "title": "A Quick Note About How To Bundle Rust Crates For An R Package",
    "section": "",
    "text": "Recently, CRAN published a document titled “Using Rust in CRAN packages” and it refers to my R package, string2path, as an example of bundling all the source codes of the dependency Rust crates (I’m not sure if my name is still there at the point when you’re reading this). As I didn’t intend to develope the package as such a good example, let me explain about two possible pitfalls that you might not find easily just by reading the code.\nNote that, I don’t talk about the case that requires downloading here."
  },
  {
    "objectID": "post/a-quick-note-about-how-to-bundle-rust/index.html#my-stance",
    "href": "post/a-quick-note-about-how-to-bundle-rust/index.html#my-stance",
    "title": "A Quick Note About How To Bundle Rust Crates For An R Package",
    "section": "My stance",
    "text": "My stance\nFirst of all, while I’m hoping CRAN will success with Rust (so I sent some feedback about unclear points that everyone would wonder), I’d say, ultimately, CRAN is not a suitable place for using Rust. You should first consider R-universe to distribute your R pacakge using Rust. I believe R community needs a strong alternative to CRAN and R-univese can be. If you are interested in why I insist so, please read my blog post I wrote last year."
  },
  {
    "objectID": "post/a-quick-note-about-how-to-bundle-rust/index.html#cargo-vendor",
    "href": "post/a-quick-note-about-how-to-bundle-rust/index.html#cargo-vendor",
    "title": "A Quick Note About How To Bundle Rust Crates For An R Package",
    "section": "cargo vendor",
    "text": "cargo vendor\nBundling, or “vendoring,” all dependency crates for your package can be easily done by cargo vendor. More specifically,\n\nrun cargo vendor and\ncopy the configuration shown in the output of cargo vendor to .cargo/config.toml to use that vendored dependency\n\nYou can try adding --offline option to cargo build to check if the build really uses the vendored ones.\nWhile this mechanism is very simple, you can hit the following problems."
  },
  {
    "objectID": "post/a-quick-note-about-how-to-bundle-rust/index.html#path-length-limit-on-windows",
    "href": "post/a-quick-note-about-how-to-bundle-rust/index.html#path-length-limit-on-windows",
    "title": "A Quick Note About How To Bundle Rust Crates For An R Package",
    "section": "Path length limit on Windows",
    "text": "Path length limit on Windows\nVendored directories are so deep that you can easily hit with the path length limit on Windows. So, you probably cannot package the vendor directory as it is.\nThere can be some better solution, but I use tar command to avoid this failure; create a tar archive file and expand it in Makevars before cargo build. Here’s the command line that I actually use (ref):\n# c.f. https://reproducible-builds.org/docs/archives/\ntar \\\n  --sort=name \\\n  --mtime='1970-01-01 00:00:00Z' \\\n  --owner=0 \\\n  --group=0 \\\n  --numeric-owner \\\n  --xz \\\n  --create \\\n  --file=vendor.tar.xz \\\n  vendor\nAnother reason to use tar is that the size of these source codes. R package itself is compressed, but I found it’s more compact if I create another .xz file."
  },
  {
    "objectID": "post/a-quick-note-about-how-to-bundle-rust/index.html#r-cmd-check-considers-.cargo-as-a-hidden-directory",
    "href": "post/a-quick-note-about-how-to-bundle-rust/index.html#r-cmd-check-considers-.cargo-as-a-hidden-directory",
    "title": "A Quick Note About How To Bundle Rust Crates For An R Package",
    "section": "R CMD check considers .cargo as a “hidden directory”",
    "text": "R CMD check considers .cargo as a “hidden directory”\nAs I described above, you have to configure .cargo/config.toml to use the vendored sources. But, if you include it naively, you’ll see this NOTE on R CMD check because .cargo starts with .:\nFound the following hidden files and directories\nSo, you have to include .cargo/config.toml as a different name, and put it to the right place before cargo build. I do this like this (the file is named cargo_vendor_config.toml):\n    # vendoring (Note: to avoid NOTE of \"Found the following hidden files and\n    # directories\", .cargo needs to be created here)\n    if [ \"$(VENDORING)\" = \"yes\" ]; then \\\n        $(TAR) --extract --xz -f ./rust/vendor.tar.xz -C ./rust && \\\n        mkdir -p ./rust/.cargo && \\\n        cp ./cargo_vendor_config.toml ./rust/.cargo/config.toml; \\\n    fi\n\n    @BEFORE_CARGO_BUILD@ cd ./rust && cargo build --target=$(TARGET) --lib --release --offline\nThe full Makevars can be found here."
  },
  {
    "objectID": "post/a-tip-to-debug-ggplot2/index.html",
    "href": "post/a-tip-to-debug-ggplot2/index.html",
    "title": "A Tip to Debug ggplot2",
    "section": "",
    "text": "Since the tidyverse developer day is near, I share my very very secret technique to debug ggplot2. Though this is a very small thing, hope this helps someone a bit."
  },
  {
    "objectID": "post/a-tip-to-debug-ggplot2/index.html#ggplot2-is-unbreakable",
    "href": "post/a-tip-to-debug-ggplot2/index.html#ggplot2-is-unbreakable",
    "title": "A Tip to Debug ggplot2",
    "section": "ggplot2 is unbreakable!",
    "text": "ggplot2 is unbreakable!\nYou might want to debug() the methods of Geoms or Stats.\ndebug(GeomPoint$draw_panel)\nBut, this is not effective because the geom_point() generates different instances, so their draw_panel are all different objects (c.f. R6 classes have debug method for this). (edit: @BrodieGaslamtold me I’m wrong. The reason we can’t do debug(GeomPoint$draw_panel) s because $ is overridden and debug(get(\"draw_panel\", GeomPoint)) definitely works.)\nThen what about RStudio’s nice breakpoint features?\n\nUsually, this is enough. But, ggplot2’s ggprotos are not the case. You cannot use breakpoints to dig into them.\n\nHmm… But, no, you don’t need to scratch your head. The solution is pretty simple."
  },
  {
    "objectID": "post/a-tip-to-debug-ggplot2/index.html#use-browser",
    "href": "post/a-tip-to-debug-ggplot2/index.html#use-browser",
    "title": "A Tip to Debug ggplot2",
    "section": "Use browser()",
    "text": "Use browser()\nYou just need to\n\nadd browser() on the line where you want to debug, and\nload all (Cmd+Shift+L for Mac, Ctrl+Shift+L for Windows, and C-c C-w l for Emacs/ESS).\n\n\nThen, you’ll be on debug mode at last!"
  },
  {
    "objectID": "post/a-tip-to-debug-ggplot2/index.html#ymmv",
    "href": "post/a-tip-to-debug-ggplot2/index.html#ymmv",
    "title": "A Tip to Debug ggplot2",
    "section": "YMMV",
    "text": "YMMV\nThat’s all for this posts. But, I guess there are many alternative ways to achieve this, and I’m almost sure, at the end of the developer day, I will feel shame to have published this post, which just describes my debug skill is so poor… I’m really looking forward to learning from others. See you there!"
  },
  {
    "objectID": "post/double-dispatch-of-s3-method/index.html",
    "href": "post/double-dispatch-of-s3-method/index.html",
    "title": "Double dispatch of S3 method",
    "section": "",
    "text": "When I tried to define an S3 class that contains multiple ggplot objects, I’ve faced the lessor-know mechanism of S3 method dispatch, double dispatch."
  },
  {
    "objectID": "post/double-dispatch-of-s3-method/index.html#problem",
    "href": "post/double-dispatch-of-s3-method/index.html#problem",
    "title": "Double dispatch of S3 method",
    "section": "Problem",
    "text": "Problem\nTake a look at this example. manyplot class contains many plots, and displays them nicely when printted.\n\nlibrary(ggplot2)\n\nset.seed(100)\nd1 &lt;- data.frame(x = 1:100, y = cumsum(runif(100)))\nd2 &lt;- data.frame(x = 1:100, y = cumsum(runif(100)))\n\nplot_all &lt;- function(...) {\n  l &lt;- lapply(list(...), function(d) ggplot(d, aes(x, y)) + geom_line())\n  l &lt;- unname(l)\n  class(l) &lt;- \"manyplot\"\n  l\n}\n\nprint.manyplot &lt;- function(x, ...) {\n  do.call(gridExtra::grid.arrange, x)\n}\n\np &lt;- plot_all(d1, d2)\np\n\n\n\n\n\n\n\n\nSo far, so good.\nNext, I want to define + method, so that I can customize the plots just as I do with usual ggplot2.\n\n`+.manyplot` &lt;- function(e1, e2) {\n  l &lt;- lapply(e1, function(x) x + e2)\n  class(l) &lt;- \"manyplot\"\n  l\n}\n\nBut, this won’t work…\n\np + theme_bw()\n\nWarning: Incompatible methods (\"+.manyplot\", \"+.gg\") for \"+\"\n\n\nError in p + theme_bw(): non-numeric argument to binary operator\n\n\nWhat’s this cryptic error? To understand what happened, we need to dive into the concept of S3’s “double dispatch”"
  },
  {
    "objectID": "post/double-dispatch-of-s3-method/index.html#double-dispatch",
    "href": "post/double-dispatch-of-s3-method/index.html#double-dispatch",
    "title": "Double dispatch of S3 method",
    "section": "Double dispatch?",
    "text": "Double dispatch?\nUsually, S3’s method dispatch depends only on the type of first argument. But, in cases of some infix operators like + and *, it uses both of their arguments; this is called double dispatch.\nWhy is this needed? According to Advanced R:\n\nThis is necessary to preserve the commutative property of many operators, i.e. a + b should equal b + a.\n\nTo ensure this, if both a and b are S3 objects, the method chosen in a + b can be (c.f. how do_arith() works with S3 objects):\n\n\n\n\n\n\n\n\n\nDoes a have an S3 method?\nDoes b have an S3 method?\nAre the methods same?\nWhet method is chosen?\n\n\n\n\nyes\nyes\nyes\na’s method or b’s method (they are the same)\n\n\nyes\nyes\nno\ninternal method\n\n\nyes\nno\n-\na’s method\n\n\nno\nyes\n-\nb’s method\n\n\nno\nno\n-\ninternal method\n\n\n\nHere’s examples to show them clearly:\n\nfoo &lt;- function(x) structure(x, class = \"foo\")\n`+.foo` &lt;- function(e1, e2) message(\"foo!\")\n\nbar &lt;- function(x) structure(x, class = \"bar\")\n`+.bar` &lt;- function(e1, e2) message(\"bar?\")\n\n# both have the same S3 method\nfoo(1) + foo(1)\n\nfoo!\n\n\nNULL\n\n# both have different S3 methods\nfoo(1) + bar(1)\n\nWarning: Incompatible methods (\"+.foo\", \"+.bar\") for \"+\"\n\n\n[1] 2\nattr(,\"class\")\n[1] \"foo\"\n\n# `a` has a method, and `b` doesn't\nfoo() + 1\n\nError in structure(x, class = \"foo\"): argument \"x\" is missing, with no default\n\n# `b` has a method, and `a` doesn't\n1 + foo()\n\nError in structure(x, class = \"foo\"): argument \"x\" is missing, with no default\n\n# both don't have methods\nrm(`+.foo`)\nfoo(1) + foo(1)\n\n[1] 2\nattr(,\"class\")\n[1] \"foo\""
  },
  {
    "objectID": "post/double-dispatch-of-s3-method/index.html#explanation",
    "href": "post/double-dispatch-of-s3-method/index.html#explanation",
    "title": "Double dispatch of S3 method",
    "section": "Explanation",
    "text": "Explanation\nSo, now it’s clear to our eyes what happened in the code below; they have different methods (+.manyplot and +.gg) so it falled back to internal method. But, because fundamentally they are list, the internal mechanism refused to add these two objects…\n\np + theme_bw()"
  },
  {
    "objectID": "post/double-dispatch-of-s3-method/index.html#how-can-i-overcome-this",
    "href": "post/double-dispatch-of-s3-method/index.html#how-can-i-overcome-this",
    "title": "Double dispatch of S3 method",
    "section": "How can I overcome this?",
    "text": "How can I overcome this?\nHadley says ggplot2 might eventually end up using the double-dispatch approach in vctrs. So, we can wait for the last hope.\nIf you cannot wait, use S4. S4 can naturally do double dispatch because their method dispatch depends on the whole combination of types of the arguments."
  },
  {
    "objectID": "post/gather-and-spread-explained-by-gt/index.html",
    "href": "post/gather-and-spread-explained-by-gt/index.html",
    "title": "gather() and spread() Explained By gt",
    "section": "",
    "text": "This is episode 0 of my long adventure to multi-spread and multi-gather (this is my homework I got at the tidyverse developer day…). This post might seem to introduce the different semantics from the current tidyr’s one, but it’s probably just because my idea is still vague. So, I really appreciate any feedbacks!"
  },
  {
    "objectID": "post/gather-and-spread-explained-by-gt/index.html#tldr",
    "href": "post/gather-and-spread-explained-by-gt/index.html#tldr",
    "title": "gather() and spread() Explained By gt",
    "section": "tl;dr",
    "text": "tl;dr\nI now think gather() and spread() are about\n\ngrouping and\nenframe()ing and deframe()ing within each group\n\nDo you get what I mean? Let me explain step by step."
  },
  {
    "objectID": "post/gather-and-spread-explained-by-gt/index.html#what-does-gt-teach-us",
    "href": "post/gather-and-spread-explained-by-gt/index.html#what-does-gt-teach-us",
    "title": "gather() and spread() Explained By gt",
    "section": "What does gt teach us?",
    "text": "What does gt teach us?\nA while ago, gt package, Richard Iannone’s work-in-progress great work, was made public.\ngt package is wonderful, especially in that it makes us rethink about the possible semantics of columns. I mean, not all columns are equal. No, I don’t say anything new; this is what you already know with spread() and gather().\n\nspread()ed data explained\nTake a look at this example data, a simpler version of the one in ?gather:\n\nlibrary(tibble)\nlibrary(gt)\n\nset.seed(1)\n# example in ?gather\nstocks &lt;- tibble(\n  time = as.Date('2009-01-01') + 0:2,\n  X = rnorm(3, 0, 1),\n  Y = rnorm(3, 0, 2),\n  Z = rnorm(3, 0, 4)\n)\n\nstocks\n\n# A tibble: 3 × 4\n  time            X      Y     Z\n  &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 2009-01-01 -0.626  3.19   1.95\n2 2009-01-02  0.184  0.659  2.95\n3 2009-01-03 -0.836 -1.64   2.30\n\n\nHere, X, Y, and Z are the prices of stock X, Y, and Z. Of course, we can gather() the columns as this is the very example for this, but, we also can bundle these columns using tab_spanner():\n\ngt(stocks) %&gt;%\n  tab_spanner(\"price\", c(X, Y, Z))\n\n\n\n\n\n\n\ntime\nprice\n\n\nX\nY\nZ\n\n\n\n\n2009-01-01\n-0.6264538\n3.1905616\n1.949716\n\n\n2009-01-02\n0.1836433\n0.6590155\n2.953299\n\n\n2009-01-03\n-0.8356286\n-1.6409368\n2.303125\n\n\n\n\n\n\n\nYet another option is to specify groupname_col. We roughly think each row is a group and time is the grouping variable here:\n\ngt(stocks, groupname_col = \"time\")\n\n\n\n\n\n\n\nX\nY\nZ\n\n\n\n\n2009-01-01\n\n\n-0.6264538\n3.1905616\n1.949716\n\n\n2009-01-02\n\n\n0.1836433\n0.6590155\n2.953299\n\n\n2009-01-03\n\n\n-0.8356286\n-1.6409368\n2.303125\n\n\n\n\n\n\n\n\n\ngather()ed data explained\nLet’s see the gathered version next. Here’s the data:\n\nstocksm &lt;- stocks %&gt;%\n  tidyr::gather(\"name\", \"value\", X:Z)\n\nstocksm\n\n# A tibble: 9 × 3\n  time       name   value\n  &lt;date&gt;     &lt;chr&gt;  &lt;dbl&gt;\n1 2009-01-01 X     -0.626\n2 2009-01-02 X      0.184\n3 2009-01-03 X     -0.836\n4 2009-01-01 Y      3.19 \n5 2009-01-02 Y      0.659\n6 2009-01-03 Y     -1.64 \n7 2009-01-01 Z      1.95 \n8 2009-01-02 Z      2.95 \n9 2009-01-03 Z      2.30 \n\n\nThis can be represented in a similar way. This time, a group doesn’t consist of a single row, but the rows with the same grouping values. Accordingly, the grouping is the same as above.\n\nstocksm %&gt;%\n  gt(groupname_col = \"time\")\n\n\n\n\n\n\n\nname\nvalue\n\n\n\n\n2009-01-01\n\n\nX\n-0.6264538\n\n\nY\n3.1905616\n\n\nZ\n1.9497162\n\n\n2009-01-02\n\n\nX\n0.1836433\n\n\nY\n0.6590155\n\n\nZ\n2.9532988\n\n\n2009-01-03\n\n\nX\n-0.8356286\n\n\nY\n-1.6409368\n\n\nZ\n2.3031254\n\n\n\n\n\n\n\nYou can see the only difference is the rotation. So, theoretically, this can be implemented as grouping + rotating."
  },
  {
    "objectID": "post/gather-and-spread-explained-by-gt/index.html#do-it-yourself-by-enframe-and-deframe",
    "href": "post/gather-and-spread-explained-by-gt/index.html#do-it-yourself-by-enframe-and-deframe",
    "title": "gather() and spread() Explained By gt",
    "section": "Do it yourself by enframe() and deframe()",
    "text": "Do it yourself by enframe() and deframe()\nBefore entering into the implementations, I explain two tibble’s functions, enframe() and deframe() briefly. They can convert a vector to/from a two-column data.frame.\n\nlibrary(tibble)\n\nx &lt;- 1:3\nnames(x) &lt;- c(\"foo\", \"bar\", \"baz\")\n\nenframe(x)\n\n# A tibble: 3 × 2\n  name  value\n  &lt;chr&gt; &lt;int&gt;\n1 foo       1\n2 bar       2\n3 baz       3\n\n\n\ndeframe(enframe(x))\n\nfoo bar baz \n  1   2   3 \n\n\n\ngather()\nFirst, nest the data by time.\n\nd &lt;- dplyr::group_nest(stocks, time)\nd\n\n# A tibble: 3 × 2\n  time                     data\n  &lt;date&gt;     &lt;list&lt;tibble[,3]&gt;&gt;\n1 2009-01-01            [1 × 3]\n2 2009-01-02            [1 × 3]\n3 2009-01-03            [1 × 3]\n\n\nThen, coerce the columns of the 1-row data.frames to vectors. (In practice, we should check if the elements are all coercible.)\n\nd$data &lt;- purrr::map(d$data, ~ vctrs::vec_c(!!! .))\nd\n\n# A tibble: 3 × 2\n  time       data     \n  &lt;date&gt;     &lt;list&gt;   \n1 2009-01-01 &lt;dbl [3]&gt;\n2 2009-01-02 &lt;dbl [3]&gt;\n3 2009-01-03 &lt;dbl [3]&gt;\n\n\nLastly, enframe() the vectors and unnest the whole data.\n\nd$data &lt;- purrr::map(d$data, enframe)\nd\n\n# A tibble: 3 × 2\n  time       data            \n  &lt;date&gt;     &lt;list&gt;          \n1 2009-01-01 &lt;tibble [3 × 2]&gt;\n2 2009-01-02 &lt;tibble [3 × 2]&gt;\n3 2009-01-03 &lt;tibble [3 × 2]&gt;\n\n\n\ntidyr::unnest(d)\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(data)`\n\n\n# A tibble: 9 × 3\n  time       name   value\n  &lt;date&gt;     &lt;chr&gt;  &lt;dbl&gt;\n1 2009-01-01 X     -0.626\n2 2009-01-01 Y      3.19 \n3 2009-01-01 Z      1.95 \n4 2009-01-02 X      0.184\n5 2009-01-02 Y      0.659\n6 2009-01-02 Z      2.95 \n7 2009-01-03 X     -0.836\n8 2009-01-03 Y     -1.64 \n9 2009-01-03 Z      2.30 \n\n\nDone.\n\n\nspread()\nFirst step is the same as gather(). Just nest the data by time.\n\nd &lt;- dplyr::group_nest(stocksm, time)\nd\n\n# A tibble: 3 × 2\n  time                     data\n  &lt;date&gt;     &lt;list&lt;tibble[,2]&gt;&gt;\n1 2009-01-01            [3 × 2]\n2 2009-01-02            [3 × 2]\n3 2009-01-03            [3 × 2]\n\n\nThen, deframe() the data.frames. (In practice, we have to fill the missing rows to ensure all data.frames have the same variables.)\n\nd$data &lt;- purrr::map(d$data, deframe)\nd\n\n# A tibble: 3 × 2\n  time       data     \n  &lt;date&gt;     &lt;list&gt;   \n1 2009-01-01 &lt;dbl [3]&gt;\n2 2009-01-02 &lt;dbl [3]&gt;\n3 2009-01-03 &lt;dbl [3]&gt;\n\n\nThen, convert the vectors to data.frames.\n\nd$data &lt;- purrr::map(d$data, ~ tibble::tibble(!!! .))\nd\n\n# A tibble: 3 × 2\n  time       data            \n  &lt;date&gt;     &lt;list&gt;          \n1 2009-01-01 &lt;tibble [1 × 3]&gt;\n2 2009-01-02 &lt;tibble [1 × 3]&gt;\n3 2009-01-03 &lt;tibble [1 × 3]&gt;\n\n\nLastly, unnest the whole data.\n\ntidyr::unnest(d)\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(data)`\n\n\n# A tibble: 3 × 4\n  time            X      Y     Z\n  &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 2009-01-01 -0.626  3.19   1.95\n2 2009-01-02  0.184  0.659  2.95\n3 2009-01-03 -0.836 -1.64   2.30\n\n\nDone."
  },
  {
    "objectID": "post/gather-and-spread-explained-by-gt/index.html#whats-next",
    "href": "post/gather-and-spread-explained-by-gt/index.html#whats-next",
    "title": "gather() and spread() Explained By gt",
    "section": "What’s next?",
    "text": "What’s next?\nI’m not sure… I roughly believe this can be extended to multi-gather and multi-spread (groups can have multiple vectors and data.frames), but I’m yet to see how different (or same) this is from the current tidyr’s semantics. Again, any feedbacks are welcome!"
  },
  {
    "objectID": "post/gghighlight-0-2-0/index.html",
    "href": "post/gghighlight-0-2-0/index.html",
    "title": "gghighlight 0.2.0",
    "section": "",
    "text": "gghighlight 0.2.0 is on CRAN a while ago. This post briefly introduces the three new features. For basic usages, please refer to “Introduction to gghighlight”."
  },
  {
    "objectID": "post/gghighlight-0-2-0/index.html#keep_scales",
    "href": "post/gghighlight-0-2-0/index.html#keep_scales",
    "title": "gghighlight 0.2.0",
    "section": "keep_scales",
    "text": "keep_scales\nTo put it simply, gghighlight doesn’t drop any data points but drops their colours. This means, while non-colour scales (e.g. x, y and size) are kept as they are, colour scales get shrinked. This might be inconvenient when we want to compare the original version and the highlighted version, or the multiple highlighted versions.\n\nlibrary(gghighlight)\n\nLoading required package: ggplot2\n\nlibrary(patchwork)\n\nset.seed(3)\n\nd &lt;- data.frame(\n  value = 1:9,\n  category = rep(c(\"a\",\"b\",\"c\"), 3),\n  cont_var = runif(9),\n  stringsAsFactors = FALSE\n)\n\np &lt;- ggplot(d, aes(x = category, y = value, color = cont_var)) +\n  geom_point(size = 10) +\n  scale_colour_viridis_c()\n\np1 &lt;- p + ggtitle(\"original\")\np2 &lt;- p + \n  gghighlight(dplyr::between(cont_var, 0.3, 0.7),\n              use_direct_label = FALSE) +\n  ggtitle(\"highlighted\")\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\np1 * p2\n\n\n\n\n\n\n\n\nYou can see the colour of the points are different between the left plot and the right plot because the scale of the colours are different. In such a case, you can specify keep_scale = TRUE to keep the original scale (under the hood, gghighlight simply copies the original data to geom_blank()).\n\np3 &lt;- p +\n  gghighlight(dplyr::between(cont_var, 0.3, 0.7),\n              keep_scales = TRUE,\n              use_direct_label = FALSE) +\n  ggtitle(\"highlighted (keep_scale = TRUE)\")\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\np1 * p3"
  },
  {
    "objectID": "post/gghighlight-0-2-0/index.html#calculate_per_facet",
    "href": "post/gghighlight-0-2-0/index.html#calculate_per_facet",
    "title": "gghighlight 0.2.0",
    "section": "calculate_per_facet",
    "text": "calculate_per_facet\nWhen used with facet_*(), gghighlight() puts unhighlighted data on all facets and calculate the predicates on the whole data.\n\nSys.setlocale(locale = \"C\")\n\n[1] \"C\"\n\nset.seed(16)\n\nd &lt;- tibble::tibble(\n  day = rep(as.Date(\"2020-01-01\") + 0:89, times = 4),\n  month = lubridate::ceiling_date(day, \"month\"),\n  value = c(\n    cumsum(runif(90, -1.0, 1.0)),\n    cumsum(runif(90, -1.1, 1.1)),\n    cumsum(runif(90, -1.1, 1.0)),\n    cumsum(runif(90, -1.0, 1.1))\n  ),\n  id = rep(c(\"a\", \"b\", \"c\", \"d\"), each = 90)\n)\n\np &lt;- ggplot(d) +\n  geom_line(aes(day, value, colour = id)) +\n  facet_wrap(~ month, scales = \"free_x\")\n\np + \n  gghighlight(mean(value) &gt; 0, keep_scales = TRUE)\n\nlabel_key: id\n\n\n\n\n\n\n\n\n\nBut, it sometimes feels better to highlight facet by facet. For such a need, gghighlight() now has a new argument calculate_per_facet.\n\np + \n  gghighlight(mean(value) &gt; 0,\n              calculate_per_facet = TRUE,\n              keep_scales = TRUE)\n\nlabel_key: id\n\n\n\n\n\n\n\n\n\nNote that, as a general rule, only the layers before adding gghighlight() are modified. So, if you add facet_*() after adding gghighlight(), this option doesn’t work (though this behaviour might also be useful in some cases).\n\nggplot(d) +\n  geom_line(aes(day, value, colour = id)) +\n  gghighlight(mean(value) &gt; 0,\n              calculate_per_facet = TRUE,\n              keep_scales = TRUE) +\n  facet_wrap(~ month, scales = \"free_x\")\n\nlabel_key: id"
  },
  {
    "objectID": "post/gghighlight-0-2-0/index.html#unhighlighted_params",
    "href": "post/gghighlight-0-2-0/index.html#unhighlighted_params",
    "title": "gghighlight 0.2.0",
    "section": "unhighlighted_params",
    "text": "unhighlighted_params\ngghighlight() now allows users to override the parameters of unhighlighted data via unhighlighted_params. This idea was suggested by @ClausWilke.\n\n\nI think you could support a broader set of use cases if you allowed a list of aesthetics default values, like bleach_aes = list(colour = &quot;grey40&quot;, fill =&quot;grey80&quot;, size = 0.2).\n\n— Claus Wilke (@ClausWilke) July 4, 2018\n\n\nTo illustrate the original motivation, let’s use an example on the ggridges’ vignette. gghighlight can highlight almost any Geoms, but it doesn’t mean it can “unhighlight” arbitrary colour aesthetics automatically. In some cases, you need to unhighlight them manually. For example, geom_density_ridges() has point_colour.\n\nlibrary(ggplot2)\nlibrary(gghighlight)\nlibrary(ggridges)\n\np &lt;- ggplot(Aus_athletes, aes(x = height, y = sport, color = sex, point_color = sex, fill = sex)) +\n  geom_density_ridges(\n    jittered_points = TRUE, scale = .95, rel_min_height = .01,\n    point_shape = \"|\", point_size = 3, size = 0.25,\n    position = position_points_jitter(height = 0)\n  ) +\n  scale_y_discrete(expand = c(0, 0)) +\n  scale_x_continuous(expand = c(0, 0), name = \"height [cm]\") +\n  scale_fill_manual(values = c(\"#D55E0050\", \"#0072B250\"), labels = c(\"female\", \"male\")) +\n  scale_color_manual(values = c(\"#D55E00\", \"#0072B2\"), guide = \"none\") +\n  scale_discrete_manual(\"point_color\", values = c(\"#D55E00\", \"#0072B2\"), guide = \"none\") +\n  coord_cartesian(clip = \"off\") +\n  guides(fill = guide_legend(\n    override.aes = list(\n      fill = c(\"#D55E00A0\", \"#0072B2A0\"),\n      color = NA, point_color = NA)\n    )\n  ) +\n  ggtitle(\"Height in Australian athletes\") +\n  theme_ridges(center = TRUE)\n\np + \n  gghighlight(sd(height) &lt; 5.5)\n\nPicking joint bandwidth of 2.8\n\n\nPicking joint bandwidth of 2.23\n\n\n\n\n\n\n\n\n\nYou should notice that these vertical lines still have their colours. To grey them out, we can specify point_colour = \"grey80\" on unhighlighted_params (Be careful, point_color doesn’t work…).\n\np + \n  gghighlight(sd(height) &lt; 5.5, \n              unhighlighted_params = list(point_colour = \"grey80\"))\n\nPicking joint bandwidth of 2.8\n\n\nPicking joint bandwidth of 2.23\n\n\n\n\n\n\n\n\n\nunhighlighted_params is also useful when you want more significant difference between the highlighted data and unhighligted ones. In the following example, size and colour are set differently.\n\nset.seed(2)\nd &lt;- purrr::map_dfr(\n  letters,\n  ~ data.frame(\n      idx = 1:400,\n      value = cumsum(runif(400, -1, 1)),\n      type = .,\n      flag = sample(c(TRUE, FALSE), size = 400, replace = TRUE),\n      stringsAsFactors = FALSE\n    )\n)\n\nggplot(d) +\n  geom_line(aes(idx, value, colour = type), size = 5) +\n  gghighlight(max(value) &gt; 19,\n              unhighlighted_params = list(size = 1, colour = alpha(\"pink\", 0.4)))\n\nsize aesthetic has been deprecated for use with lines as of ggplot2 3.4.0\ni Please use linewidth aesthetic instead\nlabel_key: type\n\nThis message is displayed once every 8 hours."
  },
  {
    "objectID": "post/intro-to-savvy-part1/index.html",
    "href": "post/intro-to-savvy-part1/index.html",
    "title": "Introduction of Savvy, (Not Really) an Alternative to Extendr: Part 1",
    "section": "",
    "text": "For this half a year, I’ve been working on re-inventing the wheel of extendr. Now, I’m happy to announce it reached a usable state at last!🎉\nhttps://github.com/yutannihilation/savvy/\nBut, wait, I’m not writing this blog post to advertise my framework, savvy. This is not really an alternative to extendr, but just an explanatory material about the current and possible mechanism of extendr. Since extendr is so feature-rich that I cannot figure out the whole picture, I seriously needed a simple one to experiment."
  },
  {
    "objectID": "post/intro-to-savvy-part1/index.html#whats-savvy",
    "href": "post/intro-to-savvy-part1/index.html#whats-savvy",
    "title": "Introduction of Savvy, (Not Really) an Alternative to Extendr: Part 1",
    "section": "What’s “savvy”?",
    "text": "What’s “savvy”?\nAs you can guess from above, you can consider savvy as something like a degraded copy of extendr. It’s inconvenient and unfriendly. The name “savvy” comes from the fact that the pronunciation is similar to a Japanese word “錆,” which means “rust”. The name fits also because this is intended to be used by R-API-savvy people."
  },
  {
    "objectID": "post/intro-to-savvy-part1/index.html#part-1",
    "href": "post/intro-to-savvy-part1/index.html#part-1",
    "title": "Introduction of Savvy, (Not Really) an Alternative to Extendr: Part 1",
    "section": "“Part 1”?",
    "text": "“Part 1”?\nWhile I added “part 1” in the title of this post, I’m not sure if there will be part 2 and so on. But, there are at least three topics I wanted to experiment with savvy:\n\nError handling\nExternal SEXP and owned SEXP\nNo embedded usage\n\nFirst one is already covered in the previous posts, so I have little to add. One small update is that savvy now has CLI to generate C and R code automatically. I might write about it in future.\n\nR, Rust, Protect, And Unwinding\nDon’t panic!, We Can Unwind On Rust\n\nThird one is a boring topic to the ordinary users, and I don’t find a solution to the inconvenience it brings yet. So let’s skip it for now. If you are interested, you can join the discussion.\nSo, let’s focus on the second topic today."
  },
  {
    "objectID": "post/intro-to-savvy-part1/index.html#external-sexp-and-owned-sexp",
    "href": "post/intro-to-savvy-part1/index.html#external-sexp-and-owned-sexp",
    "title": "Introduction of Savvy, (Not Really) an Alternative to Extendr: Part 1",
    "section": "External SEXP and owned SEXP",
    "text": "External SEXP and owned SEXP\nThe below functions are both identity function for a character vector.\n\nextendr version\n#[extendr]\nfn identity_string(x: Vec&lt;String&gt;) -&gt; Vec&lt;String&gt; {\n    x\n}\n\n\nsavvy version\n#[savvy]\nfn identity_string(x: StringSxp) -&gt; Result&lt;SEXP&gt; {\n    let mut out = OwnedStringSxp::new(x.len());\n\n    for (i, e) in x.iter().enumerate() {\n        out.set_elt(i, e);\n    }\n\n    Ok(out.into())\n}\nAs you see, the savvy version is more redundant and esoteric. There are two types for wrapping a character vector in the code. The difference is:\n\nStringSxp : a read-only SEXP wrapper for objects passed to the function from outside.\nOwnedStringSxp a writable SEXP wrapper for objects created on Rust’s side.\n\nBut, why do they need separate types? I have two main reasons. Let me explain one by one."
  },
  {
    "objectID": "post/intro-to-savvy-part1/index.html#reason-1-avoid-unnecessary-protection",
    "href": "post/intro-to-savvy-part1/index.html#reason-1-avoid-unnecessary-protection",
    "title": "Introduction of Savvy, (Not Really) an Alternative to Extendr: Part 1",
    "section": "Reason 1: Avoid unnecessary protection",
    "text": "Reason 1: Avoid unnecessary protection\nI explained the basic concept of R’s protection mechanism in R, Rust, Protect, And Unwinding. So, if you are not familiar with these, it might be better to read it first.\nYeah, protection is important. But, we don’t need to protect what’s already protected. Actually, WRE says:\n\nFor functions from packages as well as R to safely co-operate in protecting objects, certain rules have to be followed:\n\nCaller protection. It is the responsibility of the caller that all arguments passed to a function are protected and will stay protected for the whole execution of the callee. (…snip…)\nProtecting return values. Any R objects returned from a function are unprotected (the callee must maintain pointer-protection balance), and hence should be protected immediately by the caller. (…snip…)\n\n(…snip…)\n\nSo, this means,\n\nR objects passed from the caller should be already protected because it’s the caller’s responsibility. Actually, if the input comes from the R session, the R objects are what belongs to some environment (e.g., global environment), which manes it never gets GCed accidentally.\nOn the other hand, R objects returned from R API functions like Rf_allocVector() are unprotected. It’s us who have to protect.\n\nIf we protect, we have to unprotect later. Since the object might outlive the Rust function call, unprotection is typically done in Drop trait like this. If we want to enable this destructor only on the R objects that we protected, i.e. the “owned” version, we need a separate type for each.\nimpl Drop for OwnedStringSxp {\n    fn drop(&mut self) {\n        protect::release_from_preserved_list(self.token);\n    }\n}"
  },
  {
    "objectID": "post/intro-to-savvy-part1/index.html#reason-2-avoid-unnecessary-altrep-checks",
    "href": "post/intro-to-savvy-part1/index.html#reason-2-avoid-unnecessary-altrep-checks",
    "title": "Introduction of Savvy, (Not Really) an Alternative to Extendr: Part 1",
    "section": "Reason 2: Avoid unnecessary ALTREP checks",
    "text": "Reason 2: Avoid unnecessary ALTREP checks\nFor INTSXP and REALSXP, we might be able to skip calling INTEGER_ELT() and REAL_ELT() by accessing the underlying C array of int (i32) or double (f64) directly. But, it’s not always the case that the SEXP has the underlying C array; it can be ALTREP. So, we typically need if branches for ALTREP case and other cases. For example, cpp11’s code:\n  return is_altrep_ ? INTEGER_ELT(data_, pos) : data_p_[pos];\n  if (is_altrep_) {\n    SET_INTEGER_ELT(data_, length_, value);\n  } else {\n    data_p_[length_] = value;\n  }\nBut, if the object is created by calling Rf_allocVector() by ourselves, it’s probably reasonable to assume it’s not ALTREP (I’m not sure if it’s guaranteed that Rf_allocVector() does and will always return non-ALTREP SEXP, though. Use this at your own risk!). Under the assumption, I can write a bit simpler code like this:\nimpl OwnedRealSxp {\n    pub fn new(len: usize) -&gt; Self {\n        let inner = unsafe { Rf_allocVector(REALSXP, len as _) };\n        let token = protect::insert_to_preserved_list(inner);\n        \n        // store the raw pointer\n        let raw = unsafe { REAL(inner) };\n\n        Self {\n            inner,\n            token,\n            len,\n            raw,\n        }\n    }\n    \n    pub fn elt(&mut self, i: usize) -&gt; f64 {\n        unsafe { *(self.raw.add(index)) }\n    }\n\n    pub fn set_elt(&mut self, i: usize, v: f64) {\n        unsafe { (self.raw.add(index)) = v }\n    }\n}\nThat said, if such a variable like is_altrep_ is stored on creation, it’s probably not very costly to check it every time. Also, this only matters on INTSXP and REALSXP (the internal representation of LGLSXP is int, not bool. STRSXP is a vector of CHARSXP). So, my direction might not be very clever. I’m yet to figure out."
  },
  {
    "objectID": "post/intro-to-savvy-part1/index.html#cpp11s-writable",
    "href": "post/intro-to-savvy-part1/index.html#cpp11s-writable",
    "title": "Introduction of Savvy, (Not Really) an Alternative to Extendr: Part 1",
    "section": "{cpp11}’s writable",
    "text": "{cpp11}’s writable\nThis design is very much inspired by the concept of cpp11’s writable. You know, unlike C/C++, Rust’s variable is immutable by default. So, we don’t need a separate type to represent the difference of mutability. Actually, extendr treats input as immutable without a separate type. But, above reasons, especially the first one, are still true .\nOne notable difference from cpp11’s writable is that a savvy vector doesn’t have the copy-on-write semantics (i.e., cannot be used for a function argument). This mainly is because = is not what you can overload in Rust. Here’s the list of overloadable operators. As you can see, there are traits like AddAssign and BitOrAssign, but no Assign or IndexAssign.\nhttps://doc.rust-lang.org/std/ops/index.html#traits\nBut, even if it were possible, I would avoid copy-on-write. I should avoid hidden memory allocation because allocation requires extreme carefulness during handling R objects in Rust."
  },
  {
    "objectID": "post/intro-to-savvy-part1/index.html#caveats",
    "href": "post/intro-to-savvy-part1/index.html#caveats",
    "title": "Introduction of Savvy, (Not Really) an Alternative to Extendr: Part 1",
    "section": "Caveats",
    "text": "Caveats\nAt the moment, I’m not sure whether this approach is better or worse than the current extendr’s implementation. Also, even if it’s confirmed as good, I have no idea how these ideas can be applied to extendr. Especially, I’m feeling this design works in exchange for allowing users to access raw SEXPs. At least, this requires users decent amount of knowledge about R’s C API (e.g., the protection mechanism). I’m afraid only “savvy” people can know how to use this.\nI’ll keep experimenting for a while and try to write more blog posts like this. I already ported my R package string2path from extendr to savvy. Please stay tuned."
  },
  {
    "objectID": "post/r-rust-protect-and-unwinding/index.html",
    "href": "post/r-rust-protect-and-unwinding/index.html",
    "title": "R, Rust, Protect, And Unwinding",
    "section": "",
    "text": "In the recent half a year, I’ve been struggling to understand how the extendr framework works. One of the things I found is that it’s extremely hard to protect and unprotect R objects properly from Rust’s side. Let me share my incomplete knowledges.\n(Disclaimer: I’m not an expert around here. My explanations and terms might be inaccurate or incorrect.)"
  },
  {
    "objectID": "post/r-rust-protect-and-unwinding/index.html#protect",
    "href": "post/r-rust-protect-and-unwinding/index.html#protect",
    "title": "R, Rust, Protect, And Unwinding",
    "section": "Protect",
    "text": "Protect\nFirst, let’s talk about how to protect R objects. Protect from what? From the garbage collection (GC) mechanism of R. Writing R Extension (WRE) says:\n\nThe memory allocated for R objects is not freed by the user; instead, the memory is from time to time garbage collected. That is, some or all of the allocated memory not being used is freed or marked as re-usable.\n\nSo, we have to claim the objects we use are in use, otherwise they might be accidentally freed while we are using it, which will causes serious problems. As far as I know, there are mainly 3 ways to do this “protection”.\n\nPROTECT() (or Rf_protect())\nThe most basic one is the PROTECT() macro. WRE says:\n\nIf you create an R object in your C code, you must tell R that you are using the object by using the PROTECT macro on a pointer to the object. This tells R that the object is in use so it is not destroyed during garbage collection\n\nPROTECT() takes an SEXP and returns the SEXP, so you can use this like the following example on WRE:\nSEXP ab;\nab = PROTECT(allocVector(REALSXP, 2));\nREAL(ab)[0] = 123.45;\nREAL(ab)[1] = 67.89;\nYou can also use PROTECT() in a separate line.\nSEXP ab;\nab = allocVector(REALSXP, 2);\nPROTECT(ab);\nREAL(ab)[0] = 123.45;\nREAL(ab)[1] = 67.89;\nWow, super simple. Now that it gets protected, we’ve all done, right? Well, no. You have to remove that PROTECT()ion when it gets no longer needed, otherwise your memory will be exhausted.\nThis can be done by UNPROTECT(). So, the full function definition would be like this:\nSEXP new_real2() {\n    SEXP ab;\n    ab = PROTECT(allocVector(REALSXP, 2));\n\n    REAL(ab)[0] = 123.45;\n    REAL(ab)[1] = 67.89;\n\n    UNPROTECT(1);\n    return ab;\n}\nHere, you might wonder why UNPROTECT() takes an integer while the corresponding function PROTECT() takes an SEXP. This is because the protection mechanism is stack-based. WRE says:\n\nThe protection mechanism is stack-based, so UNPROTECT(n) unprotects the last n objects which were protected.\n\nUNPROTECT() can unprotect only from the top of the stack. This means you cannot do something like “let’s return an object with protection and unprotect later in another function.” If something gets PROTECT()ed in a function, it’s required to be UNPROTECT() within the function. WRE says:\n\nCalls to PROTECT and UNPROTECT should balance in each function. A function may only call UNPROTECT or REPROTECT on objects it has itself protected.\n\nI don’t know well about the design of data structures, but I guess the advantages of being stack-based are\n\nfast\ncan be unwound (WRE says: “Note that the pointer protection stack balance is restored automatically on non-local transfer of control (..snip..) as if a call to UNPROTECT was invoked with the right argument.”)\n\n\n\nR_PreserveObject()\nWhile the rule of PROTECT() requires as above, we certainly have many valid cases that are not covered by this:\n\nWe want the object to live among multiple function calls\nWe want the object to live forever (i.e., until the R session ends)\n\nAn example of case 1 is to wrap SEXP objects in a C++ class, which should protect in its constructor and unprotect in its destructor (how to call the destructor properly is another headache, but let’s discuss later).\nFortunately, R provides R_PreserveObject() and the corresponding function R_ReleaseObject() for this. WRE says:\n\na call to R_PreserveObject adds an object to an internal list of objects not to be collects, and a subsequent call to R_ReleaseObject removes it from that list. This provides a way for objects which are not returned as part of R objects to be protected across calls to compiled code\n\nSo, it sounds like R_PreserveObject() supersedes PROTECT(). Why not use it all the time?? Well, while R_PreserveObject() is great, it’s slow. WRE says:\n\nIt is less efficient than the normal protection mechanism, and should be used sparingly.\n\nWhy slow? This is because how it’s implemented. The explanation in the related performance issue says:\n\nThis is a simple linked list, so has to be searched linearly to remove objects pushed on early.\n\nSo, are there any efficient ways to provide protection longer than one function call? The answer is simple. To prevent an SEXP from being considered as unused, we can actually use it!\n\n\nGet referenced by another SEXP\nWRE says:\n\nProtecting an R object automatically protects all the R objects pointed to in the corresponding SEXPREC, for example all elements of a protected list are automatically protected.\n\nTo put this into simpler words, if an R object already belongs to another R object (e.g., an element of a list), it doesn’t need the protection of PROTECT() or R_PreserveObject(). Actually, that’s why functions are supposed to return unprotected results; on the R session, the returned value is immediately assinged to a variable in some environment.\nWe can utilize this spec in several ways. A straightforward implementation of this is to have one big R_PreserveObject()ed list as the “anchor” and assign R objects to it. extendr uses this way (code). But, if this is done naively, it would also be inefficient as R_ReleaseObject() to search linearly which one to unprotect. So, extendr uses a hashmap nicely. Another reason of hashmap is that extendr allows Clone, so it must track the reference count on Rust’s side as well, but let’s not talk about the details here.\nMore sophisticated example is cpp11. It uses a doubly-linked-list approach (code), which is based on the suggestion in the issue above. This is more efficient when unprotecting. The C++ code is a bit advanced, so my naive implementation in Rust might be a bit easier to read.\n\n\nWhen to use which?\nIn summary, we have mainly 3 options to protect:\n\nPROTECT()\nR_PreserveObject()\nGet refrenced by another SEXP\n\nYou might wonder if we should always use case 3. But, if you look at the implementation of cpp11, it will easily remind you that PROTECT() is needed anyway until the object gets referenced. If you care about efficiency, you should use PROTECT() when it’s enough.\nIn my understanding, ideally we should\n\nUse PROTECT() everywhere as long as the protection is needed within the function\nUse R_PreserveObject() for objects that are never released during the R session\nUse the doubly-linked list when an object needs protection longer than one function call but shorter than one R session\n\nThis topic still has more room to discuss, but let’s move on as this is not the main one of this post!"
  },
  {
    "objectID": "post/r-rust-protect-and-unwinding/index.html#unwinding-and-longjmp",
    "href": "post/r-rust-protect-and-unwinding/index.html#unwinding-and-longjmp",
    "title": "R, Rust, Protect, And Unwinding",
    "section": "Unwinding and longjmp",
    "text": "Unwinding and longjmp\nWhat is unwinding? Honestly, I’m not confident what exactly this term refers to, but it seems it’s a cleanup process when some exception happens. For example, Wikipedia says:\n\nReturning from the called function will pop the top frame off the stack, perhaps leaving a return value. The more general act of popping one or more frames off the stack to resume execution elsewhere in the program is called stack unwinding and must be performed when non-local control structures are used, such as those used for exception handling.\n\nUsually, this explanation should be satisfying. But, as I’m talking about Rust, things are a bit more complicated. “non-local control structures” means panic!() to Rust, which we don’t try to catch in usual cases. Yes…, this is the core of the problem I’m writing. Let’s revisit later.\n\nDifferent languages have different unwinding mechanism\nLet’s forget about Rust. Suppose we want to use C++.\nC++ and Rust are the same in that it has difficulty to handle longjmp. C++’s class has destructor, which is called when the object is deleted. So, in theory, a C++ class of a wrapper of an R object can manage the protection with including the unprotecting operation in its destructor.\nHowever, the problem is that, the destructor can be called only in C++’s exception handling. If some R error, which is implemented using longjmp, happens on calling R’s C APIs, the destructor is not called. More details can be found the following blog post by the R core member:\nUse of C++ in Packages - The R Blog\n\nUnfortunately, RAII does not work with setjmp/longjmp functions provided by the C runtime for exception handling.\n\n\n\nR_UnwindProtect()\nFortunately, R’s C API provides a function for this, R_UnwindProtect(). This is something like tryCatch() at the C-level. The signature is:\nSEXP R_UnwindProtect(SEXP (*fun)(void *data), void *data,\n                     void (*clean)(void *data, Rboolean jump), void *cdata,\n                     SEXP cont);\nBasically, this is to wrap fun(data). If a longjmp error happens during the execution of fun(data), clean(cdata, TRUE) will be called before actually doing longjmp.\nIn the C++’s case, it is intended to throw C++ exception in clean() to let C++’s stack unwinding happen first. Then, R_ContinueUnwind(cont) can be used for moving back to C’s (or R’s) exception handling. cont is an R object created by R_MakeUnwindCont().\nFor a real example, cpp11’s implemntation is (code):\n  static SEXP token = [] {\n    SEXP res = R_MakeUnwindCont();\n    R_PreserveObject(res);\n    return res;\n  }();\n\n  std::jmp_buf jmpbuf;\n  if (setjmp(jmpbuf)) {\n    should_unwind_protect = TRUE;\n    throw unwind_exception(token);\n  }\n\n  SEXP res = R_UnwindProtect(\n      [](void* data) -&gt; SEXP {\n        auto callback = static_cast&lt;decltype(&code)&gt;(data);\n        return static_cast&lt;Fun&&&gt;(*callback)();\n      },\n      &code,\n      [](void* jmpbuf, Rboolean jump) {\n        if (jump == TRUE) {\n          // We need to first jump back into the C++ stacks because you can't safely\n          // throw exceptions from C stack frames.\n          longjmp(*static_cast&lt;std::jmp_buf*&gt;(jmpbuf), 1);\n        }\n      },\n      &jmpbuf, token);\nNote that this is a bit more complex than what WRE describes. As the comment says, this first jumps into the C++ stack by longjmp(), and throws the C++ exception there (the if branch with setjmp()). The error is caught by a try-catch block like this (a simplefied version of this original code):\nSEXP err = R_NilValue\ntry {\n\n  // ...snip...\n\n}\ncatch (cpp11::unwind_exception & e) {\n  err = e.token\n  R_ContinueUnwind(err);\n}\n\n\nSo, what about Rust?\nOkay, let’s come back to Rust. Rust also has destructor (i.e., Drop trait), so we’ll face the same problem. Can we survive with the same approach as C++?\nYes and no. Rust has a kind of try-catch, std::panic::catch_unwind() while its document says:\n\nIt is not recommended to use this function for a general try/catch mechanism.\n\nSo…, it’s kind of possible. Actually, extendr uses catch_unwind() (code). In the code below, the cleanup function do_cleanup() calls panic!(), which is caught by catch_unwind().\npub fn catch_r_error&lt;F&gt;(f: F) -&gt; Result&lt;SEXP&gt;\nwhere\n    F: FnOnce() -&gt; SEXP + Copy,\n    F: std::panic::UnwindSafe,\n{\n    // ...snip...\n\n    unsafe extern \"C\" fn do_cleanup(_: *mut raw::c_void, jump: Rboolean) {\n        if jump != 0 {\n            panic!(\"R has thrown an error.\");\n        }\n    }\n\n    unsafe {\n        let fun_ptr = do_call::&lt;F&gt; as *const ();\n        let clean_ptr = do_cleanup as *const ();\n        let x = false;\n        let fun = std::mem::transmute(fun_ptr);\n        let cleanfun = std::mem::transmute(clean_ptr);\n        let data = std::mem::transmute(&f);\n        let cleandata = std::mem::transmute(&x);\n        let cont = R_MakeUnwindCont();\n        Rf_protect(cont);\n\n        // Note that catch_unwind does not work for 32 bit windows targets.\n        let res = match std::panic::catch_unwind(|| {\n            R_UnwindProtect(fun, data, cleanfun, cleandata, cont)\n        }) {\n            Ok(res) =&gt; Ok(res),\n            Err(_) =&gt; Err(\"Error in protected R code\".into()),\n        };\n        Rf_unprotect(1);\n        res\n    }\n}\nIn general, panic!() should be avoided when the Rust function is called over FFI. panic!() causes unwinding, and cross-language unwinding is considered as undefined behavior (cf., Rust “ffi-unwind” project - FAQ).\nHowever, the problem is, we have to jump. We must escape from the cleanup function. Otherwise, R_ContinueUnwind(cont) will be called (code) automatically.\n    cleanfun(cleandata, jump);\n\n    if (jump)\n        R_ContinueUnwind(cont); \nRust doesn’t have longjmp / setjmp (rust-lang/rfcs#2625), so the last resort is panic!(). While we know it’s not good, there’s no option left as far as I know.\n(Update: I found I was wrong. We can get rid of panic!())\nI’m honestly not sure the current extendr’s implementation is really safe, but it seems we anyway need a similar implementation to get things work. (However, I think it’s probably a mistake that it doesn’t call R_ContinueUnwind(cont) to resume the unwinding process on C’s side.)"
  },
  {
    "objectID": "post/r-rust-protect-and-unwinding/index.html#my-take",
    "href": "post/r-rust-protect-and-unwinding/index.html#my-take",
    "title": "R, Rust, Protect, And Unwinding",
    "section": "My take",
    "text": "My take\nI’m concluding unwinding cannot be done correctly only with Rust. My take is\n\nRust functions should catch all R errors by R_UnwindProtect().\nRust functions should not use panic!() and panic_unwind() as a substitute for try-catch, but it seems there’s no other options.\nRust functions should never call Rf_errorcall() directly. Instead, it should return the error information to the C wrapper function, and accordingly the C function should call Rf_errorcall() (or R_ContinueUnwind()). Note that this is because it’s also possible to throw an error at R-level, but R_ContinueUnwind() is only possible at C-level.\n\nMy work-in-progress implementation using tagged pointer can be found here:\n\nRust code\nC code"
  },
  {
    "objectID": "post/r-rust-protect-and-unwinding/index.html#references",
    "href": "post/r-rust-protect-and-unwinding/index.html#references",
    "title": "R, Rust, Protect, And Unwinding",
    "section": "References",
    "text": "References\n\n5.9.1 Handling the effects of garbage collection - Writing R Extension\nCommon PROTECT Errors - The R Blog\nThe Rustonomicon - FFI and unwinding\nRust “ffi-unwind” project - FAQ"
  },
  {
    "objectID": "post/rust-and-cran-repository-policy/index.html",
    "href": "post/rust-and-cran-repository-policy/index.html",
    "title": "Rust and the CRAN Repository Policy",
    "section": "",
    "text": "One year ago, I succeeded to release an R package using Rust (string2path) on CRAN. After that, I wrote a blog post about how to use Rust in an R package, and I said\nbut it turned out it was not far enough. Enough for what? For satisfying the CRAN Repository Policy.\nLast month, I got an email titled “CRAN packages downloading rust files” from the CRAN maintainer. What it basically says was that my package violates the policy and will be removed from CRAN if I don’t correct it until 2022-08-10."
  },
  {
    "objectID": "post/rust-and-cran-repository-policy/index.html#what-went-wrong",
    "href": "post/rust-and-cran-repository-policy/index.html#what-went-wrong",
    "title": "Rust and the CRAN Repository Policy",
    "section": "What went wrong?",
    "text": "What went wrong?\nIn summary, there were three problems that I had to address (there was one more problem, but let’s ignore for now for simplicity).\n\nMy package downloads the Rust sources\nMy package doesn’t describe the authorship and copyright of the Rust sources in the DESCRIPTION file\nMy package downloads the pre-compiled binary without the agreement of the CRAN team\n\nLet’s look at the details one by one.\n\n1. downloads the Rust sources\nThe CRAN Policy says:\n\nWhere a package wishes to make use of a library not written solely for the package, the package installation should first look to see if it is already installed and if so is of a suitable version. In case not, it is desirable to include the library sources in the package and compile them as part of package installation. If the sources are too large, it is acceptable to download them as part of installation, but do ensure that the download is of a fixed version rather than the latest. Only as a last resort and with the agreement of the CRAN team should a package download pre-compiled software.\n\nIf you are familiar with Rust, you probably notice this doesn’t quite fit the Rust cases. The dependency Rust crates are not “installed” on the machine, but are resolved and downloaded automatically by Cargo, the Rust package manager. Usually, we can just include Cargo.lock, and then Cargo always downloads the fixed versions and verifies the checksums. But, that’s the rule. We should prevent Cargo from downloading any sources.\nThe solution is simple. We can use cargo vendor to include the sources of the dependencies. At first, I thought it was not realistic because my dependency was over 100MB. But, David B. Dahl, an author of another R package using Rust, kindly suggested we can compress them to a tarball.\nConverting them into a tarball is necessary also because otherwise we would get this warning:\nstoring paths of more than 100 bytes is not portable.\nMore details can be found in the following files:\n\nsrc/Makevars.in\nsrc/cargo_vendor_config.toml : this will be moved to .cargo/config.toml to use the vendored sources on compiling.\nsrc/rust/vendor.sh : this updates the tarball\n\n\n\n2. doesn’t describe the authorship and copyright\nThis was simply my oversight. The CRAN Policy says:\n\nThe ownership of copyright and intellectual property rights of all components of the package must be clear and unambiguous (including from the authors specification in the DESCRIPTION file). Where code is copied (or derived) from the work of others (including from R itself), care must be taken that any copyright/license statements are preserved and authorship is not misrepresented.\n…snip…\n(‘All components’ includes any downloaded at installation or during use.)\n\nFirst, let’s think about the authorship. It also explains how to describe the information:\n\nPreferably, an ‘Authors@R’ field would be used with ‘ctb’ roles for the authors of such code. Alternatively, the ‘Author’ field should list these authors as contributors.\nWhere copyrights are held by an entity other than the package authors, this should preferably be indicated via ‘cph’ roles in the ‘Authors@R’ field, or using a ‘Copyright’ field (if necessary referring to an inst/COPYRIGHTS file).\n\nThis is a bit complex; this requires to treat humans and non-human entities differently, but it’s a bit tough job to judge human and non-human one by one… Fortunately, it seems it’s a common practice to use either inst/AUTHORS or inst/COPYRIGHTS and write “see inst/… for details” in DESCRIPTION even when there’s both humans and non-humans. These files are explained in Writing R Extensions:\n\nAnother file sometimes needed in inst is AUTHORS or COPYRIGHTS to specify the authors or copyright holders when this is too complex to put in the DESCRIPTION file.\n\nThen, how about the copyright/license information? Typically, like R packages, a Rust crate describes the license in the metadata. So, rather than actually concatenating all the license statements, I thought it’s enough to summarize the licenses. But, how? Writing R Extensions says:\n\nTo include comments about the licensing rather than the body of a license, use a file named something like LICENSE.note.\n\nso it seems this LICENSE.note is a good place for this. Actually, several CRAN packages use this file. So, I hope this works.\nThe details can be found in the following files:\n\ninst/AUTHORS\nLICENSE.note\nupdate_authors.R : R script to generate the above two files from the `Cargo.toml` of the dependency crates.\n\n\n\n3. downloads the pre-compiled binary without the agreement of the CRAN team\nRegarding this one, I have no idea how to do this properly.\nFirst, let’s go back to the sentence of the CRAN Policy:\n\nOnly as a last resort and with the agreement of the CRAN team should a package download pre-compiled software.\n\nYes, I believe it’s a “last resort.” My package tries to compile the Rust code first, and only when no Rust compiler is available on the machine, it falls back to downloading the pre-compiled binary. It downloads the fixed version of the binary and verifies the checksum.\nBut…, how can I get “the agreement”? What state is considered they agree on the use of pre-compiled binary?\nI explained above in the cran-comments.md on my latest submission, in the hope that they would manually review it so that the acceptance means the agreement on downloading. However, my package went to CRAN soon after it passed the auto checks. The manual review never happened. So, while my package is still on CRAN at the time of writing this, I’m not sure if that means the problem is fixed."
  },
  {
    "objectID": "post/rust-and-cran-repository-policy/index.html#is-cran-suitable-for-rust",
    "href": "post/rust-and-cran-repository-policy/index.html#is-cran-suitable-for-rust",
    "title": "Rust and the CRAN Repository Policy",
    "section": "Is CRAN suitable for Rust?",
    "text": "Is CRAN suitable for Rust?\nHonestly, I was surprised that the CRAN Policy prohibits to rely on the standard mechanism of a language. At the same time, I do understand their stance. It’s a common conflict between the package managers.\nIn response to my email that mistakenly explained about the download mechanism (while the problem was not about how it downloads the binary), the CRAN maintainer wrote:\n\n&gt; That mechanism can be found in tools/configure.R.\n\nBut the comment in configure says that is for binary downloads. Your code is complicated, and I have spent far too long looking at it. As the CRAN policy says\n\n“The time of the volunteers is CRAN’s most precious resource”\n\nI’m really sorry that the maintainer had to read my messy code (although I never intended to force it). It’s almost impossible to check all the dependency management mechanism outside of R’s one no matter if it’s a major ecosystem like Rust or a minor tool like my script. So, I understand they need to be strict on this topic.\nFor example, Debian Rust Packaging Policy is stricter; it requires:\n\nPackage builds must not allow Cargo to access the network when building. In particular, they must not download or check out any sources at build time.\n\nIn Debian package’s case, if I understand correctly, it requires creating one Debian package per crate (but the packaging tool is provided so it shouldn’t be that difficult, I guess). Probably we can do the same thing on CRAN, but it feels a bit overkill.\nI still believe it’s possible to keep my package on CRAN, but I don’t casually recommend it to others. It requires considerable amount of efforts to comply with the CRAN Policy, at least at the moment."
  },
  {
    "objectID": "post/rust-and-cran-repository-policy/index.html#so-should-we-give-up-on-rust",
    "href": "post/rust-and-cran-repository-policy/index.html#so-should-we-give-up-on-rust",
    "title": "Rust and the CRAN Repository Policy",
    "section": "So…, should we give up on Rust?",
    "text": "So…, should we give up on Rust?\nTo be clear, I don’t think so.\nIn the context of Debian package, the distro’s official repository is not the only way to distribute a Debian package. It can be distributed via unofficial PPA; it’s “unofficial” in the sense it’s not provided by the distro, but it can be “official” if the developers of the software officially maintain the PPA.\nFor another example, Emacs has the official repository, GNU ELPA. But, the users are not tied to it because there is the popular alternative, MELPA. GNU ELPA has strict requirements, but the users can enjoy MELPA at the same time.\nI think R needs such an alternative to CRAN. I’m expecting R-universe will eventually be what MELPA is to ELPA. That would be a good thing to CRAN, too. Much of the frustration that we currently feel about CRAN probably comes from the fact that CRAN takes on too much responsibility.\nI actually use R-universe to distribute a non-CRAN package using Rust, and it works fine. If you don’t try R-universe yet, I recommend it (probably I’ll write tutorial for R-universe if I can find time)."
  },
  {
    "objectID": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html",
    "href": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html",
    "title": "Some more notes about using Rust code in R packages",
    "section": "",
    "text": "When I first tried to use Rust code within R package five years ago, it was like crawling in the dark and I wasted several days just to find I didn’t understand anything. But, now we have Using Rust code in R packages, a great presentation by Jeroen Ooms. It taught me almost everything! But still, I needed to learn myself some more things for my purpose. Let me leave some notes about those."
  },
  {
    "objectID": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html#passing-a-string-from-r-to-rust",
    "href": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html#passing-a-string-from-r-to-rust",
    "title": "Some more notes about using Rust code in R packages",
    "section": "Passing a string from R to Rust",
    "text": "Passing a string from R to Rust\nhellorust covers how to pass a string from Rust to R, but not the vice versa. I learned this from the code on clauswilke/sinab.\nFor example, let’s consider an improved version of hellorust::hello() that takes an argument name to say hello to.\n\nR code\nLet’s name it hello2.\nhello2 &lt;- function(name) {\n  .Call(hello_wrapper2, name)\n}\n\n\nC code\nhello_wrapper2 would be like the code below. STRING_ELT(x, i) takes i-th element of a character vector x, and Rf_translateCharUTF8() converts it to a pointer to the string encoded in UTF-8.\nSEXP hello_wrapper2(SEXP name){\n  char* res = string_from_rust2(Rf_translateCharUTF8(STRING_ELT(name, 0)));\n  return Rf_ScalarString(Rf_mkCharCE(res, CE_UTF8));\n}\n\n\napi.h\nThe string is passed as const char *.\nchar * string_from_rust2(const char *);\n\n\nRust code\nThe function takes the string as *const c_char. If we process the string in Rust code, we need to create a String. This is done by std::ffi::CStr::from_ptr(). CStr is a representation of a borrowed C string, and can be converted to String by to_string() or to_string_lossy(). Since this is an unsafe operation, it needs to be wrapped with unsafe.\nuse std;\nuse std::ffi::{CStr, CString};\nuse std::os::raw::c_char;\n\n// Utility function to convert c_char to string\nfn c_char_to_string(c: *const c_char) -&gt; String {\n    unsafe { CStr::from_ptr(c).to_string_lossy().into_owned() }\n}\n\n#[no_mangle]\npub extern fn string_from_rust2(c_name: *const c_char) -&gt; *const c_char {\n    let name = c_char_to_string(c_name);\n\n    let s = CString::new(format!(\"Hello {} !\", name)).unwrap();\n    let p = s.as_ptr();\n    std::mem::forget(s);\n    p\n}\n\n\nResult\nYou can view the diff here:\nhttps://github.com/r-rust/hellorust/commit/a42346c728a408fb1b2e6e7522082e19ec5b8a04"
  },
  {
    "objectID": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html#passing-a-vector-from-rust-to-r-or-vice-versa",
    "href": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html#passing-a-vector-from-rust-to-r-or-vice-versa",
    "title": "Some more notes about using Rust code in R packages",
    "section": "Passing a vector from Rust to R, or vice versa",
    "text": "Passing a vector from Rust to R, or vice versa\n(Update: this code is incomplete, please read the next section as well)\nIt took me some time to figure out how to handle arrays. I’m still not confident if I understand this correctly, but let me try to explain…\nWe cannot simply pass a variable length of vector to FFI because the length is not known. So, what we need to do is obvious; pass the data with the length at the same time. To do this, we need to define the same struct both in C and in Rust.\nSuppose we want to implement a function that takes one double vector and reverse it.\nIn api.h, let’s define a struct named Slice:\ntypedef struct\n{\n  double *data;  // since we want to process `REALSXP` here, the data type is `double`\n  uint32_t len;\n} Slice;\nand in Rust code define the same one. #[repr(C)] means “do what C does.” This is needed to match the alignment of the field with C.\nuse std::os::raw::{c_double, c_uint};\n\n#[repr(C)]\npub struct Slice {\n    data: *mut c_double,\n    len: c_uint,\n}\n\nR code\nThe R code is pretty simple.\nrev &lt;- function(x) {\n  x &lt;- as.double(x)\n  .Call(rev_wrapper, x)\n}\n\n\nC code\nWe need to allocate a REALSXP vector and copy the result into it.\nSEXP rev_wrapper(SEXP x){\n  Slice s = {REAL(x), Rf_length(x)};\n  Slice s_rev = rev_slice(s);\n\n  SEXP out = PROTECT(Rf_allocVector(REALSXP, s_rev.len));\n  for (int i = 0; i &lt; s_rev.len; i++) {\n    SET_REAL_ELT(out, i, s_rev.data[i]);\n  }\n  UNPROTECT(1);\n\n  return out;\n}\n\n\nRust code\nTo convert the Slice into Rust’s slice, we can use std::slice::from_raw_parts_mut. This is unsafe operation, so it needs to be wrapped with unsafe.\nslice and vector can be converted into an unsafe pointer by as_mut_ptr().\n#[no_mangle]\npub extern fn rev_slice(s: Slice) -&gt; Slice {\n    // convert from Slice to Rust slice\n    let s = unsafe { std::slice::from_raw_parts_mut(s.data, s.len as _) };\n\n    let mut v = s.to_vec();\n    v.reverse();\n    let len = v.len();\n\n    let v_ptr = v.as_mut_ptr();\n    std::mem::forget(v);\n\n    Slice {\n        data: v_ptr,\n        len: len as _,\n    }\n}\n\n\nResult\nYou can view the diff here:\nhttps://github.com/r-rust/hellorust/commit/e278d1541301ae18446bf1149a15d7aed868bd51"
  },
  {
    "objectID": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html#update-free-the-rust-allocated-memory",
    "href": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html#update-free-the-rust-allocated-memory",
    "title": "Some more notes about using Rust code in R packages",
    "section": "Update: free the Rust-allocated memory",
    "text": "Update: free the Rust-allocated memory\nThe code above works, but I noticed the memory is never freed. Yes, that’s because I forgot to free it. This was my nice lesson to learn that Rust is not always automatically saving me from doing silly things :P\nOf course we can free it, but it’s a bit tricky. Since Slice is allocated by Rust, it needs to be freed by Rust (c.f. How to return byte array from Rust function to FFI C? - help - The Rust Programming Language Forum). (IIUC, if the length is known in advance, it might be good idea to allocate on C’s side and pass it to the Rust, as the answer on the forum above suggests. rev() is the case, but let me explain the different one for now…)\n\nRust code\nLet’s define a Rust function to free the memory. Box::from_raw() constructs a Box, a pointer for heap allocation, from the raw pointer. After that, the raw pointer is owned by the box, which means it’s now Rust’s role to destruct it and free the memory.\n#[no_mangle]\npub extern \"C\" fn free_slice(s: Slice) {\n    // convert to Rust slice\n    let s = unsafe { std::slice::from_raw_parts_mut(s.data, s.len as _) };\n    let s = s.as_mut_ptr();\n    unsafe {\n        Box::from_raw(s);\n    }\n}\nI still don’t understand how to use Box properly, but it seems Sized structs can be handled simpler using Box in the argument: https://doc.rust-lang.org/std/boxed/index.html#memory-layout\n\n\nC code\nCall the function above from C to free the memory as soon as it’s no longer in use.\n// Need to include to use memcpy()\n#include &lt;string.h&gt;\n\n// ...snip...\n\nSEXP rev_wrapper(SEXP x){\n  Slice s = {REAL(x), Rf_length(x)};\n  Slice s_rev = rev_slice(s);\n\n  SEXP out = PROTECT(Rf_allocVector(REALSXP, s_rev.len));\n  memcpy(REAL(out), s_rev.data, s.len * sizeof(double));\n  free_slice(s_rev); // free!!!\n  UNPROTECT(1);\n\n  return out;\n}\n\n\nResult\nThe full diff is here:\nhttps://github.com/r-rust/hellorust/commit/97b3628b4a66eae9e25898a79ebf20fa59741063\n\n\nCan I do zero-copy?\nCopying memory to memory is not very cool, but it just works. I don’t know any nicer way yet. Apache Arrow seems a overkill for this simple usage, but will I need it in future…? Or flatbuffer? This seems a battle for another day, so I’ll stop here for now."
  },
  {
    "objectID": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html#precompiled-binary-for-windows",
    "href": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html#precompiled-binary-for-windows",
    "title": "Some more notes about using Rust code in R packages",
    "section": "Precompiled binary for Windows",
    "text": "Precompiled binary for Windows\nAs you might already notice, hellorust’s installation instruction for Windows is a bit long. But, do I really need to require the users to install cargo, just to compile my useless package? Now that we have GitHub Actions CI, maybe preparing a precompiled binary is a choice.\nHere’s the YAML I’m using to compile on windows runners and attach the binary on the releases (This creates a two separate releases for x86_64 and i686, which might be improved…).\non:\n  push:\n    tags:\n      - 'windows*'\n\nname: Build Windows\n\njobs:\n  build:\n    strategy:\n      matrix:\n        target:\n          - x86_64\n          - i686\n\n    name: build-${{ matrix.target }}-pc-windows-gnu\n\n    runs-on: windows-latest\n\n    steps:\n      - name: Checkout sources\n        uses: actions/checkout@v2\n\n      - name: Install stable toolchain\n        uses: actions-rs/toolchain@v1\n        with:\n          toolchain: stable\n          target: ${{ matrix.target }}-pc-windows-gnu\n          profile: minimal\n          default: true\n\n      - name: Run cargo build\n        uses: actions-rs/cargo@v1\n        with:\n          command: build\n          args: --release --target=${{ matrix.target }}-pc-windows-gnu --manifest-path=src/string2path/Cargo.toml\n\n      - name: List files\n        run: ls ./src/string2path/target/${{ matrix.target }}-pc-windows-gnu/release/\n        shell: bash\n\n      - name: Create Release\n        id: create_release\n        uses: actions/create-release@v1\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        with:\n          tag_name: ${{ github.ref }}-${{ matrix.target }}\n          release_name: Release ${{ github.ref }}-${{ matrix.target }}\n          draft: false\n          prerelease: true\n      - name: Upload Release Asset\n        id: upload-release-asset\n        uses: actions/upload-release-asset@v1\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        with:\n          upload_url: ${{ steps.create_release.outputs.upload_url }}\n          asset_path: ./src/string2path/target/${{ matrix.target }}-pc-windows-gnu/release/libstring2path.a\n          asset_name: libstring2path.a\n          asset_content_type: application/octet-stream\nIf there’s a precompiled binary, we can skip the compilation by tweaking Makevars.win like this:\nCRATE = string2path\n\n# Change this when created a new tag\nBASE_TAG = windows7\n\nTARGET = $(subst 64,x86_64,$(subst 32,i686,$(WIN)))\nLIBDIR = windows/$(TARGET)\nSTATLIB = $(LIBDIR)/lib$(CRATE).a\nPKG_LIBS = -L$(LIBDIR) -l$(CRATE) -lws2_32 -ladvapi32 -luserenv\n\nall: clean\n\n$(SHLIB): $(STATLIB)\n\n$(STATLIB):\n    mkdir -p $(LIBDIR)\n    # Not sure, but $@ doesn't seem to work here...\n    curl -L -o $(STATLIB) https://github.com/yutannihilation/$(CRATE)/releases/download/$(BASE_TAG)-$(TARGET)/lib$(CRATE).a\n\nclean:\n    rm -Rf $(SHLIB) $(STATLIB) $(OBJECTS)\nBy the way, at the time when hellorust was created, the extension of staticlib was .lib on Windows (MinGW), but recently (as of v1.44) this is changed to .a. Be careful."
  },
  {
    "objectID": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html#why-rust",
    "href": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html#why-rust",
    "title": "Some more notes about using Rust code in R packages",
    "section": "Why Rust?",
    "text": "Why Rust?\nLastly, let me answer to what some of you might wonder. I know you want me to say something like “memory safe” or “fast,” but…, it was just I was more familiar with Rust than C/C++.\nI just happened to learn Rust. I was searching for some alternative of Processing, a great creative coding framework, and I found nannou. At first, I didn’t expect I needed to learn Rust seriously, as the framework wraps the things very nicely. But, since nannou is still maturing, I found I needed to dive a bit deeper into the world of Rust to make things work on my environment. I’m now learning wgpu, a Rust implementation of WebGPU. If you are interested in, here’s some resources:\n\nLearn WGPU\nA Taste of WebGPU in Firefox - Mozilla Hacks - the Web developer blog"
  }
]