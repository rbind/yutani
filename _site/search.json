[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is where I struggle to learn about R (and improve my poor English)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wannabe Rstats-fu",
    "section": "",
    "text": "Rust\n\n\nextendr\n\n\n\n\nconfigure.win and configure\n\n\n\n\n\n\nSep 21, 2021\n\n\nHiroaki Yutani\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nRust\n\n\nextendr\n\n\n\n\nIntegrate R and Rust with extendr\n\n\n\n\n\n\nAug 1, 2021\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRust\n\n\nextendr\n\n\n\n\nIntegrate R and Rust with extendr\n\n\n\n\n\n\nJun 14, 2021\n\n\nHiroaki Yutani\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngghighlight\n\n\nggplot2\n\n\nR package\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2021\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRust\n\n\nextendr\n\n\n\n\nIntegrate R and Rust with extendr.\n\n\n\n\n\n\nJun 6, 2021\n\n\nHiroaki Yutani\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR Markdown\n\n\nSlidev\n\n\n\n\nSlidev is a tool to create slides from Markdown. This post is about some tips to use it in combination with R Markdown.\n\n\n\n\n\n\nJun 5, 2021\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRust\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2020\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngghighlight\n\n\nggplot2\n\n\npackage\n\n\n\n\ngghighlight 0.2.0 is released!\n\n\n\n\n\n\nFeb 17, 2020\n\n\nHiroaki Yutani\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ntidyr\n\n\ntidy data\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2019\n\n\nHiroaki Yutani\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nrlang\n\n\nWindows\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2019\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyr\n\n\ntidy data\n\n\ngt\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2019\n\n\nHiroaki Yutani\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npackage development\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2019\n\n\nHiroaki Yutani\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nggplot2\n\n\nmagick\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2018\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyeval\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2018\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyverse\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2018\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR internal\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2018\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2\n\n\nGIS\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2018\n\n\nHiroaki Yutani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngghighlight\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nJun 3, 2018\n\n\nHiroaki Yutani\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngghighlight\n\n\nggplot2\n\n\npackage\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2017\n\n\nHiroaki Yutani\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/2017-11-07-ggplot-add/index.html",
    "href": "post/2017-11-07-ggplot-add/index.html",
    "title": "An Example Usage of ggplot_add()",
    "section": "",
    "text": "A generic function ggplot_add() was added to ggplot2 by this PR:\nAllow addition of custom objects by thomasp85 · Pull Request #2309 · tidyverse/ggplot2\nI think creating a custom Geom or Stat and its constructor (geom_*() or stat_*()) is enough for the most of the extension packages of ggplot2, but some people, including me, need this."
  },
  {
    "objectID": "post/2017-11-07-ggplot-add/index.html#why-are-there-no-geom_highlight",
    "href": "post/2017-11-07-ggplot-add/index.html#why-are-there-no-geom_highlight",
    "title": "An Example Usage of ggplot_add()",
    "section": "Why are there no geom_highlight()?",
    "text": "Why are there no geom_highlight()?\nHere is an example code of my package gghighlight:\ngghighlight_point(d, aes(foo, bar), predicate = bar > 20 & baz == \"A\")\nYou may wonder why this can’t be written like this:\nggplot(d, aes(foo, bar)) +\n  geom_highlight_point(bar > 20 & baz == \"A\")\nLet me explain a bit."
  },
  {
    "objectID": "post/2017-11-07-ggplot-add/index.html#geom_stat_-doesnt-know-about-the-other-layers",
    "href": "post/2017-11-07-ggplot-add/index.html#geom_stat_-doesnt-know-about-the-other-layers",
    "title": "An Example Usage of ggplot_add()",
    "section": "geom_*()/stat_*() doesn’t know about the other layers",
    "text": "geom_*()/stat_*() doesn’t know about the other layers\ngeom_highlight_point(bar > 20 & baz == \"A\") is passed bar > 20 & baz == \"A\" without the data d, with which the expression should be evaluated. It needs d specified in ggplot(...).\nBut, considering the structure of the above code, geom_highlight_point(...) cannot access to the result of ggplot(...) in any usual way:\n`+`(ggplot(...), geom_highlight_point(...))\nIf ggplot2 were designed pipe-friendly, this\n`%>%`(ggplot(...), geom_highlight_point(...))\nwould be evaluated as this, which means geom_highlight_point(...) could take d from ggplot(...)…\ngeom_highlight_point(ggplot(...), ...)\nAnyway, let’s give up here. All I have to do is set this expression as an attribute of a custom Geom and pray that it will be evaluated with the proper data in the phase of building a plot."
  },
  {
    "objectID": "post/2017-11-07-ggplot-add/index.html#geomstat-doesnt-know-about-the-original-data",
    "href": "post/2017-11-07-ggplot-add/index.html#geomstat-doesnt-know-about-the-original-data",
    "title": "An Example Usage of ggplot_add()",
    "section": "Geom*/Stat* doesn’t know about the original data",
    "text": "Geom*/Stat* doesn’t know about the original data\nTake a look at the simplest example in the vignette “Extending ggplot2”:\nStatChull <- ggproto(\"StatChull\", Stat,\n  compute_group = function(data, scales) {\n    data[chull(data$x, data$y), , drop = FALSE]\n  },\n\n  required_aes = c(\"x\", \"y\")\n)\nYou may notice that compute_group() expects data has the fixed column x and y. Actually, data is not the original data but the one the mapping is already applied. So, there is no column bar and baz anymore; bar is renamed to y and baz is dropped. Oh no, I cannot evaluate bar > 20 & baz == \"A\" here, too…"
  },
  {
    "objectID": "post/2017-11-07-ggplot-add/index.html#ggplot_add",
    "href": "post/2017-11-07-ggplot-add/index.html#ggplot_add",
    "title": "An Example Usage of ggplot_add()",
    "section": "ggplot_add()",
    "text": "ggplot_add()\nLet’s remember this one:\n`+`(ggplot(...), geom_highlight_point(...))\nOnly + (or +.gg) can access to both ggplot(...) and geom_highlight_point(...). This means that, inside +.gg, bar > 20 & baz == \"A\" can be evaluated.\nSo…, should I implement my custom +.gg by myself? No, because I will be able to use ggplot_add()!\nggplot_add() is called in +.gg() via add_ggplot() (very confusing..) and is passed both the original plot and the new object. The current implementation is this:\n\"+.gg\" <- function(e1, e2) {\n  # Get the name of what was passed in as e2, and pass along so that it\n  # can be displayed in error messages\n  e2name <- deparse(substitute(e2))\n\n  if      (is.theme(e1))  add_theme(e1, e2, e2name)\n  else if (is.ggplot(e1)) add_ggplot(e1, e2, e2name)\n  else if (is.ggproto(e1)) {\n    stop(\"Cannot add ggproto objects together.\",\n         \" Did you forget to add this object to a ggplot object?\",\n         call. = FALSE)\n  }\n}\n(https://github.com/tidyverse/ggplot2/blob/7d0549a03e5ea08c27c768e88d5717f18cb4a5ce/R/plot-construction.r#L40-L52)\nadd_ggplot <- function(p, object, objectname) {\n  if (is.null(object)) return(p)\n\n  p <- plot_clone(p)\n  p <- ggplot_add(object, p, objectname)\n  set_last_plot(p)\n  p\n}\n(https://github.com/tidyverse/ggplot2/blob/7d0549a03e5ea08c27c768e88d5717f18cb4a5ce/R/plot-construction.r#L59-L66)\nBy using this, I can implement the proof-of-concept version of geom_highlight_point() as bellow:\ngeom_highlight_point <- function(expr) {\n  structure(list(expr = rlang::enquo(expr)), class = \"highlight\")\n}\n\nggplot_add.highlight <- function(object, plot, object_name) {\n  new_data <- dplyr::filter(plot$data, !! object$expr)\n  new_layer <- geom_point(data = new_data,\n                          mapping = plot$mapping,\n                          colour = alpha(\"red\", 0.5),\n                          size = 5)\n  plot$layers <- append(plot$layers, new_layer)\n  plot\n}\nlibrary(ggplot2)\n\nd <- data.frame(foo = 11:30, bar = 11:30, baz = rep(c(\"A\", \"B\", \"C\"), length.out = 20),\n                stringsAsFactors = FALSE)\n\nggplot(d, aes(foo, bar)) +\n  geom_highlight_point(bar > 20 & baz == \"A\") +\n  geom_point()  # for comparison\n\nI’m not sure if this is the intended usage of ggplot_add(), but this seems very nice. Looking forward to the next release of ggplot2!"
  },
  {
    "objectID": "post/2018-06-03-anatomy-of-gghighlight/index.html",
    "href": "post/2018-06-03-anatomy-of-gghighlight/index.html",
    "title": "Anatomy of gghighlight",
    "section": "",
    "text": "I’m overhauling my gghighlight package toward the upcoming release of ggplot2 2.3.0. I think I will introduce about the new gghighlight() soon, but before that, I want to write out the ideas behind gghighlight.\nNote that what I’ll write here contains few new things, as the basic idea is already covered by this great post:\nMy post is mainly for organizing my thought, yet I hope someone find this useful :)"
  },
  {
    "objectID": "post/2018-06-03-anatomy-of-gghighlight/index.html#data",
    "href": "post/2018-06-03-anatomy-of-gghighlight/index.html#data",
    "title": "Anatomy of gghighlight",
    "section": "Data",
    "text": "Data\nSuppose we have this data:\n\n\n\n\n\nx\ny\ntype\nvalue\n\n\n\n\n3\n3\na\n0\n\n\n8\n3\na\n1\n\n\n13\n3\na\n0\n\n\n2\n2\nb\n0\n\n\n7\n2\nb\n10\n\n\n12\n2\nb\n10\n\n\n1\n1\nc\n10\n\n\n6\n1\nc\n20\n\n\n11\n1\nc\n0"
  },
  {
    "objectID": "post/2018-06-03-anatomy-of-gghighlight/index.html#simple-plot",
    "href": "post/2018-06-03-anatomy-of-gghighlight/index.html#simple-plot",
    "title": "Anatomy of gghighlight",
    "section": "Simple plot",
    "text": "Simple plot\nIf we plot the data very simply, the code would be like this:\n\nlibrary(tidyverse)\n\nggplot(d, aes(x, y, colour = type)) +\n  geom_point(size = 10)\n\nWarning: `expand_scale()` was deprecated in ggplot2 3.3.0.\nPlease use `expansion()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated."
  },
  {
    "objectID": "post/2018-06-03-anatomy-of-gghighlight/index.html#highlighted-plot",
    "href": "post/2018-06-03-anatomy-of-gghighlight/index.html#highlighted-plot",
    "title": "Anatomy of gghighlight",
    "section": "Highlighted plot",
    "text": "Highlighted plot\nNow, what if we want to highlight only the points of records whose type are \"b\"?\nWe need two layers:\n\nunhighlighted layer\nhighlighted layer\n\n\nCreate an unhighlighted layer\nAn unhighlighted layer is the colorless version of the above points with the same data. To create this, we can simply remove colour from aes() and specify a static colour \"grey\". I call this operation as bleach.\n\nbleached_layer <- geom_point(data = d, aes(x, y),\n                             size = 10, colour = \"grey\")\n\nIf we plot this, the result would be below:\n\nggplot() +\n  bleached_layer\n\n\n\n\n\n\nCreate a highlighted layer\nA highlighted layer is the fewer-data version of the above points with (not necessarily the same) colors. To create this, we need some data manipulation. Let’s filter the data.\n\nd_sieved <- filter(d, type == \"b\")\n\nThen the layer we want can be created like below. I call this operation as sieve (filter might be a better word, but I wanted to choose another word than dplyr’s verbs to avoid confusion).\n\nsieved_layer <- geom_point(data = d_sieved, aes(x, y, colour = type),\n                           size = 10)\n\nIf we plot this, the result would be below:\n\nggplot() +\n  sieved_layer\n\n\n\n\n\n\nJoin the two layers\nNow we can draw the highlighted version of the plot as below:\n\nggplot() +\n  bleached_layer +\n  sieved_layer"
  },
  {
    "objectID": "post/2018-06-03-anatomy-of-gghighlight/index.html#by-point-vs-by-group",
    "href": "post/2018-06-03-anatomy-of-gghighlight/index.html#by-point-vs-by-group",
    "title": "Anatomy of gghighlight",
    "section": "“by point” vs “by group”",
    "text": "“by point” vs “by group”\nSo far, so good. Then, let’s consider a bit about the case when the geom is not point, but line.\nWhile points can be plotted one by one, lines cannot be drawn without the relationship between points. For example, haven’t you experienced an unexpected zigzag line?\n\nggplot(d, aes(x, y)) +\n  geom_line(size = 3)\n\nsize aesthetic has been deprecated for use with lines as of ggplot2 3.4.0\nℹ Please use linewidth aesthetic instead\nThis message is displayed once every 8 hours.\n\n\n\n\n\nLines need group variable, which indicates the series of data points.\n\nggplot(d, aes(x, y, group = type)) +\n  geom_line(size = 3)\n\n\n\n\nNote that group doesn’t need to be declared explicitly, as ggplot2 infers the groups from the specified variables. More precisely, it calculates group IDs based on the combination of discrete variables here. So, usually, specifying a discrete variable on colour or fill is enough.\n\nggplot(d, aes(x, y, colour = type)) +\n  geom_line(size = 3)\n\n\n\n\nAnyway, lines need groups. Accordingly, we need to consider the group when we sieve the data. Otherwise, the lines will be incomplete as this example:\n\n# data whose values are >=10\nd_sieved2 <- filter(d, value >= 10)\n\nggplot() +\n  geom_line(data = d, aes(x, y, group = type), size = 3, colour = \"grey\") +\n  geom_line(data = d_sieved2, aes(x, y, colour = type), size = 3)\n\n\n\n\nSo, the correct way of doing this is to use group_by() and some aggregate functions like max() so that the calculations are done by group.\n\n# data series whose max values are >=10\nd_sieved3 <- d %>%\n  group_by(type) %>% \n  filter(max(value) >= 10)\n\nggplot() +\n  geom_line(data = d, aes(x, y, group = type), size = 3, colour = \"grey\") +\n  geom_line(data = d_sieved3, aes(x, y, colour = type), size = 3)"
  },
  {
    "objectID": "post/2018-06-03-anatomy-of-gghighlight/index.html#prevent-unhighlighted-layer-from-facetted",
    "href": "post/2018-06-03-anatomy-of-gghighlight/index.html#prevent-unhighlighted-layer-from-facetted",
    "title": "Anatomy of gghighlight",
    "section": "Prevent unhighlighted layer from facetted",
    "text": "Prevent unhighlighted layer from facetted\nNext topic is facetting. Let’s naively facet the plot above.\n\nggplot() +\n  geom_line(data = d, aes(x, y, group = type), size = 3, colour = \"grey\") +\n  geom_line(data = d_sieved3, aes(x, y, colour = type), size = 3) +\n  facet_wrap(~ type)\n\n\n\n\nHmm…, unhighlighted lines are facetted. But, maybe we want the grey ones exists in all of the panels.\nfacet_*() facets all layers if the data contains the specified variable, in this case type. In other words, if the layer’s data doesn’t have the variable, it won’t get facetted. Let’s rename it.\n\nd_bleached <- d\nnames(d_bleached)[3] <- \"GROUP\"\n\nggplot() +\n  geom_line(data = d_bleached, aes(x, y, group = GROUP), size = 3, colour = \"grey\") +\n  geom_line(data = d_sieved3, aes(x, y, colour = type), size = 3) +\n  facet_wrap(~ type)\n\n\n\n\nYou may notice about one more good thing; the panel for \"a\" disappeared. This is because now d_sieved3 is the only data that contains type and it has only records of \"b\" and \"c\"."
  },
  {
    "objectID": "post/2018-06-03-anatomy-of-gghighlight/index.html#some-spoilers",
    "href": "post/2018-06-03-anatomy-of-gghighlight/index.html#some-spoilers",
    "title": "Anatomy of gghighlight",
    "section": "Some spoilers",
    "text": "Some spoilers\nThe next version of gghighlight will do the above things almost automatically. All you have to do is just adding gghighlight().\n\nlibrary(gghighlight)\n\nggplot(d, aes(x, y, colour = type)) +\n  geom_point(size = 10) +\n  gghighlight(type == \"b\")\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\n\nlabel_key: type\n\n\n\n\n\n\nggplot(d, aes(x, y, colour = type)) +\n  geom_line(size = 3) +\n  gghighlight(max(value) >= 10)\n\nlabel_key: type\n\n\n\n\n\nStay tuned!"
  },
  {
    "objectID": "post/2018-06-09-plot-osm-tiles/index.html",
    "href": "post/2018-06-09-plot-osm-tiles/index.html",
    "title": "Plot geom_sf() On OpenStreetMap Tiles",
    "section": "",
    "text": "mapview is a very nice package to explore an sf object. It can overlay sf object on the map images:\nBut, how can I do this with ggplot2? (My friend told me mapview::mapshot() can generate a PNG, but I want to do this with ggplot2!)"
  },
  {
    "objectID": "post/2018-06-09-plot-osm-tiles/index.html#rtfm",
    "href": "post/2018-06-09-plot-osm-tiles/index.html#rtfm",
    "title": "Plot geom_sf() On OpenStreetMap Tiles",
    "section": "RTFM",
    "text": "RTFM\nBefore anything, I need to read Tile Usage Policy to use the OpenStreetMap tiles. For “Requirements” section, this is important:\n\nClearly display license attribution.\n\nAccording to Copyright and License, it’s so simple as just adding this caption to my plots:\n\nlabs(caption = \"\\U00a9 OpenStreetMap contributors\")\n\nFor “Technical Usage Requirements” section, I have to read this more carefully. Let’s look at the requirements one by one.\n\nValid HTTP User-Agent identifying application. Faking another app’s User-Agent WILL get you blocked.\n\nOh, it seems I need to add User-Agent header. OK, let’s invent some nice name… If I use httr::GET(), the code will probably like this:\n\nGET(\n  \"https://...\",\n  add_headers(`User-Agent` = \"Yutani's blog post\")\n)\n\n\nIf known, a valid HTTP Referer.\n\nI don’t have Referers, so I skip this.\n\nDO NOT send no-cache headers. (“Cache-Control: no-cache”, “Pragma: no-cache” etc.)\n\nI do nothing other than swearing I’ll never use this header.\n\nCache Tile downloads locally according to HTTP Expiry Header, alternatively a minimum of 7 days.\n\nAh, this is important. Let’s implement later.\n\nMaximum of 2 download threads. (Unmodified web browsers’ download thread limits are acceptable.)\n\nFortunately, I’m not good at parallelism, so this is fine."
  },
  {
    "objectID": "post/2018-06-09-plot-osm-tiles/index.html#get-tile-urls",
    "href": "post/2018-06-09-plot-osm-tiles/index.html#get-tile-urls",
    "title": "Plot geom_sf() On OpenStreetMap Tiles",
    "section": "Get Tile URLs",
    "text": "Get Tile URLs\nAccording to Slippy map tilenames, the URL of a tile follows this format:\nhttps://[abc].tile.openstreetmap.org/zoom/x/y.png \nI have to fill these four parts:\n\n[abc]\nzoom\nx\ny\n\nLet’s look at these one by one.\n\n[abc]\n[abc] means there are three domains; a.tile.openstreetmap.org, b.tile.openstreetmap.org, c.tile.openstreetmap.org. But, why? It says:\n\nBrowser-based applications can thus request multiple tiles from multiple subdomains faster than from one subdomain.\n\nSo, as I’m not browser-based, I can choose arbitrary one.\n\n\nzoom\nZoom parameter is an integer number from 0 to 19. If zoom is 0, there’s only one tile. Likewise 2 x 2 tiles for zoom 1, 4 x 4 tiles for zoom 2, and so on. Then, which one should I choose? This can be roughly determined based on the size of the bbox (boundary box) of the sf object.\n\n# get the bbox\nb <- sf::st_bbox(nc)\nb\n\n     xmin      ymin      xmax      ymax \n-84.32385  33.88199 -75.45698  36.58965 \n\n# calculate the lengths of x and y of the bbox\nx_len <- b[\"xmax\"] - b[\"xmin\"]\ny_len <- b[\"ymax\"] - b[\"ymin\"]\n\n# calculate the minimum zoom level that is smaller than the lengths\nx_zoom <- sum(x_len < 360 / 2^(0:19)) - 1\ny_zoom <- sum(y_len < 170.1022 / 2^(0:19)) - 1\n\nzoom <- min(x_zoom, y_zoom)\nzoom\n\n[1] 5\n\n\nBut, since the tile is so small as 256 × 256 pixel, it’s often better to zoom more.\n\nzoom <- zoom + 2\n\n(I’m not sure how to do this correctly, I guess the zoom level should be determined by the size of the plot canvas, not the bbox.)\n\n\nx and y\nA pair of x and y represents the location of a tile. x corresponds to longitude, y to latitude. If the zoom is given, I can convert longitudes and latitudes to x and y according to the pseudo code on OpenStreetMap’s wiki:\n\nsec <- function(x) {\n  1 / cos(x)\n}\n\nlonlat2xy <- function(lat_deg, lon_deg, zoom) {\n  n <- 2^zoom\n\n  x <- (n * (lat_deg + 180)) %/% 360\n  lon_rad <- lon_deg * pi / 180\n  y <- (n * (1 - log(tan(lon_rad) + sec(lon_rad)) / pi)) %/% 2\n\n  list(x = x, y = y)\n}\n\nBut, how can I find the set of tiles which covers the whole bbox?\n\nlibrary(ggplot2)\n\np <- ggplot(nc) +\n  geom_sf() +\n  annotate(\"rect\", xmin = b[\"xmin\"], xmax = b[\"xmax\"], ymin = b[\"ymin\"], ymax = b[\"ymax\"],\n           colour = alpha(\"red\", 0.4), fill = \"transparent\", linetype = \"dashed\", size = 1.2)\n\nsize aesthetic has been deprecated for use with lines as of ggplot2 3.4.0\nℹ Please use linewidth aesthetic instead\nThis message is displayed once every 8 hours.\n\np\n\n\n\n\nThis problem can be simplified; I can focus only two corners, the north-west and the south-east. If I calculate which tiles of x and y those two points fall in, I can find the rest of the tiles by filling the sequences of x and y between these two tiles.\n\ncorners <- expand.grid(x = b[c(1, 3)], y = b[c(2, 4)])\n\np +\n  geom_point(aes(x, y), corners[2:3,], colour = \"red\", size = 5)\n\n\n\n\nHere’s the tiles:\n\nxy <- lonlat2xy(b[c(\"xmin\", \"xmax\")], b[c(\"ymin\", \"ymax\")], zoom)\n\ntiles <- expand.grid(x = seq(xy$x[\"xmin\"], xy$x[\"xmax\"]),\n                     y = seq(xy$y[\"ymin\"], xy$y[\"ymax\"]))\n\ntiles\n\n   x  y\n1 34 51\n2 35 51\n3 36 51\n4 37 51\n5 34 50\n6 35 50\n7 36 50\n8 37 50\n\n\n\n\nTile URLs\nFrom the results above, I can yield the URLs of the tiles:\n\nurls <- sprintf(\"https://a.tile.openstreetmap.org/%d/%d/%d.png\", zoom, tiles$x, tiles$y)\nurls\n\n[1] \"https://a.tile.openstreetmap.org/7/34/51.png\"\n[2] \"https://a.tile.openstreetmap.org/7/35/51.png\"\n[3] \"https://a.tile.openstreetmap.org/7/36/51.png\"\n[4] \"https://a.tile.openstreetmap.org/7/37/51.png\"\n[5] \"https://a.tile.openstreetmap.org/7/34/50.png\"\n[6] \"https://a.tile.openstreetmap.org/7/35/50.png\"\n[7] \"https://a.tile.openstreetmap.org/7/36/50.png\"\n[8] \"https://a.tile.openstreetmap.org/7/37/50.png\"\n\n\nOK, now I can download them. But, before that, let’s confirm that the tiles really cover the expected area.\n\n\nTile positions\nTo plot these tiles, I calculate the north-west corner of a tile by the following code (the pseudo code for this is also found on the wiki):\n\nxy2lonlat <- function(x, y, zoom) {\n  n <- 2^zoom\n\n  lon_deg <- x / n * 360.0 - 180.0\n  lat_rad <- atan(sinh(pi * (1 - 2 * y / n)))\n  lat_deg <- lat_rad * 180.0 / pi\n\n  list(lon_deg = lon_deg, lat_deg = lat_deg)\n}\n\nThen, south-east corners can be also calculated easily. Let’s calculate the both corners and bind them to a data.frame.\n\nlibrary(purrr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nnw_corners <- pmap_dfr(tiles, xy2lonlat, zoom = zoom)\n# add 1 to x and y to get the south-east corners\nse_corners <- pmap_dfr(mutate_all(tiles, `+`, 1), xy2lonlat, zoom = zoom)\n\nnames(nw_corners) <- c(\"xmin\", \"ymax\")\nnames(se_corners) <- c(\"xmax\", \"ymin\")\n\ntile_positions <- bind_cols(nw_corners, se_corners)\ntile_positions\n\n# A tibble: 8 × 4\n   xmin  ymax  xmax  ymin\n  <dbl> <dbl> <dbl> <dbl>\n1 -84.4  34.3 -81.6  32.0\n2 -81.6  34.3 -78.8  32.0\n3 -78.8  34.3 -75.9  32.0\n4 -75.9  34.3 -73.1  32.0\n5 -84.4  36.6 -81.6  34.3\n6 -81.6  36.6 -78.8  34.3\n7 -78.8  36.6 -75.9  34.3\n8 -75.9  36.6 -73.1  34.3\n\n\nNow I can plot the empty tiles as below:\n\np +\n  geom_point(aes(x, y), corners[2:3,], colour = \"red\", size = 5) +\n  geom_rect(data = tile_positions,\n            aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),\n            colour = \"blue\", fill = \"transparent\")\n\n\n\n\nYay, confirmed! Let’s proceed to the next step."
  },
  {
    "objectID": "post/2018-06-09-plot-osm-tiles/index.html#get-tile-data",
    "href": "post/2018-06-09-plot-osm-tiles/index.html#get-tile-data",
    "title": "Plot geom_sf() On OpenStreetMap Tiles",
    "section": "Get tile data",
    "text": "Get tile data\nIf I just get the response from a URL, httr::GET() is handy. But, this time, for the requirement of caching, I have to save the responses to disk first. So, I use curl::curl_download() here.\nNote that PNG data can be read into R session by png::readPNG().\n\nget_tile <- function(url) {\n  # build a local path\n  path <- stringr::str_extract(url, \"/\\\\d+/\\\\d+/\\\\d+.png\")\n  local_png <- here::here(file.path(\"data\", \"osm-tiles\", path))\n\n  if (!file.exists(local_png)) {\n    dir.create(dirname(local_png), showWarnings = FALSE, recursive = TRUE)\n    \n    # add header\n    h <- curl::new_handle()\n    curl::handle_setheaders(h, `User-Agent` = \"Yutani's blog post\")\n    \n    curl::curl_download(url, destfile = local_png)\n  }\n\n  png::readPNG(local_png)\n}\n\nThen, let’s get all tiles.\n\npngs <- map(urls, get_tile)"
  },
  {
    "objectID": "post/2018-06-09-plot-osm-tiles/index.html#plot-tiles",
    "href": "post/2018-06-09-plot-osm-tiles/index.html#plot-tiles",
    "title": "Plot geom_sf() On OpenStreetMap Tiles",
    "section": "Plot tiles",
    "text": "Plot tiles\nTo plot tiles, I use annotation_raster(), whose necessary arguments are:\n\nraster\nxmin\nxmax\nymin\nymax\n\nThe first one is pngs and the others are contained in tile_positions. So, let’s combine them so that I can use pmap().\n\nargs <- tile_positions %>%\n  mutate(raster = pngs)\n\nargs\n\n# A tibble: 8 × 5\n   xmin  ymax  xmax  ymin raster               \n  <dbl> <dbl> <dbl> <dbl> <list>               \n1 -84.4  34.3 -81.6  32.0 <dbl [256 × 256 × 3]>\n2 -81.6  34.3 -78.8  32.0 <dbl [256 × 256 × 3]>\n3 -78.8  34.3 -75.9  32.0 <dbl [256 × 256 × 3]>\n4 -75.9  34.3 -73.1  32.0 <dbl [256 × 256 × 3]>\n5 -84.4  36.6 -81.6  34.3 <dbl [256 × 256 × 3]>\n6 -81.6  36.6 -78.8  34.3 <dbl [256 × 256 × 3]>\n7 -78.8  36.6 -75.9  34.3 <dbl [256 × 256 × 3]>\n8 -75.9  36.6 -73.1  34.3 <dbl [256 × 256 × 3]>\n\n\nNow I can plot tiles at last.\n\nggplot(nc) +\n  pmap(args, annotation_raster, interpolate = TRUE) +\n  geom_sf(fill = alpha(\"red\", 0.3)) +\n  # don't forget the license notice!\n  labs(caption = \"\\U00a9 OpenStreetMap contributors\") +\n  theme_minimal()\n\n\n\n\nDone!\nBut, I didn’t expect the code would be this long… Maybe I need to create a package for this."
  },
  {
    "objectID": "post/2018-06-09-plot-osm-tiles/index.html#caveats",
    "href": "post/2018-06-09-plot-osm-tiles/index.html#caveats",
    "title": "Plot geom_sf() On OpenStreetMap Tiles",
    "section": "Caveats",
    "text": "Caveats\nNote that, I didn’t care about the CRS because nc’s CRS is fortunately EPSG 3857, which OpenStreetMap uses. If the sf object I want to plot has the different CRS, there may be a bit more to consider (and I don’t understand the CRS well…).\nUpdate:\nSorry, I was wrong… nc’s CRS is EPSG 4267 and OpenStreetMap tiles use EPSG 4326. Thanks Edzer for pointing this out!\n\nsf::st_crs(nc)\n\nCoordinate Reference System:\n  User input: NAD27 \n  wkt:\nGEOGCRS[\"NAD27\",\n    DATUM[\"North American Datum 1927\",\n        ELLIPSOID[\"Clarke 1866\",6378206.4,294.978698213898,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4267]]\n\n\nSo, I should have converted nc to the CRS first.\n\nnc_4326 <- sf::st_transform(nc, 4326)\n\nFortunately, the difference between EPSG 4267 and EPSG 4326 is rather negligible for this scale, so the result map should look almost same if I used nc_4326 instead of nc. Here’s the difference (can you see there’s a little red colors?):\n\nnc_4326_not_transformed <- sf::`st_crs<-`(nc, 4326)\n\nWarning: st_crs<- : replacing crs does not reproject data; use st_transform for\nthat\n\nggplot() +\n  geom_sf(data = nc_4326_not_transformed, fill = \"transparent\", colour = \"red\") +\n  geom_sf(data = nc_4326, fill = \"transparent\", colour = \"blue\") +\n  theme_minimal()"
  },
  {
    "objectID": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html",
    "href": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html",
    "title": "Tips To Turn R Markdown Into Slidev Presentation",
    "section": "",
    "text": "Slidev is a tool to create slides from Markdown. Recently, I used this with R Markdown and it was generally comfortable to use.\nThat said, there were some points I needed to google around to find solutions, so let me share these in this post.\n(You might wonder what this presentation talks about R and Rust, but the contents are all in Japanese, sorry. I’m preparing another blog post for this, so stay tuned!)"
  },
  {
    "objectID": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#getting-started",
    "href": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#getting-started",
    "title": "Tips To Turn R Markdown Into Slidev Presentation",
    "section": "Getting started",
    "text": "Getting started\nFirst of all, we need to create a project that has Slidev installed. This is done by:\nnpm init slidev\nThen, you’ll be asked several questions:\n❯ npm init slidev\nnpx: installed 22 in 3.269s\n\n  ●■▲\n  Slidev Creator  v0.19.6\n\n✔ Project name: … slidev-rmarkdown-test\n  Scaffolding project in slidev-rmarkdown-test ...\n  Done.\n\n✔ Install and start it now? … yes\n✔ Choose the agent › npm\n[ .................] / fetchMetadata: sill pacote version manifest for @nodelib/fs.scandir@2.1.5 fetched in 2883ms\nProject name will be the directory name of the new project, so choose a name that doesn’t exist yet.\nAfter finishing the installation, a web browser is launched and displays the template slides. These slides are generated from slides.md by a local Node.js server. slides.md is located in the top directory of the project created right now.\n❯ tree -L 1 slidev-rmarkdown-test\nslidev-rmarkdown-test\n|-- README.md\n|-- components\n|-- netlify.toml\n|-- node_modules\n|-- package-lock.json\n|-- package.json\n|-- slides.md\n`-- vercel.json\n\n2 directories, 6 files\nSo, what we should do next is obvious; let’s place slides.Rmd in the same directory so that we can render it to overwrite slides.md. The Node.js server detects the change on slides.md and regenerates slides from it on the fly.\nWe can stop the server with Ctrl+C on console. To launch again, execute this:\nnpm run dev"
  },
  {
    "objectID": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#choose-md_document",
    "href": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#choose-md_document",
    "title": "Tips To Turn R Markdown Into Slidev Presentation",
    "section": "Choose md_document",
    "text": "Choose md_document\nFirst of all, the target we want to generate is a Markdown file, so let’s specify md_document as the output.\n---\n# R Markdown metadata\ntitle: R Markdown to Slidev\noutput:\n  md_document:\n    variant: \"markdown_github\"\n---"
  },
  {
    "objectID": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#preserve-yaml-front-matter",
    "href": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#preserve-yaml-front-matter",
    "title": "Tips To Turn R Markdown Into Slidev Presentation",
    "section": "Preserve YAML front-matter",
    "text": "Preserve YAML front-matter\nWhat’s a bit tricky here is that Slidev also uses YAML front-matter for defining metadata. So, we need to put both items for R Markdown and those for Slidev together, and preserve it after rendering. Let’s specify preserve_yaml: true.\n---\n...\n  md_document:\n    variant: \"markdown_github\"\n    preserve_yaml: true\n---\nThen, put settings for Slidev. There might be some name collision between R Markdown and Slidev, but I don’t find it yet, fortunately. The full YAML would be like this:\n---\n# R Markdown metadata\ntitle: R Markdown to Slidev\noutput:\n  md_document:\n    variant: \"markdown_github\"\n    preserve_yaml: true\n\n# Slidev metadata\ntheme: seriph\nbackground: ./images/top.gif\nclass: 'text-center'\nhighlighter: shiki\ninfo: |\n  ## Use Slidev with R Markdown\n  \n  Source code can be found on <https://github.com/yutannihilation/slidev-rmarkdown>.\n---"
  },
  {
    "objectID": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#per-slide-yaml-front-matter",
    "href": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#per-slide-yaml-front-matter",
    "title": "Tips To Turn R Markdown Into Slidev Presentation",
    "section": "Per-slide YAML Front-matter",
    "text": "Per-slide YAML Front-matter\nOne of the great things with Slidev is that it’s very customizable. Per-slide YAML Front-matter is a good example of this; for example, we can specify layout per slide like this (image is a specific parameter for image-right layout):\n---\nlayout: image-right\nimage: './images/image.png'\n---\nThis will generate a slide like this:\n\n\n\nslides\n\n\nBut, the problem is, R Markdown (or probably underlying Pandoc?) seems to allow only one YAML front-matter. So, if we simply write\n---\nlayout: image-right\nimage: './images/image.png'\n---\nit will just disappear, alas… What can we do??\nWell, we can use Pandoc’s raw attribute to bypass the unnecessary conversion.\n```{=html}\n---\nlayout: image-right\nimage: './images/image.png'\n---\n```"
  },
  {
    "objectID": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#setting-base-url",
    "href": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#setting-base-url",
    "title": "Tips To Turn R Markdown Into Slidev Presentation",
    "section": "Setting Base URL",
    "text": "Setting Base URL\nThe base.url chunk option must be specified, otherwise the generated images will be broken. If the slides will be served on the root path (/), the setting should be like this:\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_knit$set(base.url = \"/\")\n```\n:::\nFor another example, if the presentation is served under some path like /slides/presentation1, then the base.url should be like this:\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_knit$set(base.url = \"/slides/presentation1/\")\n```\n:::"
  },
  {
    "objectID": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#add-blank-lines-between-custom-tags",
    "href": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#add-blank-lines-between-custom-tags",
    "title": "Tips To Turn R Markdown Into Slidev Presentation",
    "section": "Add blank lines between custom tags",
    "text": "Add blank lines between custom tags\nSlidev provides some custome tags. For example, <v-click> is a tag to apply annimations (c.f. https://sli.dev/guide/animations.html). But, for unknown reason, this won’t work (I don’t find the reason yet, but I think something is happening in conversions by Pandoc):\n<v-click>\n* item1\n</v-click>\n<v-click>\n* item2\n</v-click>\nIt seems we need to insert blank lines between the tags.\n<v-click>\n\n* item1\n\n</v-click>\n\n<v-click>\n\n* item2\n\n</v-click>"
  },
  {
    "objectID": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#deploy-to-github-pages",
    "href": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#deploy-to-github-pages",
    "title": "Tips To Turn R Markdown Into Slidev Presentation",
    "section": "Deploy to GitHub Pages",
    "text": "Deploy to GitHub Pages\nSlidev needs Node.js server to serve the slides, but it can also be exported as an standalone single-page application by the following command:\nnpm run build\nThe generated result goes to dist/, which can be deployed to GitHub Pages.\nSlidev also provides PDF export. For more details, please refer to the official document."
  },
  {
    "objectID": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#limitations",
    "href": "post/2021-06-05-tips-to-turn-r-markdown-into-slidev-presentation/index.html#limitations",
    "title": "Tips To Turn R Markdown Into Slidev Presentation",
    "section": "Limitations",
    "text": "Limitations\nWhile Slidev has many cool features, I doubt it can be an R package like revealjs or xaringan, as Slidev requires Node.js to build or run, which is not very portable. So, I’m not sure how useful this post is to R users, but hope you enjoy!"
  },
  {
    "objectID": "post/2021-06-06-unofficial-introduction-to-extendr-1-your-first-r-package-with-rust/index.html",
    "href": "post/2021-06-06-unofficial-introduction-to-extendr-1-your-first-r-package-with-rust/index.html",
    "title": "Unofficial Introduction To extendr (1): Your First R Package With Rust",
    "section": "",
    "text": "https://extendr.github.io/rextendr/articles/package.html"
  },
  {
    "objectID": "post/2021-06-07-gghighlight-032/index.html",
    "href": "post/2021-06-07-gghighlight-032/index.html",
    "title": "gghighlight 0.3.2",
    "section": "",
    "text": "gghighlight 0.3.2 is on CRAN now!\nThis release is mainly for fixing the potential test failures with upcoming version of ggplot2, but this version contains two new features."
  },
  {
    "objectID": "post/2021-06-07-gghighlight-032/index.html#n",
    "href": "post/2021-06-07-gghighlight-032/index.html#n",
    "title": "gghighlight 0.3.2",
    "section": "n()",
    "text": "n()\nSince gghighlight uses dplyr inside, you can now use dplyr’s expression, n(). This is useful to highlight based on the size of the group.\nSuppose we have this data:\n\nlibrary(gghighlight)\n\nLoading required package: ggplot2\n\nlibrary(dplyr, warn.conflicts = FALSE)\n\nset.seed(1098)\ncenters <- tibble(\n  id = sample(letters, 11),\n  x = c(-1, -2, -3,   0,  1,  4,  2,  5,  7,  1, -1),\n  y = c( 4, -3, -7, -10, -8, -3,  9,  5, -1,  1, -1),\n  n = c(50, 50, 100, 50, 50, 120, 40, 10, 20, 5, 8)\n)\n\nd <- centers %>% \n  rowwise() %>% \n  summarise(id = id, x = x + rnorm(n, sd = 3), y = y + rnorm(n, sd = 3))\n\np <- ggplot(d, aes(x, y, colour = id)) + geom_point()\np\n\n\n\n\nBy using n(), we can focus on the large groups.\n\np +\n  gghighlight(n() >= 100, use_direct_label = FALSE)\n\n\n\n\nOr small groups.\n\np +\n  gghighlight(n() < 10, use_direct_label = FALSE)\n\n\n\n\nYou can also use n() as a non-logical predicate, whose values are used for sorting data and the top max_highlight of rows/groups are highlighted.\n\n# Same result as above\np +\n  gghighlight(-n(), max_highlight = 2, use_direct_label = FALSE)"
  },
  {
    "objectID": "post/2021-06-07-gghighlight-032/index.html#to-unhighlight-or-not-to-unhighlight",
    "href": "post/2021-06-07-gghighlight-032/index.html#to-unhighlight-or-not-to-unhighlight",
    "title": "gghighlight 0.3.2",
    "section": "To unhighlight or not to unhighlight…",
    "text": "To unhighlight or not to unhighlight…\nBy default, unhighlighted data are grayed out. unhighlighted_params is the option to override this. Now, you can even choose not to unhighlight at all by specifying explicit NULL to colour or fill!\n\np +\n  gghighlight(n() < 10, use_direct_label = FALSE,\n              unhighlighted_params = list(colour = NULL))\n\n\n\n\nHmm…, but this is the very same plot as the original one. How can this be useful? Well, remember we still can tweak other parameters like alpha.\n\np +\n  gghighlight(n() < 10, use_direct_label = FALSE,\n              unhighlighted_params = list(colour = NULL, alpha = 0.2))\n\n\n\n\nThis plot doesn’t look very nice in that the colors are a bit difficult to distinguish. This is mainly because I didn’t come up with some nice data, but it’s generally a tough job to tweak colors by alpha properly, so I don’t recommend this much. But, hope you can find some good use case for this!"
  },
  {
    "objectID": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html",
    "href": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html",
    "title": "Unofficial Introduction To extendr (2): Type Conversion Between R and Rust",
    "section": "",
    "text": "extendr is a project that provides an interface between R and Rust. In the last post, I explained about how to create an R package with extendr briefly. This time, we’ll walk though how to handle various R types."
  },
  {
    "objectID": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#vector",
    "href": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#vector",
    "title": "Unofficial Introduction To extendr (2): Type Conversion Between R and Rust",
    "section": "Vector",
    "text": "Vector\nLet’s start with the last example in the last post.\n\n#[extendr]\nfn add(x: i32, y: i32) -> i32 {\n    x + y\n}\n\nWhile this works perfectly fine with a single value, this fails when the length is more than one.\n\nadd(1:2, 2:3)\n\nError in add(1:2, 2:3): Input must be of length 1. Vector of length >1 given.\n\n\nThis is very easy to fix. In Rust, we can use Vec<T> to represent a vector of values of type T.\n\n// I don't explain much about the Rust code this time, but, for now, please don't\n// worry if you can't understand what it does at the moment. Probably it's not\n// very important to understand this post. Move forward.\n\n#[extendr]\nfn add2(x: Vec<i32>, y: Vec<i32>) -> Vec<i32> {\n    x.iter().enumerate().map(|(i, x)| x + y[i]).collect()\n}\n\n\nadd2(1:2, 2:3)\n\n[1] 3 5\n\n\nEasy!\n\nWait, didn’t you say we can’t do this…!?\nSome of you might remember, in this post, I wrote\n\nWe cannot simply pass a variable length of vector\n\nfrom R to Rust.\nYeah, it’s true it was too difficult because I was struggling to do it via FFI! There’s no metadata available about the length or the structure of the data by default. But, with extendr, we can seamlessly access these metadata via R’s C API. So, in short, extendr is the game changer.\n\n\n&[T]\nIf you are already familiar with Rust, you might feel using Vec<T> as arguments looks a bit weird. In fact, the document of Vec<T> says:\n\nIn Rust, it’s more common to pass slices as arguments rather than vectors when you just want to provide read access. The same goes for String and &str.\n(https://doc.rust-lang.org/std/vec/struct.Vec.html#slicing)\n\nYes, you can use &[T] instead of Vec<T>, and this seems to matter on the performance slightly. If you are familiar with Rust to the extent that you know the difference between &[T] and Vec<T> (confession: I’m not!), you can should use &[T] instead. Otherwise, Vec<T> just works.\n\n#[extendr]\nfn add2_slice(x: &[i32], y: &[i32]) -> Vec<i32> {\n    x.iter().enumerate().map(|(i, x)| x + y[i]).collect()\n}\n\n\nadd2_slice(1:2, 2:3)\n\n[1] 3 5\n\n\nPlease note that this isn’t the reference to the original R object, just that to the copied values. If you really want no copying, you should use the “proxy” types, which I’ll cover in the next post."
  },
  {
    "objectID": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#na",
    "href": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#na",
    "title": "Unofficial Introduction To extendr (2): Type Conversion Between R and Rust",
    "section": "NA",
    "text": "NA\nOne more caveat about add() is that this cannot handle a missing value, NA.\n\nadd(1L, NA)\n\nError in add(1L, NA): unable to convert R object to primitive\n\n\nIn Rust, we can use Option<T> to represent an optional, or possibly missing, value.\n\n// pattern match is one of the most powerful things in Rust, btw!\n\n#[extendr]\nfn add3(x: Option<i32>, y: Option<i32>) -> Option<i32> {\n    match (x, y) {\n        (Some(x), Some(y)) => Some(x + y),\n        _ => NA_INTEGER\n    }\n}\n\nThis function can handle NA.\n\nadd3(1L, 2L)\n\n[1] 3\n\nadd3(1L, NA)\n\n[1] NA\n\n\nIt might be safe to always use Option since there’s always possibility that R value can be NA by nature. But, we might want to choose non-Option version to avoid the overhead (c.f. How much overhead is there with Options and Results? - The Rust Programming Language Forum), so it depends."
  },
  {
    "objectID": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#primitive-types",
    "href": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#primitive-types",
    "title": "Unofficial Introduction To extendr (2): Type Conversion Between R and Rust",
    "section": "Primitive types",
    "text": "Primitive types\nOkay, let’s learn about the primitive types at last. Here’s the corresponding table of R types and Rust types. We don’t have the direct equivalent of factor and complex here, but let’s talk about it later.\n\n\n\nR\nRust\n\n\n\n\ninteger\ni32\n\n\nnumeric\nf64\n\n\nlogical\nbool\n\n\ncharacter\nString &str\n\n\nfactor\n-\n\n\ncomplex\n-\n\n\n\n\ninteger and numeric\ninteger and numeric can mainly be converted into i32 and f64 respectively. I used “mainly” because it’s not that strict. They both can be converted into either of:\n\nu8\nu16\nu32\nu64\ni8\ni16\ni32\ni64\nf32\nf64\n\nSo, in other words, if you don’t want to prevent from numeric values are coerced into integers, you’ll need to check the types by yourself.\n\n\nlogical\nlogical is translated from/into bool. That’s all.\n\n\ncharacter\ncharacter is a bit tricky in that you can convert it to either of String and &str. You’ll probably have to scratch your head to understand the concept of “lifetime” to choose the proper one (confession: I still don’t understand it). But, in short,\n\nString : choose this when you modify the content strings\n&str: choose this (probably with 'static lifetime) when you only reference the strings\n\nIf you are not familiar with Rust yet, I recommend you to start with String. String is copied around so you might have unnecessary overhead, but it’s generally easier to handle because we need to think about the lifetimes less frequently.\n\n\nfactor\nTo put things simpler, until this point, I deliberately chose the cases when we have the corresponding types in Rust’s side. But, factor isn’t the case. It cannot be directly converted into a simple Rust type (at least at the moment). Instead, it can be cast into StrItr. StrItr is a “proxy” to the underlying data on R’s side.\nI’ll try explaining this in another post, but keep in mind that extendr provides that “proxy”-type of interface as well as the simple conversion to Rust’s primitive types."
  },
  {
    "objectID": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#list",
    "href": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#list",
    "title": "Unofficial Introduction To extendr (2): Type Conversion Between R and Rust",
    "section": "list",
    "text": "list\nThe corresponding Rust class for list is List. A List can be converted into HashMap<&str, Robj>. Be careful that R’s list can be a different data structure than Hashmap; it can have duplicated elements and unnamed elements.\n\nuse std::collections::HashMap;\n\n#[extendr]\nfn print_a(x: List) {\n    let x_hashmap: HashMap<&str, Robj> = x.into_hashmap();\n    \n    println!(\"{:?}\", x_hashmap.get(\"a\"));\n}\n\n\nprint_a(list(a = 1, b = 2))\nprint_a(list(b = 2))\n\nr! is a macro to create an R object from a Rust expression, by the way."
  },
  {
    "objectID": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#robj",
    "href": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#robj",
    "title": "Unofficial Introduction To extendr (2): Type Conversion Between R and Rust",
    "section": "Robj?",
    "text": "Robj?\nAs a sneak peak of the next post, let’s take a look at the usage of Robj.\nSo far, I created only functions that accepts just one type. What if we want to create a function that accepts multiple types of arguments? In this case, we can create a function that takes Robj as its argument and convert it by ourselves. Robj has many methods as_XXX() to convert to (or, more precisely, extract and copy the value of R object, and turn it into) a type. Here, let’s use as_integer() to generate Option<i32> .\n\n#[extendr]\nfn int(x: Robj) -> Option<i32> {\n    x.as_integer()\n}\n\n\n# integer\nint(1L)\n\n[1] 1\n\n# not integer-ish\nint(\"foo\")\n\n[1] NA"
  },
  {
    "objectID": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#whats-next",
    "href": "post/2021-06-14-unofficial-introduction-to-extendr-2-type-conversion-between-r-and-rust/index.html#whats-next",
    "title": "Unofficial Introduction To extendr (2): Type Conversion Between R and Rust",
    "section": "What’s next?",
    "text": "What’s next?\nIn this post, I focused mainly the Rust’s side of the type ecosystem. Next, I probably need to write about more R-ish things like Function or Symbol , which I need some time to understand correctly. Stay tuned…"
  },
  {
    "objectID": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html",
    "href": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html",
    "title": "Unofficial Introduction To extendr (Appendix I): Setup GitHub Actions CI",
    "section": "",
    "text": "extendr is a project that provides an interface between R and Rust. In the series of posts, I explained how to use extendr, but, this time, let me pick a complementary topic. The CI setup is important to develop a package and it’s not difficult to tweak the existing GitHub Actions (GHA) settings to compile Rust code. By using GHA, you can even provide precompiled binaries via GitHub releases."
  },
  {
    "objectID": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html#setup-rust-toolchain",
    "href": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html#setup-rust-toolchain",
    "title": "Unofficial Introduction To extendr (Appendix I): Setup GitHub Actions CI",
    "section": "Setup Rust toolchain",
    "text": "Setup Rust toolchain\nA GitHub repository for R package development typically has such a YAML for testing:\nhttps://github.com/r-lib/actions/blob/master/examples/check-standard.yaml\nTo test a package using extendr, it’s as easy as to just add the steps to setup Rust toolchain. Note that the runners might already have Rust toolchain installed, but these steps ensure the intended toolchain is used.\n- name: Set up Rust\n  uses: actions-rs/toolchain@v1\n  with:\n    toolchain: stable\n    default: true\n\n- name: Additional Rust set up for Windows\n  if: runner.os == 'Windows'\n  run: |\n    rustup target add i686-pc-windows-gnu\n    rustup target add x86_64-pc-windows-gnu\nIf you want to run tests also on the nightly toolchain, you can include the channel in the build matrix like this1:\n- {os: windows-latest, r: 'release', rust: 'stable'}\n- {os: macOS-latest,   r: 'release', rust: 'stable'}\n- {os: ubuntu-20.04,   r: 'release', rust: 'stable',  rspm: \"...\"}\n- {os: ubuntu-20.04,   r: 'devel',   rust: 'stable',  rspm: \"...\"}\n- {os: ubuntu-20.04,   r: 'release', rust: 'nightly', rspm: \"...\"}\nand specify the channel in the actions-rs/toolchain@v1 step:\n- name: Set up Rust\n  uses: actions-rs/toolchain@v1\n  with:\n    toolchain: ${{ matrix.config.rust }}\n    default: true\nYou might need more setups depending on the crates you use, but basically that’s all you need to do."
  },
  {
    "objectID": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html#provide-precompiled-binaries-for-windows",
    "href": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html#provide-precompiled-binaries-for-windows",
    "title": "Unofficial Introduction To extendr (Appendix I): Setup GitHub Actions CI",
    "section": "Provide precompiled binaries for Windows",
    "text": "Provide precompiled binaries for Windows\nAs you might have already noticed, the setup instruction for Windows is a bit complex compared to other OSes (i.e., Linux and macOS). So, it might be worth considering providing the precompiled static libraries, just like rwinlib does for many C++ libraries.\nOther motivation is that some of CRAN machines don’t have Rust toolchain. If the package author wants to submit their package to CRAN, such a mechanism is needed.\n(edit: it seems macOS is also the case, but I don’t find what’s the best way to solve this. I’ll probably write another post for this.)\nThere probably isn’t a single standard way to achieve this, but let me share what I did for my package, string2path here. YMMV, of course.\nI used softprops/action-gh-release action to publish the binaries as a GitHub release. The setting would be like this:\n# rename the binaries before uploading so that we can distinguish them easily.\n- name: Tweak staticlib\n  if: runner.os == 'Windows'\n  run: |\n    mv ./check/string2path.Rcheck/00_pkg_src/string2path/src-i386/rust/target/i686-pc-windows-gnu/release/libstring2path.a \\\n      i686-pc-windows-gnu-libstring2path.a\n    mv ./check/string2path.Rcheck/00_pkg_src/string2path/src-x64/rust/target/x86_64-pc-windows-gnu/release/libstring2path.a \\\n      x86_64-pc-windows-gnu-libstring2path.a\n\n- name: Release\n  uses: softprops/action-gh-release@v1\n  # only run this on a tag event\n  if: runner.os == 'Windows' && startsWith(github.ref, 'refs/tags/')\n  with:\n    fail_on_unmatched_files: true\n    files: |\n      i686-pc-windows-gnu-libstring2path.a\n      x86_64-pc-windows-gnu-libstring2path.a\nWith this setup, you can publish the binaries by pushing tags. For example, let’s create windows_20210801-3 tag and push it.\ngit tag windows_20210801-3\ngit push origin windows_20210801-3\nThen, the GHA will publish the corresponding release like this:\n\nNext, tweak src/Makefile.win as follows to allow users to download the binaries when cargo is not available. There might be more nicer code to choose the latest release automatically, but I think it’s safe to specify a fixed tag name, though it’s a bit tiresome to update this manually every time you update Rust code.\n(edit: I found this violates the CRAN policy. A package is not allowed to write “anywhere else on the file system apart from the R session’s temporary directory. You need to set CARGO_HOME envvar to some temporary directory to avoid this.)\nCRATE = foo   # your crate name\nBASE_TAG = windows_20210801-3  # the tag you want to use\n\n# c.f. https://stackoverflow.com/a/34756868\n# Note that this assignment (`:=`) is not available on Solaris, so you need to\n# add \"GNU make\" to SystemRequirements field on DESCRIPTION, even though this\n# can never compile on Solaris anyway...\nCARGO_EXISTS := $(shell cargo --version 2> /dev/null)\n\n# ..snip...\n\n$(STATLIB):\nifdef CARGO_EXISTS\n    cargo build --target=$(TARGET) --lib --release --manifest-path=./rust/Cargo.toml\nelse\n    mkdir -p $(LIBDIR)\n    curl -L -o $(STATLIB) https://github.com/yutannihilation/$(CRATE)/releases/download/$(BASE_TAG)/$(TARGET)-lib$(CRATE).a\nendif\nOne caveat is that this won’t work when cargo is installed but with the GNU toolchain (extendr requires the MSVC toolchain on Windows). I guess some friendlier check can be done in configure.win, but this post won’t look into the details."
  },
  {
    "objectID": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html#other-topics-i-couldnt-cover",
    "href": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html#other-topics-i-couldnt-cover",
    "title": "Unofficial Introduction To extendr (Appendix I): Setup GitHub Actions CI",
    "section": "Other topics I couldn’t cover",
    "text": "Other topics I couldn’t cover\n\nsccache: Builds can be faster by using sccache, a ccache-like compiler caching tool for Rust. A blog post describes how to use this on GHA, but I don’t think I understand it to the extent where I can explain it here in clear words, sorry…\nHow to run tests on Rust’s side?: This post doesn’t explain how to run Rust tests (i.e., cargo test) or lints (i.e., cargo fmt, and cargo clippy). I even don’t figure out what tests should live in R or in Rust."
  },
  {
    "objectID": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html#example",
    "href": "post/2021-08-01-unofficial-introduction-to-extendr-appendix-i-setup-github-actions-ci-and-more/index.html#example",
    "title": "Unofficial Introduction To extendr (Appendix I): Setup GitHub Actions CI",
    "section": "Example",
    "text": "Example\nHere’s the real example of the settings on my repo:\nhttps://github.com/yutannihilation/string2path/blob/main/.github/workflows/check-pak.yaml"
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "",
    "text": "I’ve been struggling with configure.win for several days. I think I’ve done, but it seems I’ve come too far from the last post. So, let me try to explain what a configure.win (or configure) would look like.\nLet’s start with this Makevars.win, basically the same one on the last blog post."
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#tweak-makevars.win",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#tweak-makevars.win",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "Tweak Makevars.win",
    "text": "Tweak Makevars.win\nBefore talking about configure scripts, I have to write a bit about Makevars.win (and Makevars) because it has something to be fixed.\n\nSet CARGO_HOME\nCan you see what part is wrong in the above Makevars.win? After the first CRAN submission of this package, I got this reply:\n\nChecking this creates ~/.cargo sized 82MB, in violation of the CRAN Policy. Please fix as necessary and resubmit.\n\nThe CRAN Policy says:\n\n\nPackages should not write in the user’s home filespace (including clipboards), nor anywhere else on the file system apart from the R session’s temporary directory (or during installation in the location pointed to by TMPDIR: and such usage should be cleaned up).\n\n\nBy default, cargo uses ~/.cargo for caching various things like the crates.io index and the dependency crates. Apparently, this is not allowed. To avoid this, we can set CARGO_HOME to the package’s local directory, like gifski package does.\nexport CARGO_HOME=$(PWD)/.cargo\nAlternatively, we can set this on the head of the line of cargo build directly.\nCARGO_HOME=$(PWD)/.cargo\n\n...snip...\n\n  CARGO_HOME=$(PWD)/.cargo cargo build --target=$(TARGET) --lib --release --manifest-path=./rust/Cargo.toml\nIn addition to this, as it “should be cleaned up,” we also need to add these two lines after cargo build:\n    rm -Rf $(CARGO_HOME)\n    rm -Rf $(LIBDIR)/build\nBut, this is a bit painful in terms of development. Why do I need to compile it always from scratch even on my local??\n\n\nNOT_CRAN environmental variable\nFortunately, devtools utilities provide NOT_CRAN envvar to distinguish CRAN and other environment. So, maybe we can determine whether to set CARGO_HOME, depending on NON_CRAN. It would be:\n# An envvar cannot be referred to as $(NOT_CRAN)\nNOT_CRAN_ENVVAR = ${NOT_CRAN}\n\n$(STATLIB):\nifeq ($(NOT_CRAN_ENVVAR),\"true\")\n  cargo build --target=$(TARGET) --lib --release --manifest-path=./rust/Cargo.toml\nelse\n  CARGO_HOME=$(PWD)/.cargo cargo build --target=$(TARGET) --lib --release --manifest-path=./rust/Cargo.toml\n    rm -Rf $(CARGO_HOME)\n    rm -Rf $(LIBDIR)/build\nendif\n\n\nSet PATH\nOne more thing my Makevars (not Makevars.win this time) couldn’t covered was the case when cargo is on PATH but rustc is not. It seems gifski package handles this by including $(HOME)/.cargo/bin in PATH (Makevars).\nI think sourcing \"$(HOME)/.cargo/env\" should also work (this actually just sets PATH), so I’ll try this next time. Note that source is not available on dash, so use . for this.\n. \"$(HOME)/.cargo/env\" && cargo build ...\nOkay, done. Let’s move onto configure scripts."
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#what-is-a-configure-script",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#what-is-a-configure-script",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "What is a configure script?",
    "text": "What is a configure script?\nA configure script is often used for configuring Makefile or Makevars, depending on the user’s setup.\nWriting R Extensions says:\n\nIf your package needs some system-dependent configuration before installation you can include an executable (Bourne shell script configure in your package which (if present) is executed by R CMD INSTALL before any other action is performed.\n\nconfigure is executed on UNIX-alikes, and Windows uses a different file configure.win1. Actually, I got this request from CRAN:\n\nBy the way, ideally string2path would use configure to test for cargo\n\nSo far, I used Makevars.win for testing the existence of cargo, but it seems configure scripts are the better place for this. Moreover, I do want to check the cargo functionality more precisely, for example,\n\nif the Rust version is not too old to support (“MSRV”)\n(Windows only) if the Rust installation has the required toolchain, stable-msvc\n(Windows only) if the Rust installation has the required targets, x86_64-pc-windows-gnu and i686-pc-windows-gnu\n\nbut Makevars.win is a bit too narrow to write a complex shell script."
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#biarch-true",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#biarch-true",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "Biarch: true",
    "text": "Biarch: true\nIf we use configure.win, we have to add the following line to DESCRIPTION.\nBiarch: true\nOtherwise, the 32-bit version won’t get built for unknown reason and it makes CRAN angry. This behaviour is found on R for Windows FAQ, but it doesn’t explain what we should do. I found this on the following post on RStudio Community.\n\nhttps://community.rstudio.com/t/configure-win-and-cran-submission/24684/4"
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#check-cargo",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#check-cargo",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "Check cargo",
    "text": "Check cargo\n\nCheck cargo is installed\nThis is simple.\ncargo version >/dev/null 2>&1\nif [ $? -ne 0 ]; then\n  echo \"cargo command is not available\"\n  exit 1\nfi\nNote that we don’t want to exit here, because the absence of cargo isn’t the end of the world; there might be a precompiled binary for the platform. But, let’s tweak it later.\n\n\nCheck if the Rust version is not too old to support\nThis isn’t always necessary, but we might want to reject the older Rust in the case when we use some feature that is available after the specific version of Rust. You know, comparing versions is tricky. But, this can be archived by sort command with -V option and -C option (c.f. SO answer). -V means version sort. -C means checking if the input is already sorted, and errors when it’s not. In summary, the implementation is the following:\n# c.f. https://github.com/ron-rs/ron/issues/256#issuecomment-657999081\nMIN_RUST_VERSION=\"1.41.0\"\n\nRUST_VERSION=\"`cargo --version | cut -d' ' -f2`\"\nif ! printf '%s\\n' \"${MIN_RUST_VERSION}\" \"${RUST_VERSION}\" | sort -C -V; then\n  echo \"The installed version of cargo (${RUST_VERSION}) is older than the requirement (${MIN_RUST_VERSION})\"\n  exit 1\nfi\n(Btw, did you know we cannot use $(expr) notation in configure because this syntax isn’t available on Solaris? We need to use `expr` instead)\n\n\nCheck if the Rust installation has the required toolchain and targets (Windows only)\nOn Windows, extendr provides support only the specified set of toolchain and target. So, we need to check it.\nChecking toolchain is simple. Use + to specify the toolchain.\nEXPECTED_TOOLCHAIN=\"stable-msvc\"\n\ncargo \"+${EXPECTED_TOOLCHAIN}\" version >/dev/null 2>&1\nif [ $? -ne 0 ]; then\n  echo \"${EXPECTED_TOOLCHAIN} toolchain is not installed\"\n  exit 1\nfi\nInstalled targets can be listed by rustup target list --installed. So, the check would be like\nEXPECTED_TARGET=\"x86_64-pc-windows-gnu\"\n\nif ! rustup target list --installed | grep -q \"${EXPECTED_TARGET}\"; then\n  echo \"target ${EXPECTED_TARGET} is not installed\"\n  exit 1\nfi\nOne thing tricky here is that, unlike Makevars.win, configure.win is executed only once, not per architecture. So, we need to manually enumerate both 64-bit and 32-bit.\nOne more tricky thing at the time of writing this is…, you might not notice, the 32-bit version no longer exists in R-devel, which is supposed to be released as R 4.2! So, we want to check 32-bit only when there’s 32-bit. How? I couldn’t come up with some nice way, but it seems check on ${R_HOME}/bin/i386/ works:\ncheck_cargo_target() {\n  EXPECTED_TARGET=\"$1\"\n  \n  if ! rustup target list --installed | grep -q \"${EXPECTED_TARGET}\"; then\n    echo \"target ${EXPECTED_TARGET} is not installed\"\n    exit 1\n  fi\n}\n\ncheck_cargo_target x86_64-pc-windows-gnu\n\nif [ -d \"${R_HOME}/bin/i386/\" ]; then\n  check_cargo_target i686-pc-windows-gnu\nfi"
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#but-what-can-we-do",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#but-what-can-we-do",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "But what can we do?",
    "text": "But what can we do?\nNow we can check the cargo installation. But, if the check fails, what should we do? Actually, on CRAN, Windows and macOS machines don’t have Rust installed.\nThere are two options.\n\nInstall cargo into the temporary directory\nDownload the precompiled binaries (which means you have to serve the binaries on somewhere beforehand).\n\nFor example, the gifski package uses option 1 for macOS and option 2 for Windows. My string2path package now uses option 2 both for macOS and for Windows."
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#install-cargo-on-the-fly",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#install-cargo-on-the-fly",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "Install cargo on the fly",
    "text": "Install cargo on the fly\nThe implementation for macOS is below. The actual process is written in the downloaded script, but what it does is basically downloading cargo.\n# Try local version on MacOS, otherwise error\n[ `uname` = \"Darwin\" ] && curl \"https://autobrew.github.io/scripts/rust\" -sSf | sh && exit 0\nI don’t write much about this this time, but cargo package should also be useful for this strategy."
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#download-the-precompiled-binaries",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#download-the-precompiled-binaries",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "Download the precompiled binaries",
    "text": "Download the precompiled binaries\nThis is what was done in Makevars.win.\n$(STATLIB):\nifdef CARGO_EXISTS\n    cargo build --target=$(TARGET) --lib --release --manifest-path=./rust/Cargo.toml\nelse\n    mkdir -p $(LIBDIR)\n    curl -L -o $(STATLIB) https://github.com/yutannihilation/$(CRATE)/releases/download/$(BASE_TAG)/$(TARGET)-lib$(CRATE).a\nendif\nTo move this to configure and configure.win. There are several things to consider. For example:\n\nAs this downloads the binary before $(STATLIB) is executed, we need some tweak to ensure it’s not removed by C_clean\n(Windows only) unlike Makevars.win is executed per arch, configure.win is executed only once\n\nWhat’s more, I want to make one addition:\n\nVerify the checksums\n\nSome might have wondered if it’s safe to download an arbitrary binary from GitHub. First of all, I’d argue it’s safe. To say the least, essentially, it’s no unsafer than downloading a cargo binary itself, (or than “curl URL | sh”). If it’s downloaded over HTTPS, the data hardly gets compromised as long as their servers are not compromised. I believe GitHub servers and Rust servers are both very secure.\nThat said, we can improve the security further by “pinning” the binary. So, I recommend verifying the checksum when downloading a binary. For example, the stringi package does this.\n\nMakevars.in and Makevars.win.in\nRemember Makevars.win has these lines ($(STATLIB) is the artifact of the Rust code):\nall: C_clean\nC_clean:\n    rm -Rf $(SHLIB) $(STATLIB) $(OBJECTS)\nRemoving $(STATLIB) is needed to invoke cargo build in the case when cargo is installed. Otherwise, $(STATLIB) keeps existing, which means that target is never executed.\nOn the other hand, we don’t want to execute cargo build if there’s no cargo. So, in this case, we need to prevent the downloaded binary from getting removed.\nIn order to change the logic like this, we can use Makevars.in and Makevars.win.in as a template to generate Makevars and Makevars.win respectively. Generating these files can be done in configure and configure.win. For example,\nMakevars.in:\nC_clean:\n    rm -Rf $(SHLIB) $(OBJECTS) @CLEAN_EXTRA@\nconfigure:\n# cargo installed\nsed -e 's|@CLEAN_EXTRA@|$(STATLIB)|' src/Makevars.in > src/Makevars\n\n# no cargo\nsed -e 's|@CLEAN_EXTRA@||' src/Makevars.in > src/Makevars\nIf we generate Makevars and Makevars.win in configure scripts, we also need to clean up these by the cleanup script (this is required by Writing R Extensions).\nrm -f src/Makevars src/Makevars.win\nOf course, don’t forget to add src/Makevars and src/Makevars.win to .gitignore.\n(Probably, we can generate more sophisticated Makevars, but this time I used only this one replacement.)\n\n\nVerify the checksums\nTo verify the checksum, we can use sha256sum on platforms other than macOS, and shasum -a 256 on macOS. For the example of macOS:\nSHA256SUM_EXPECTED=be65f074cb7ae50e5784e7650f48579fff35f30ff663d1c01eabdc9f35c1f87c\n\n# Verify the checksum\nSHA256SUM_ACTUAL=`shasum -a 256 \"${DST}\" | cut -d' ' -f1`\nif [ -z \"${SHA256SUM_ACTUAL}\" ]; then\n  echo \"Failed to get the checksum\"\n  exit 1\nfi\n\nif [ \"${SHA256SUM_ACTUAL}\" != \"${SHA256SUM_EXPECTED}\" ]; then\n  echo \"Checksum mismatch for the pre-compiled binary\"\n  exit 1\nfi"
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#most-important-ask-cran-maintainers-to-exclude-solaris",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#most-important-ask-cran-maintainers-to-exclude-solaris",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "[MOST IMPORTANT!] Ask CRAN maintainers to exclude Solaris",
    "text": "[MOST IMPORTANT!] Ask CRAN maintainers to exclude Solaris\nOh, sorry, I forgot to enumerate the most important option!\n\nGive up\n\nBecause 32-bit Solaris is not a supported platform by Rust, there’s no option other than giving up. I wrote this on cran-comment and it seems this was accepted:\n\nI would like to request to exclude Solaris from the build targets because Solaris is not a supported platform by Rust. This should be in line with the treatments of other CRAN packages that use Rust; gifski, baseflow, and salso are not built on Solaris. I’m sorry that I didn’t write this in the first submission.\n\nBe sure to add some comment like this when you submit an R package with Rust to CRAN!"
  },
  {
    "objectID": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#example",
    "href": "post/2021-09-21-writing-a-configure-script-for-an-r-package-using-rust/index.html#example",
    "title": "Writing A Configure Script For An R Package Using Rust",
    "section": "Example",
    "text": "Example\nFor a real example, please refer to string2path, though your mileage may vary."
  },
  {
    "objectID": "post/a-survival-guide-to-install-rlang-from-github-on-windows/index.html",
    "href": "post/a-survival-guide-to-install-rlang-from-github-on-windows/index.html",
    "title": "A Survival Guide To Install rlang From GitHub On Windows",
    "section": "",
    "text": "I don’t have any strong feelings about OSs. They are just tools. I had been a Mac user for 10+ years since I was 10, and now I’m using Windows for no reason. All OSs have their pros and cons. For example, I like Mac, but, in the late 90s, I was very disappointed at Mac because it didn’t have fonts to display Shift_JIS art nicely.\nAnyway, I’m using Windows and I need to survive. Here’s an error I often see when I try to install rlang package from GitHub by devtools::install_github():\nThis is because rlang.dll is used by the current R session (or other session?), so Windows won’t let me overwrite it. What should I do? Here’s some advice."
  },
  {
    "objectID": "post/a-survival-guide-to-install-rlang-from-github-on-windows/index.html#restart-the-r-session",
    "href": "post/a-survival-guide-to-install-rlang-from-github-on-windows/index.html#restart-the-r-session",
    "title": "A Survival Guide To Install rlang From GitHub On Windows",
    "section": "Restart the R session",
    "text": "Restart the R session\nThis is always necessary. Since rlang is very fundamental package, it might be loaded as a dependency of some attached or loaded package (if you are curious about the differences between load and attach, R Packages helps). You need a fresh session with no packages (except for base packages) loaded. On RStudio, Ctrl+Shift+F10, or “Restart R” in “Session” menu.\n\nFor usual packages, this is enough. But, rlang is not the case…"
  },
  {
    "objectID": "post/a-survival-guide-to-install-rlang-from-github-on-windows/index.html#use-remotesinstall_github-instead-of-devtools",
    "href": "post/a-survival-guide-to-install-rlang-from-github-on-windows/index.html#use-remotesinstall_github-instead-of-devtools",
    "title": "A Survival Guide To Install rlang From GitHub On Windows",
    "section": "Use remotes::install_github() instead of devtools",
    "text": "Use remotes::install_github() instead of devtools\ndevtools::install_github() is just re-exported from remotes package. So, are the same one. But, if I use devtools::, devtools’s dependencies are loaded, and, at the moment, rlang is included here. So, the same error will occur.\nOn the other hand, remotes:: doesn’t need rlang directly or indirectly. So, run\nremotes::install_github(\"r-lib/rlang\")\n(You might also need remotes::install_github() for other dependency packages like glue.)\nNote that, pkg package seems aware of this kind of problems, so we’ll be free from this kind of problems when pkg is mature!"
  },
  {
    "objectID": "post/a-survival-guide-to-install-rlang-from-github-on-windows/index.html#extra-steps",
    "href": "post/a-survival-guide-to-install-rlang-from-github-on-windows/index.html#extra-steps",
    "title": "A Survival Guide To Install rlang From GitHub On Windows",
    "section": "Extra steps?",
    "text": "Extra steps?\nUsually, restarting the session + using remotes::install_github() works. But, in the past, I needed some extra steps. I don’t know why, but it seemed RStudio loads rlang in background (c.f. https://github.com/r-lib/remotes/issues/131). So, for future references, I note some. Hope this will never be needed again…\n\nRemove rlang\nRemoving package might help, since removed package cannot be loaded.\nremove.packages(\"rlang\")\n\n\nUse git clone, R CMD build and R CMD INSTALL\nIf you really need to be away from RStudio, or even from any R sessions. In those cases, you can use git and R CMD build and R CMD INSTALL on console.\ngit clone https://github.com/r-lib/rlang\nR.exe CMD build --no-manual --no-build-vignettes rlang/\nR.exe CMD INSTALL [--some-options-i-dont-remember] rlang_*.tar.gz"
  },
  {
    "objectID": "post/a-tip-to-debug-ggplot2/index.html",
    "href": "post/a-tip-to-debug-ggplot2/index.html",
    "title": "A Tip to Debug ggplot2",
    "section": "",
    "text": "Since the tidyverse developer day is near, I share my very very secret technique to debug ggplot2. Though this is a very small thing, hope this helps someone a bit."
  },
  {
    "objectID": "post/a-tip-to-debug-ggplot2/index.html#ggplot2-is-unbreakable",
    "href": "post/a-tip-to-debug-ggplot2/index.html#ggplot2-is-unbreakable",
    "title": "A Tip to Debug ggplot2",
    "section": "ggplot2 is unbreakable!",
    "text": "ggplot2 is unbreakable!\nYou might want to debug() the methods of Geoms or Stats.\ndebug(GeomPoint$draw_panel)\nBut, this is not effective because the geom_point() generates different instances, so their draw_panel are all different objects (c.f. R6 classes have debug method for this). (edit: @BrodieGaslamtold me I’m wrong. The reason we can’t do debug(GeomPoint$draw_panel) s because $ is overridden and debug(get(\"draw_panel\", GeomPoint)) definitely works.)\nThen what about RStudio’s nice breakpoint features?\n\nUsually, this is enough. But, ggplot2’s ggprotos are not the case. You cannot use breakpoints to dig into them.\n\nHmm… But, no, you don’t need to scratch your head. The solution is pretty simple."
  },
  {
    "objectID": "post/a-tip-to-debug-ggplot2/index.html#use-browser",
    "href": "post/a-tip-to-debug-ggplot2/index.html#use-browser",
    "title": "A Tip to Debug ggplot2",
    "section": "Use browser()",
    "text": "Use browser()\nYou just need to\n\nadd browser() on the line where you want to debug, and\nload all (Cmd+Shift+L for Mac, Ctrl+Shift+L for Windows, and C-c C-w l for Emacs/ESS).\n\n\nThen, you’ll be on debug mode at last!"
  },
  {
    "objectID": "post/a-tip-to-debug-ggplot2/index.html#ymmv",
    "href": "post/a-tip-to-debug-ggplot2/index.html#ymmv",
    "title": "A Tip to Debug ggplot2",
    "section": "YMMV",
    "text": "YMMV\nThat’s all for this posts. But, I guess there are many alternative ways to achieve this, and I’m almost sure, at the end of the developer day, I will feel shame to have published this post, which just describes my debug skill is so poor… I’m really looking forward to learning from others. See you there!"
  },
  {
    "objectID": "post/double-dispatch-of-s3-method/index.html",
    "href": "post/double-dispatch-of-s3-method/index.html",
    "title": "Double dispatch of S3 method",
    "section": "",
    "text": "When I tried to define an S3 class that contains multiple ggplot objects, I’ve faced the lessor-know mechanism of S3 method dispatch, double dispatch."
  },
  {
    "objectID": "post/double-dispatch-of-s3-method/index.html#problem",
    "href": "post/double-dispatch-of-s3-method/index.html#problem",
    "title": "Double dispatch of S3 method",
    "section": "Problem",
    "text": "Problem\nTake a look at this example. manyplot class contains many plots, and displays them nicely when printted.\n\nlibrary(ggplot2)\n\nset.seed(100)\nd1 <- data.frame(x = 1:100, y = cumsum(runif(100)))\nd2 <- data.frame(x = 1:100, y = cumsum(runif(100)))\n\nplot_all <- function(...) {\n  l <- lapply(list(...), function(d) ggplot(d, aes(x, y)) + geom_line())\n  l <- unname(l)\n  class(l) <- \"manyplot\"\n  l\n}\n\nprint.manyplot <- function(x, ...) {\n  do.call(gridExtra::grid.arrange, x)\n}\n\np <- plot_all(d1, d2)\np\n\n\n\n\nSo far, so good.\nNext, I want to define + method, so that I can customize the plots just as I do with usual ggplot2.\n\n`+.manyplot` <- function(e1, e2) {\n  l <- lapply(e1, function(x) x + e2)\n  class(l) <- \"manyplot\"\n  l\n}\n\nBut, this won’t work…\n\np + theme_bw()\n\nWarning: Incompatible methods (\"+.manyplot\", \"+.gg\") for \"+\"\n\n\nError in p + theme_bw(): non-numeric argument to binary operator\n\n\nWhat’s this cryptic error? To understand what happened, we need to dive into the concept of S3’s “double dispatch”"
  },
  {
    "objectID": "post/double-dispatch-of-s3-method/index.html#double-dispatch",
    "href": "post/double-dispatch-of-s3-method/index.html#double-dispatch",
    "title": "Double dispatch of S3 method",
    "section": "Double dispatch?",
    "text": "Double dispatch?\nUsually, S3’s method dispatch depends only on the type of first argument. But, in cases of some infix operators like + and *, it uses both of their arguments; this is called double dispatch.\nWhy is this needed? According to Advanced R:\n\nThis is necessary to preserve the commutative property of many operators, i.e. a + b should equal b + a.\n\nTo ensure this, if both a and b are S3 objects, the method chosen in a + b can be (c.f. how do_arith() works with S3 objects):\n\n\n\n\n\n\n\n\n\nDoes a have an S3 method?\nDoes b have an S3 method?\nAre the methods same?\nWhet method is chosen?\n\n\n\n\nyes\nyes\nyes\na’s method or b’s method (they are the same)\n\n\nyes\nyes\nno\ninternal method\n\n\nyes\nno\n-\na’s method\n\n\nno\nyes\n-\nb’s method\n\n\nno\nno\n-\ninternal method\n\n\n\nHere’s examples to show them clearly:\n\nfoo <- function(x) structure(x, class = \"foo\")\n`+.foo` <- function(e1, e2) message(\"foo!\")\n\nbar <- function(x) structure(x, class = \"bar\")\n`+.bar` <- function(e1, e2) message(\"bar?\")\n\n# both have the same S3 method\nfoo(1) + foo(1)\n\nfoo!\n\n\nNULL\n\n# both have different S3 methods\nfoo(1) + bar(1)\n\nWarning: Incompatible methods (\"+.foo\", \"+.bar\") for \"+\"\n\n\n[1] 2\nattr(,\"class\")\n[1] \"foo\"\n\n# `a` has a method, and `b` doesn't\nfoo() + 1\n\nError in structure(x, class = \"foo\"): argument \"x\" is missing, with no default\n\n# `b` has a method, and `a` doesn't\n1 + foo()\n\nError in structure(x, class = \"foo\"): argument \"x\" is missing, with no default\n\n# both don't have methods\nrm(`+.foo`)\nfoo(1) + foo(1)\n\n[1] 2\nattr(,\"class\")\n[1] \"foo\""
  },
  {
    "objectID": "post/double-dispatch-of-s3-method/index.html#explanation",
    "href": "post/double-dispatch-of-s3-method/index.html#explanation",
    "title": "Double dispatch of S3 method",
    "section": "Explanation",
    "text": "Explanation\nSo, now it’s clear to our eyes what happened in the code below; they have different methods (+.manyplot and +.gg) so it falled back to internal method. But, because fundamentally they are list, the internal mechanism refused to add these two objects…\n\np + theme_bw()"
  },
  {
    "objectID": "post/double-dispatch-of-s3-method/index.html#how-can-i-overcome-this",
    "href": "post/double-dispatch-of-s3-method/index.html#how-can-i-overcome-this",
    "title": "Double dispatch of S3 method",
    "section": "How can I overcome this?",
    "text": "How can I overcome this?\nHadley says ggplot2 might eventually end up using the double-dispatch approach in vctrs. So, we can wait for the last hope.\nIf you cannot wait, use S4. S4 can naturally do double dispatch because their method dispatch depends on the whole combination of types of the arguments."
  },
  {
    "objectID": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html",
    "href": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html",
    "title": "Enhancing gather() and spread() by Using “Bundled” data.frames",
    "section": "",
    "text": "Last month, I tried to explain gather() and spread() by gt package (https://yutani.rbind.io/post/gather-and-spread-explained-by-gt/). But, after I implemented experimental multi-gather() and multi-spread(), I realized that I need a bit different way of explanation… So, please forget the post, and read this with fresh eyes!"
  },
  {
    "objectID": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#wait-what-is-multi-gather-and-multi-spread",
    "href": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#wait-what-is-multi-gather-and-multi-spread",
    "title": "Enhancing gather() and spread() by Using “Bundled” data.frames",
    "section": "Wait, what is multi-gather() and multi-spread()??",
    "text": "Wait, what is multi-gather() and multi-spread()??\nIn short, the current gather() and spread() have a limitation; they can gather into or spread from only one column at once. So, if we want to handle multiple columns, we need to coerce them to one column before actually gathering or spreading.\nThis is especially problematic when the columns have different types. For example, date column is unexpectedly converted to integers with the following code:\n\nlibrary(tibble)\nlibrary(tidyr)\n\n# a bit different version of https://github.com/tidyverse/tidyr/issues/149#issue-124411755\nd <- tribble(\n  ~place, ~censor,                  ~date, ~status,\n    \"g1\",    \"c1\",  as.Date(\"2019-02-01\"),   \"ok\",\n    \"g1\",    \"c2\",  as.Date(\"2019-02-01\"),  \"bad\",\n    \"g1\",    \"c3\",  as.Date(\"2019-02-01\"),   \"ok\",\n    \"g2\",    \"c1\",  as.Date(\"2019-02-01\"),  \"bad\",\n    \"g2\",    \"c2\",  as.Date(\"2019-02-02\"),   \"ok\"\n)\n\nd %>%\n  gather(key = element, value = value, date, status) %>%\n  unite(thing, place, element, remove = TRUE) %>%\n  spread(thing, value, convert = TRUE)\n\nWarning: attributes are not identical across measure variables;\nthey will be dropped\n\n\n# A tibble: 3 × 5\n  censor g1_date g1_status g2_date g2_status\n  <chr>    <int> <chr>       <int> <chr>    \n1 c1       17928 ok          17928 bad      \n2 c2       17928 bad         17929 ok       \n3 c3       17928 ok             NA <NA>     \n\n\nHere, we need better spread() and gather(), which can handle multiple columns. For more discussions, you can read the following issues:\n\nhttps://github.com/tidyverse/tidyr/issues/149\nhttps://github.com/tidyverse/tidyr/issues/150\n\nIn this post, I’m trying to explain an approach to solve this by using “bundled” data.frames, which is originally proposed by Kirill Müller."
  },
  {
    "objectID": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#bundled-data.frames",
    "href": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#bundled-data.frames",
    "title": "Enhancing gather() and spread() by Using “Bundled” data.frames",
    "section": "“Bundled” data.frames",
    "text": "“Bundled” data.frames\nFor convenience, I use a new term “bundle” for separating some of the columns of a data.frame to another data.frame, and assigning the new data.frame to a column, and “unbundle” for the opposite operation.\nFor example, “bundling X, Y, and Z” means converting this\n\n\n\n\n\n\n  \n  \n    \n      id\n      X\n      Y\n      Z\n    \n  \n  \n    1\n0.1\na\nTRUE\n    2\n0.2\nb\nFALSE\n    3\n0.3\nc\nTRUE\n  \n  \n  \n\n\n\n\nto something like this:\n\n\n\n\n\n\n  \n  \n    \n      id\n      \n        foo\n      \n    \n    \n      X\n      Y\n      Z\n    \n  \n  \n    1\n0.1\na\nTRUE\n    2\n0.2\nb\nFALSE\n    3\n0.3\nc\nTRUE\n  \n  \n  \n\n\n\n\nYou might wonder if this is really possible without dangerous hacks. But, with tibble package (2D columns are supported now), this is as easy as:\n\ntibble(\n  id = 1:3,\n  foo = tibble(\n    X = 1:3 * 0.1,\n    Y = letters[1:3],\n    Z = c(TRUE, FALSE, TRUE)\n  )\n)\n\n# A tibble: 3 × 2\n     id foo$X $Y    $Z   \n  <int> <dbl> <chr> <lgl>\n1     1   0.1 a     TRUE \n2     2   0.2 b     FALSE\n3     3   0.3 c     TRUE \n\n\nFor more information about data.frame columns, please see Advanced R."
  },
  {
    "objectID": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#an-experimental-package-for-this",
    "href": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#an-experimental-package-for-this",
    "title": "Enhancing gather() and spread() by Using “Bundled” data.frames",
    "section": "An experimental package for this",
    "text": "An experimental package for this\nI created a package for bundling, tiedr. Since this is just an experiment, I don’t seriously introduce this. But, for convenience, let me use this package in this post because, otherwise, the code would be a bit long and hard to read…\nhttps://github.com/yutannihilation/tiedr\nI need four functions from this package, bundle(), unbundle(), gather_bundles(), and spread_bundles(). gather_bundles() and spread_bundles() are some kind of the variants of gather() and spread(), so probably you can guess the usages. Here, I just explain about the first two functions briefly.\n\nbundle()\nbundle() bundles columns. It takes data, and the specifications of bundles in the form of new_col1 = c(col1, col2, ...), new_col2 = c(col3, col4, ...), ....\n\nlibrary(tiedr)\n\nd <- tibble(id = 1:3, X = 1:3 * 0.1, Y = letters[1:3], Z = c(TRUE, FALSE, TRUE))\n\nbundle(d, foo = X:Z)\n\n# A tibble: 3 × 2\n     id foo$X $Y    $Z   \n  <int> <dbl> <chr> <lgl>\n1     1   0.1 a     TRUE \n2     2   0.2 b     FALSE\n3     3   0.3 c     TRUE \n\n\nbundle() also can rename the sub-columns at the same time.\n\nbundle(d, foo = c(x = X, y = Y, z = Z))\n\n# A tibble: 3 × 2\n     id foo$x $y    $z   \n  <int> <dbl> <chr> <lgl>\n1     1   0.1 a     TRUE \n2     2   0.2 b     FALSE\n3     3   0.3 c     TRUE \n\n\n\n\nunbundle()\nunbundle() unbundles columns. This operation is almost the opposite of what bundle() does; one difference is that this adds the names of the bundle as prefixes in order to avoid name collisions. In case the prefix is not needed, we can use sep = NULL.\n\nd %>%\n  bundle(foo = X:Z) %>% \n  unbundle(foo)\n\n# A tibble: 3 × 4\n     id foo_X foo_Y foo_Z\n  <int> <dbl> <chr> <lgl>\n1     1   0.1 a     TRUE \n2     2   0.2 b     FALSE\n3     3   0.3 c     TRUE"
  },
  {
    "objectID": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#expose-hidden-structures-in-colnames-as-bundles",
    "href": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#expose-hidden-structures-in-colnames-as-bundles",
    "title": "Enhancing gather() and spread() by Using “Bundled” data.frames",
    "section": "Expose hidden structures in colnames as bundles",
    "text": "Expose hidden structures in colnames as bundles\nOne of the meaningful usage of bundled data.frame is to express the structure of a data. Suppose we have this data (from tidyverse/tidyr#150):\n\nd <- tribble(\n  ~Race,~Female_LoTR,~Male_LoTR,~Female_TT,~Male_TT,~Female_RoTK,~Male_RoTK,\n  \"Elf\",        1229,       971,       331,     513,         183,       510,\n  \"Hobbit\",       14,      3644,         0,    2463,           2,      2673,\n  \"Man\",           0,      1995,       401,    3589,         268,      2459\n)\n\n\n\n\n\n\n\n  \n  \n    \n      Race\n      Female_LoTR\n      Male_LoTR\n      Female_TT\n      Male_TT\n      Female_RoTK\n      Male_RoTK\n    \n  \n  \n    Elf\n1229\n971\n331\n513\n183\n510\n    Hobbit\n14\n3644\n0\n2463\n2\n2673\n    Man\n0\n1995\n401\n3589\n268\n2459\n  \n  \n  \n\n\n\n\nIn this data, the prefixes Female_ and Male_ represent the column groups. Thus, as Kirill Müller suggests in the comment, these columns can be bundled (with the sub-columns renamed) to:\n\n\n\n\n\n\n  \n  \n    \n      Race\n      \n        Female\n      \n      \n        Male\n      \n    \n    \n      LoTR\n      TT\n      RoTK\n      LoTR\n      TT\n      RoTK\n    \n  \n  \n    Elf\n1229\n331\n183\n971\n513\n510\n    Hobbit\n14\n0\n2\n3644\n2463\n2673\n    Man\n0\n401\n268\n1995\n3589\n2459\n  \n  \n  \n\n\n\n\nWith bundle() we can write this as:\n\nd_bundled <- d %>% \n  bundle(\n    Female = c(LoTR = Female_LoTR, TT = Female_TT, RoTK = Female_RoTK),\n    Male   = c(LoTR = Male_LoTR,   TT = Male_TT,   RoTK = Male_RoTK)\n  )\n\nd_bundled\n\n# A tibble: 3 × 3\n  Race   Female$LoTR   $TT $RoTK Male$LoTR   $TT $RoTK\n  <chr>        <dbl> <dbl> <dbl>     <dbl> <dbl> <dbl>\n1 Elf           1229   331   183       971   513   510\n2 Hobbit          14     0     2      3644  2463  2673\n3 Man              0   401   268      1995  3589  2459"
  },
  {
    "objectID": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#gather-the-bundles",
    "href": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#gather-the-bundles",
    "title": "Enhancing gather() and spread() by Using “Bundled” data.frames",
    "section": "gather() the bundles",
    "text": "gather() the bundles\nRemember gather() strips colnames and convert it to a column. We can do this operation for bundled data.frames in the same manner. But, unlike gather() for flat data.frames, we don’t need to specify a colname for values, because the contents in bundles already have their colnames.\nLet’s gather Female and Male bundles into key column.\n\nd_gathered <- d_bundled %>%\n  gather_bundles(Female, Male, .key = \"key\")\n\nd_gathered\n\n# A tibble: 6 × 5\n  Race   key     LoTR    TT  RoTK\n  <chr>  <chr>  <dbl> <dbl> <dbl>\n1 Elf    Female  1229   331   183\n2 Hobbit Female    14     0     2\n3 Man    Female     0   401   268\n4 Elf    Male     971   513   510\n5 Hobbit Male    3644  2463  2673\n6 Man    Male    1995  3589  2459\n\n\n\n\n\n\n\n\n  \n  \n    \n      Race\n      key\n      LoTR\n      TT\n      RoTK\n    \n  \n  \n    Elf\nFemale\n1229\n331\n183\n    Hobbit\nFemale\n14\n0\n2\n    Man\nFemale\n0\n401\n268\n    Elf\nMale\n971\n513\n510\n    Hobbit\nMale\n3644\n2463\n2673\n    Man\nMale\n1995\n3589\n2459\n  \n  \n  \n\n\n\n\nNow we have all parts for implementing multi-gather(). I did bundling by manual, but we can have a helper function to find the common prefixes and bundle them automatically. So, multi-gather() will be something like:\n\nd %>%\n  auto_bundle(-Race) %>% \n  gather_bundles()\n\n# A tibble: 6 × 5\n  Race   key     LoTR    TT  RoTK\n  <chr>  <chr>  <dbl> <dbl> <dbl>\n1 Elf    Female  1229   331   183\n2 Hobbit Female    14     0     2\n3 Man    Female     0   401   268\n4 Elf    Male     971   513   510\n5 Hobbit Male    3644  2463  2673\n6 Man    Male    1995  3589  2459"
  },
  {
    "objectID": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#spread-to-the-bundles",
    "href": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#spread-to-the-bundles",
    "title": "Enhancing gather() and spread() by Using “Bundled” data.frames",
    "section": "spread() to the bundles",
    "text": "spread() to the bundles\nAs we already saw it’s possible to gather() multiple bundles, now it’s obvious that we can spread() multiple columns into multiple bundles vice versa. So, let me skip the details here.\nWe can multi-spread():\n\nd_bundled_again <- d_gathered %>%\n  spread_bundles(key, LoTR:RoTK)\n\nd_bundled_again\n\n# A tibble: 3 × 3\n  Race   Female$LoTR   $TT $RoTK Male$LoTR   $TT $RoTK\n  <chr>        <dbl> <dbl> <dbl>     <dbl> <dbl> <dbl>\n1 Elf           1229   331   183       971   513   510\n2 Hobbit          14     0     2      3644  2463  2673\n3 Man              0   401   268      1995  3589  2459\n\n\nThen, unbundle() flattens the bundles to prefixes.\n\nd_bundled_again %>%\n  unbundle(-Race)\n\n# A tibble: 3 × 7\n  Race   Female_LoTR Female_TT Female_RoTK Male_LoTR Male_TT Male_RoTK\n  <chr>        <dbl>     <dbl>       <dbl>     <dbl>   <dbl>     <dbl>\n1 Elf           1229       331         183       971     513       510\n2 Hobbit          14         0           2      3644    2463      2673\n3 Man              0       401         268      1995    3589      2459\n\n\nIt’s done. By combining these two steps, multi-spread() will be something like this:\n\nd_gathered %>%\n  spread_bundles(key, LoTR:RoTK) %>% \n  unbundle(-Race)"
  },
  {
    "objectID": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#considerations",
    "href": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#considerations",
    "title": "Enhancing gather() and spread() by Using “Bundled” data.frames",
    "section": "Considerations",
    "text": "Considerations\nAs I described above, multi-gather() doesn’t need the column name for value. On the other hand, usual gather() needs a new colname. Because, while it needs a name to become a column, an atomic column doesn’t have inner names.\nSimilarly, usual spread() can be considered as a special version of multi-spread(). Consider the case when we multi-spread()ing one column:\n\n# an example in ?tidyr::spread\ndf <- tibble(x = c(\"a\", \"b\"), y = c(3, 4), z = c(5, 6))\n\nspread_bundles(df, key = x, y, simplify = FALSE)\n\n# A tibble: 2 × 3\n      z   a$y   b$y\n  <dbl> <dbl> <dbl>\n1     5     3    NA\n2     6    NA     4\n\n\nSince y is the only one column in the data, we can simplify these 1-column data.frames to vectors:\n\nspread_bundles(df, key = x, y, simplify = TRUE)\n\n# A tibble: 2 × 3\n      z     a     b\n  <dbl> <dbl> <dbl>\n1     5     3    NA\n2     6    NA     4\n\n\nThis is usual spread().\nI’m yet to see if we can improve the current spread() and gather() to handle these differences transparently…"
  },
  {
    "objectID": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#future-plans",
    "href": "post/enhancing-gather-and-spread-by-using-bundled-data-frames/index.html#future-plans",
    "title": "Enhancing gather() and spread() by Using “Bundled” data.frames",
    "section": "Future plans",
    "text": "Future plans\nProbably, this post is too much about the implementational details. I need to think about the interfaces before proposing this on tidyr’s repo.\nAny suggestions or feedbacks are welcome!"
  },
  {
    "objectID": "post/gather-and-spread-explained-by-gt/index.html",
    "href": "post/gather-and-spread-explained-by-gt/index.html",
    "title": "gather() and spread() Explained By gt",
    "section": "",
    "text": "This is episode 0 of my long adventure to multi-spread and multi-gather (this is my homework I got at the tidyverse developer day…). This post might seem to introduce the different semantics from the current tidyr’s one, but it’s probably just because my idea is still vague. So, I really appreciate any feedbacks!"
  },
  {
    "objectID": "post/gather-and-spread-explained-by-gt/index.html#tldr",
    "href": "post/gather-and-spread-explained-by-gt/index.html#tldr",
    "title": "gather() and spread() Explained By gt",
    "section": "tl;dr",
    "text": "tl;dr\nI now think gather() and spread() are about\n\ngrouping and\nenframe()ing and deframe()ing within each group\n\nDo you get what I mean? Let me explain step by step."
  },
  {
    "objectID": "post/gather-and-spread-explained-by-gt/index.html#what-does-gt-teach-us",
    "href": "post/gather-and-spread-explained-by-gt/index.html#what-does-gt-teach-us",
    "title": "gather() and spread() Explained By gt",
    "section": "What does gt teach us?",
    "text": "What does gt teach us?\nA while ago, gt package, Richard Iannone’s work-in-progress great work, was made public.\ngt package is wonderful, especially in that it makes us rethink about the possible semantics of columns. I mean, not all columns are equal. No, I don’t say anything new; this is what you already know with spread() and gather().\n\nspread()ed data explained\nTake a look at this example data, a simpler version of the one in ?gather:\n\nlibrary(tibble)\nlibrary(gt)\n\nset.seed(1)\n# example in ?gather\nstocks <- tibble(\n  time = as.Date('2009-01-01') + 0:2,\n  X = rnorm(3, 0, 1),\n  Y = rnorm(3, 0, 2),\n  Z = rnorm(3, 0, 4)\n)\n\nstocks\n\n# A tibble: 3 × 4\n  time            X      Y     Z\n  <date>      <dbl>  <dbl> <dbl>\n1 2009-01-01 -0.626  3.19   1.95\n2 2009-01-02  0.184  0.659  2.95\n3 2009-01-03 -0.836 -1.64   2.30\n\n\nHere, X, Y, and Z are the prices of stock X, Y, and Z. Of course, we can gather() the columns as this is the very example for this, but, we also can bundle these columns using tab_spanner():\n\ngt(stocks) %>%\n  tab_spanner(\"price\", c(X, Y, Z))\n\n\n\n\n\n  \n  \n    \n      time\n      \n        price\n      \n    \n    \n      X\n      Y\n      Z\n    \n  \n  \n    2009-01-01\n-0.6264538\n3.1905616\n1.949716\n    2009-01-02\n0.1836433\n0.6590155\n2.953299\n    2009-01-03\n-0.8356286\n-1.6409368\n2.303125\n  \n  \n  \n\n\n\n\nYet another option is to specify groupname_col. We roughly think each row is a group and time is the grouping variable here:\n\ngt(stocks, groupname_col = \"time\")\n\n\n\n\n\n  \n  \n    \n      X\n      Y\n      Z\n    \n  \n  \n    \n      2009-01-01\n    \n    -0.6264538\n3.1905616\n1.949716\n    \n      2009-01-02\n    \n    0.1836433\n0.6590155\n2.953299\n    \n      2009-01-03\n    \n    -0.8356286\n-1.6409368\n2.303125\n  \n  \n  \n\n\n\n\n\n\ngather()ed data explained\nLet’s see the gathered version next. Here’s the data:\n\nstocksm <- stocks %>%\n  tidyr::gather(\"name\", \"value\", X:Z)\n\nstocksm\n\n# A tibble: 9 × 3\n  time       name   value\n  <date>     <chr>  <dbl>\n1 2009-01-01 X     -0.626\n2 2009-01-02 X      0.184\n3 2009-01-03 X     -0.836\n4 2009-01-01 Y      3.19 \n5 2009-01-02 Y      0.659\n6 2009-01-03 Y     -1.64 \n7 2009-01-01 Z      1.95 \n8 2009-01-02 Z      2.95 \n9 2009-01-03 Z      2.30 \n\n\nThis can be represented in a similar way. This time, a group doesn’t consist of a single row, but the rows with the same grouping values. Accordingly, the grouping is the same as above.\n\nstocksm %>%\n  gt(groupname_col = \"time\")\n\n\n\n\n\n  \n  \n    \n      name\n      value\n    \n  \n  \n    \n      2009-01-01\n    \n    X\n-0.6264538\n    Y\n3.1905616\n    Z\n1.9497162\n    \n      2009-01-02\n    \n    X\n0.1836433\n    Y\n0.6590155\n    Z\n2.9532988\n    \n      2009-01-03\n    \n    X\n-0.8356286\n    Y\n-1.6409368\n    Z\n2.3031254\n  \n  \n  \n\n\n\n\nYou can see the only difference is the rotation. So, theoretically, this can be implemented as grouping + rotating."
  },
  {
    "objectID": "post/gather-and-spread-explained-by-gt/index.html#do-it-yourself-by-enframe-and-deframe",
    "href": "post/gather-and-spread-explained-by-gt/index.html#do-it-yourself-by-enframe-and-deframe",
    "title": "gather() and spread() Explained By gt",
    "section": "Do it yourself by enframe() and deframe()",
    "text": "Do it yourself by enframe() and deframe()\nBefore entering into the implementations, I explain two tibble’s functions, enframe() and deframe() briefly. They can convert a vector to/from a two-column data.frame.\n\nlibrary(tibble)\n\nx <- 1:3\nnames(x) <- c(\"foo\", \"bar\", \"baz\")\n\nenframe(x)\n\n# A tibble: 3 × 2\n  name  value\n  <chr> <int>\n1 foo       1\n2 bar       2\n3 baz       3\n\n\n\ndeframe(enframe(x))\n\nfoo bar baz \n  1   2   3 \n\n\n\ngather()\nFirst, nest the data by time.\n\nd <- dplyr::group_nest(stocks, time)\nd\n\n# A tibble: 3 × 2\n  time                     data\n  <date>     <list<tibble[,3]>>\n1 2009-01-01            [1 × 3]\n2 2009-01-02            [1 × 3]\n3 2009-01-03            [1 × 3]\n\n\nThen, coerce the columns of the 1-row data.frames to vectors. (In practice, we should check if the elements are all coercible.)\n\nd$data <- purrr::map(d$data, ~ vctrs::vec_c(!!! .))\nd\n\n# A tibble: 3 × 2\n  time       data     \n  <date>     <list>   \n1 2009-01-01 <dbl [3]>\n2 2009-01-02 <dbl [3]>\n3 2009-01-03 <dbl [3]>\n\n\nLastly, enframe() the vectors and unnest the whole data.\n\nd$data <- purrr::map(d$data, enframe)\nd\n\n# A tibble: 3 × 2\n  time       data            \n  <date>     <list>          \n1 2009-01-01 <tibble [3 × 2]>\n2 2009-01-02 <tibble [3 × 2]>\n3 2009-01-03 <tibble [3 × 2]>\n\n\n\ntidyr::unnest(d)\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(data)`\n\n\n# A tibble: 9 × 3\n  time       name   value\n  <date>     <chr>  <dbl>\n1 2009-01-01 X     -0.626\n2 2009-01-01 Y      3.19 \n3 2009-01-01 Z      1.95 \n4 2009-01-02 X      0.184\n5 2009-01-02 Y      0.659\n6 2009-01-02 Z      2.95 \n7 2009-01-03 X     -0.836\n8 2009-01-03 Y     -1.64 \n9 2009-01-03 Z      2.30 \n\n\nDone.\n\n\nspread()\nFirst step is the same as gather(). Just nest the data by time.\n\nd <- dplyr::group_nest(stocksm, time)\nd\n\n# A tibble: 3 × 2\n  time                     data\n  <date>     <list<tibble[,2]>>\n1 2009-01-01            [3 × 2]\n2 2009-01-02            [3 × 2]\n3 2009-01-03            [3 × 2]\n\n\nThen, deframe() the data.frames. (In practice, we have to fill the missing rows to ensure all data.frames have the same variables.)\n\nd$data <- purrr::map(d$data, deframe)\nd\n\n# A tibble: 3 × 2\n  time       data     \n  <date>     <list>   \n1 2009-01-01 <dbl [3]>\n2 2009-01-02 <dbl [3]>\n3 2009-01-03 <dbl [3]>\n\n\nThen, convert the vectors to data.frames.\n\nd$data <- purrr::map(d$data, ~ tibble::tibble(!!! .))\nd\n\n# A tibble: 3 × 2\n  time       data            \n  <date>     <list>          \n1 2009-01-01 <tibble [1 × 3]>\n2 2009-01-02 <tibble [1 × 3]>\n3 2009-01-03 <tibble [1 × 3]>\n\n\nLastly, unnest the whole data.\n\ntidyr::unnest(d)\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(data)`\n\n\n# A tibble: 3 × 4\n  time            X      Y     Z\n  <date>      <dbl>  <dbl> <dbl>\n1 2009-01-01 -0.626  3.19   1.95\n2 2009-01-02  0.184  0.659  2.95\n3 2009-01-03 -0.836 -1.64   2.30\n\n\nDone."
  },
  {
    "objectID": "post/gather-and-spread-explained-by-gt/index.html#whats-next",
    "href": "post/gather-and-spread-explained-by-gt/index.html#whats-next",
    "title": "gather() and spread() Explained By gt",
    "section": "What’s next?",
    "text": "What’s next?\nI’m not sure… I roughly believe this can be extended to multi-gather and multi-spread (groups can have multiple vectors and data.frames), but I’m yet to see how different (or same) this is from the current tidyr’s semantics. Again, any feedbacks are welcome!"
  },
  {
    "objectID": "post/geom-sf-text-and-geom-sf-label-are-coming/index.html",
    "href": "post/geom-sf-text-and-geom-sf-label-are-coming/index.html",
    "title": "geom_sf_text() and geom_sf_label() Are Coming!",
    "section": "",
    "text": "ggplot2 v3.1.0 will be released soon (hopefully), so let me do a spoiler about a small feature I implemented, geom_sf_label() and geom_sf_text()."
  },
  {
    "objectID": "post/geom-sf-text-and-geom-sf-label-are-coming/index.html#how-can-we-add-labeltext-with-geom_sf",
    "href": "post/geom-sf-text-and-geom-sf-label-are-coming/index.html#how-can-we-add-labeltext-with-geom_sf",
    "title": "geom_sf_text() and geom_sf_label() Are Coming!",
    "section": "How can we add label/text with geom_sf()?",
    "text": "How can we add label/text with geom_sf()?\ngeom_sf() is one of the most exciting features introduced in ggplot2 v3.0.0. It magically allows us to plot sf objects according to their geometries’ shapes (polygons, lines and points).\nBut, for plotting them as some other shapes than the original ones, we cannot rely on geom_sf() so it needs a bit of data transformation beforehand. Suppose we want to add text on each geometry, we need to\n\ncalculate the proper point to add text/labels per geometry by some function like sf::st_centroid() and sf::st_point_on_surface(),\nretrieve the coordinates from the calculated points by sf::st_coordinates(), and\nuse geom_text() or geom_label() with the coordinates\n\nThe code for this would be like below:\n\nlibrary(ggplot2)\n\nnc <- sf::st_read(system.file(\"shape/nc.shp\", package = \"sf\"), quiet = TRUE)\n\n# use only first three elements\nnc3 <- nc[1:3, ]\n\n# choose a point on the surface of each geometry\nnc3_points <- sf::st_point_on_surface(nc3)\n\nWarning in st_point_on_surface.sf(nc3): st_point_on_surface assumes attributes\nare constant over geometries of x\n\n\nWarning in st_point_on_surface.sfc(st_geometry(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n# retrieve the coordinates\nnc3_coords <- as.data.frame(sf::st_coordinates(nc3_points))\nnc3_coords$NAME <- nc3$NAME\n\nnc3_coords\n\n          X        Y      NAME\n1 -81.49496 36.42112      Ashe\n2 -81.13241 36.47396 Alleghany\n3 -80.69280 36.38828     Surry\n\nggplot() +\n  geom_sf(data = nc3, aes(fill = AREA)) +\n  geom_text(data = nc3_coords, aes(X, Y, label = NAME), colour = \"white\")\n\n\n\n\nPhew, this seems not so difficult, but I feel the code is a bit too long…"
  },
  {
    "objectID": "post/geom-sf-text-and-geom-sf-label-are-coming/index.html#geom_sf_label-and-geom_sf_text",
    "href": "post/geom-sf-text-and-geom-sf-label-are-coming/index.html#geom_sf_label-and-geom_sf_text",
    "title": "geom_sf_text() and geom_sf_label() Are Coming!",
    "section": "geom_sf_label() and geom_sf_text()",
    "text": "geom_sf_label() and geom_sf_text()\nFor this purpose, upcoming ggplot2 v3.1.0 provides two new geoms, geom_sf_text() and geom_sf_label(). The code equivalent to above can be written as:\n\n# texts and labels\np <- ggplot(nc3) +\n  geom_sf(aes(fill = AREA))\n\np + geom_sf_text(aes(label = NAME), colour = \"white\")\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\nFor labels, use geom_sf_label():\n\np + geom_sf_label(aes(label = NAME))\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data"
  },
  {
    "objectID": "post/geom-sf-text-and-geom-sf-label-are-coming/index.html#protip-stat_sf_coordinates",
    "href": "post/geom-sf-text-and-geom-sf-label-are-coming/index.html#protip-stat_sf_coordinates",
    "title": "geom_sf_text() and geom_sf_label() Are Coming!",
    "section": "Protip: stat_sf_coordinates()",
    "text": "Protip: stat_sf_coordinates()\nUnder the hood, a Stat called stat_sf_coordinates() does the necessary calculations. If you are an expert of ggplot2, please play with this by combining with other Geoms. As an example, here’s a preliminary version of geom_sf_label_repel() (which I want to implement next…):\n\nggplot(nc) +\n  geom_sf() +\n  ggrepel::geom_label_repel(\n    data = nc[c(1:3, 10:14), ],\n    aes(label = NAME, geometry = geometry),\n    stat = \"sf_coordinates\",\n    min.segment.length = 0,\n    colour = \"magenta\",\n    segment.colour = \"magenta\"\n  )\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\nFor other cool NEWS of ggplot2 v3.1.0, please read the NEWS.md :)"
  },
  {
    "objectID": "post/gghighlight-0-2-0/index.html",
    "href": "post/gghighlight-0-2-0/index.html",
    "title": "gghighlight 0.2.0",
    "section": "",
    "text": "gghighlight 0.2.0 is on CRAN a while ago. This post briefly introduces the three new features. For basic usages, please refer to “Introduction to gghighlight”."
  },
  {
    "objectID": "post/gghighlight-0-2-0/index.html#keep_scales",
    "href": "post/gghighlight-0-2-0/index.html#keep_scales",
    "title": "gghighlight 0.2.0",
    "section": "keep_scales",
    "text": "keep_scales\nTo put it simply, gghighlight doesn’t drop any data points but drops their colours. This means, while non-colour scales (e.g. x, y and size) are kept as they are, colour scales get shrinked. This might be inconvenient when we want to compare the original version and the highlighted version, or the multiple highlighted versions.\n\nlibrary(gghighlight)\n\nLoading required package: ggplot2\n\nlibrary(patchwork)\n\nset.seed(3)\n\nd <- data.frame(\n  value = 1:9,\n  category = rep(c(\"a\",\"b\",\"c\"), 3),\n  cont_var = runif(9),\n  stringsAsFactors = FALSE\n)\n\np <- ggplot(d, aes(x = category, y = value, color = cont_var)) +\n  geom_point(size = 10) +\n  scale_colour_viridis_c()\n\np1 <- p + ggtitle(\"original\")\np2 <- p + \n  gghighlight(dplyr::between(cont_var, 0.3, 0.7),\n              use_direct_label = FALSE) +\n  ggtitle(\"highlighted\")\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\np1 * p2\n\n\n\n\nYou can see the colour of the points are different between the left plot and the right plot because the scale of the colours are different. In such a case, you can specify keep_scale = TRUE to keep the original scale (under the hood, gghighlight simply copies the original data to geom_blank()).\n\np3 <- p +\n  gghighlight(dplyr::between(cont_var, 0.3, 0.7),\n              keep_scales = TRUE,\n              use_direct_label = FALSE) +\n  ggtitle(\"highlighted (keep_scale = TRUE)\")\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\np1 * p3"
  },
  {
    "objectID": "post/gghighlight-0-2-0/index.html#calculate_per_facet",
    "href": "post/gghighlight-0-2-0/index.html#calculate_per_facet",
    "title": "gghighlight 0.2.0",
    "section": "calculate_per_facet",
    "text": "calculate_per_facet\nWhen used with facet_*(), gghighlight() puts unhighlighted data on all facets and calculate the predicates on the whole data.\n\nSys.setlocale(locale = \"C\")\n\n[1] \"C\"\n\nset.seed(16)\n\nd <- tibble::tibble(\n  day = rep(as.Date(\"2020-01-01\") + 0:89, times = 4),\n  month = lubridate::ceiling_date(day, \"month\"),\n  value = c(\n    cumsum(runif(90, -1.0, 1.0)),\n    cumsum(runif(90, -1.1, 1.1)),\n    cumsum(runif(90, -1.1, 1.0)),\n    cumsum(runif(90, -1.0, 1.1))\n  ),\n  id = rep(c(\"a\", \"b\", \"c\", \"d\"), each = 90)\n)\n\np <- ggplot(d) +\n  geom_line(aes(day, value, colour = id)) +\n  facet_wrap(~ month, scales = \"free_x\")\n\np + \n  gghighlight(mean(value) > 0, keep_scales = TRUE)\n\nlabel_key: id\n\n\n\n\n\nBut, it sometimes feels better to highlight facet by facet. For such a need, gghighlight() now has a new argument calculate_per_facet.\n\np + \n  gghighlight(mean(value) > 0,\n              calculate_per_facet = TRUE,\n              keep_scales = TRUE)\n\nlabel_key: id\n\n\n\n\n\nNote that, as a general rule, only the layers before adding gghighlight() are modified. So, if you add facet_*() after adding gghighlight(), this option doesn’t work (though this behaviour might also be useful in some cases).\n\nggplot(d) +\n  geom_line(aes(day, value, colour = id)) +\n  gghighlight(mean(value) > 0,\n              calculate_per_facet = TRUE,\n              keep_scales = TRUE) +\n  facet_wrap(~ month, scales = \"free_x\")\n\nlabel_key: id"
  },
  {
    "objectID": "post/gghighlight-0-2-0/index.html#unhighlighted_params",
    "href": "post/gghighlight-0-2-0/index.html#unhighlighted_params",
    "title": "gghighlight 0.2.0",
    "section": "unhighlighted_params",
    "text": "unhighlighted_params\ngghighlight() now allows users to override the parameters of unhighlighted data via unhighlighted_params. This idea was suggested by @ClausWilke.\n\n\nI think you could support a broader set of use cases if you allowed a list of aesthetics default values, like bleach_aes = list(colour = &quot;grey40&quot;, fill =&quot;grey80&quot;, size = 0.2).\n\n— Claus Wilke (@ClausWilke) July 4, 2018\n\n\nTo illustrate the original motivation, let’s use an example on the ggridges’ vignette. gghighlight can highlight almost any Geoms, but it doesn’t mean it can “unhighlight” arbitrary colour aesthetics automatically. In some cases, you need to unhighlight them manually. For example, geom_density_ridges() has point_colour.\n\nlibrary(ggplot2)\nlibrary(gghighlight)\nlibrary(ggridges)\n\np <- ggplot(Aus_athletes, aes(x = height, y = sport, color = sex, point_color = sex, fill = sex)) +\n  geom_density_ridges(\n    jittered_points = TRUE, scale = .95, rel_min_height = .01,\n    point_shape = \"|\", point_size = 3, size = 0.25,\n    position = position_points_jitter(height = 0)\n  ) +\n  scale_y_discrete(expand = c(0, 0)) +\n  scale_x_continuous(expand = c(0, 0), name = \"height [cm]\") +\n  scale_fill_manual(values = c(\"#D55E0050\", \"#0072B250\"), labels = c(\"female\", \"male\")) +\n  scale_color_manual(values = c(\"#D55E00\", \"#0072B2\"), guide = \"none\") +\n  scale_discrete_manual(\"point_color\", values = c(\"#D55E00\", \"#0072B2\"), guide = \"none\") +\n  coord_cartesian(clip = \"off\") +\n  guides(fill = guide_legend(\n    override.aes = list(\n      fill = c(\"#D55E00A0\", \"#0072B2A0\"),\n      color = NA, point_color = NA)\n    )\n  ) +\n  ggtitle(\"Height in Australian athletes\") +\n  theme_ridges(center = TRUE)\n\np + \n  gghighlight(sd(height) < 5.5)\n\nPicking joint bandwidth of 2.8\n\n\nPicking joint bandwidth of 2.23\n\n\n\n\n\nYou should notice that these vertical lines still have their colours. To grey them out, we can specify point_colour = \"grey80\" on unhighlighted_params (Be careful, point_color doesn’t work…).\n\np + \n  gghighlight(sd(height) < 5.5, \n              unhighlighted_params = list(point_colour = \"grey80\"))\n\nPicking joint bandwidth of 2.8\n\n\nPicking joint bandwidth of 2.23\n\n\n\n\n\nunhighlighted_params is also useful when you want more significant difference between the highlighted data and unhighligted ones. In the following example, size and colour are set differently.\n\nset.seed(2)\nd <- purrr::map_dfr(\n  letters,\n  ~ data.frame(\n      idx = 1:400,\n      value = cumsum(runif(400, -1, 1)),\n      type = .,\n      flag = sample(c(TRUE, FALSE), size = 400, replace = TRUE),\n      stringsAsFactors = FALSE\n    )\n)\n\nggplot(d) +\n  geom_line(aes(idx, value, colour = type), size = 5) +\n  gghighlight(max(value) > 19,\n              unhighlighted_params = list(size = 1, colour = alpha(\"pink\", 0.4)))\n\nsize aesthetic has been deprecated for use with lines as of ggplot2 3.4.0\ni Please use linewidth aesthetic instead\nlabel_key: type\n\nThis message is displayed once every 8 hours."
  },
  {
    "objectID": "post/how-to-convert-a-human-to-waves-by-magick-package/index.html",
    "href": "post/how-to-convert-a-human-to-waves-by-magick-package/index.html",
    "title": "How To Convert A Human To Waves By Magick Package",
    "section": "",
    "text": "I saw this tweet about Mathematica last year, which naturally urged me to write the R version of this code.\nAt that time, I faild because I didn’t know how to zoom images. But, now I know magick package. Let’s try again…"
  },
  {
    "objectID": "post/how-to-convert-a-human-to-waves-by-magick-package/index.html#zoom-images-by-magick",
    "href": "post/how-to-convert-a-human-to-waves-by-magick-package/index.html#zoom-images-by-magick",
    "title": "How To Convert A Human To Waves By Magick Package",
    "section": "Zoom images by magick",
    "text": "Zoom images by magick\nWe can enlarge a image by either image_resize(), image_scale(), or image_sample(). I don’t know about the details of the differences, but it seems image_resize() does better for my purpose.\n\nlibrary(magick)\n\nLinking to ImageMagick 6.9.12.3\nEnabled features: cairo, freetype, fftw, ghostscript, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fontconfig, x11\n\nrose <- image_convert(image_read(\"rose:\"), \"png\")\n\nroses <- c(\n    image_resize(rose, \"400x\"),\n    image_scale(rose, \"400x\"),\n    image_sample(rose, \"400x\")\n)\n\nimage_append(roses, stack = TRUE)\n\n\n\n\nBut, zooming is not just about resizing; I want to focus on the center of the image as well. To do this, we can use image_crop(). But, it’s our job to calculate the perper offset to centering the image (IIUC, there’s no equivalent of -gravity center in magick package, right??).\nSuppose we want to zoom by 200%. First of all, extract the original width and height.\n\ninfo <- image_info(rose)\norig_width <- info$width\norig_height <- info$height\n\nThen, let’s calculate the offset; to center the image, the offset should be half of the diffrence between the original size and the size you want.\n\nwidth <- as.integer(orig_width * 2)\nheight <- as.integer(orig_height * 2)\n\noffset_x <- as.integer((width - orig_width) / 2)\noffset_y <- as.integer((height - orig_height) / 2)\n\nNow we have enough information to crop the image. To provide these to Imagemagick, we need to learn a bit about Geometry syntax. It’s as simple as:\n<width>x<height>{+-}<xoffset>{+-}<yoffset>\nWe can construct the geometries by sprintf() (use %+d instead of %d when we need explicit plus sign) as follows:\n\n(g_resize <- sprintf(\"%dx%d\", width, height))\n\n[1] \"140x92\"\n\n(g_crop <- sprintf(\"%dx%d%+d%+d\", orig_width, orig_height, offset_x, offset_y))\n\n[1] \"70x46+35+23\"\n\n\nNow we can zoom\n\nrose_zoomed <- rose %>%\n  image_resize(g_resize) %>% \n  image_crop(g_crop)\n\nimage_append(c(rose, rose_zoomed), stack = TRUE)"
  },
  {
    "objectID": "post/how-to-convert-a-human-to-waves-by-magick-package/index.html#zoom-animatedly",
    "href": "post/how-to-convert-a-human-to-waves-by-magick-package/index.html#zoom-animatedly",
    "title": "How To Convert A Human To Waves By Magick Package",
    "section": "Zoom animatedly",
    "text": "Zoom animatedly\nNow that we know how to zoom, we can create a function to draw rose at the specified zoom level.\n\nzoom_rose <- function(zoom = 1) {\n  width <- as.integer(orig_width * zoom)\n  height <- as.integer(orig_height * zoom)\n  \n  offset_x <- as.integer((width - orig_width) / 2)\n  offset_y <- as.integer((height - orig_height) / 2)\n  \n  g_resize <- sprintf(\"%dx%d\", width, height)\n  g_crop <- sprintf(\"%dx%d%+d%+d\", orig_width, orig_height, offset_x, offset_y)\n\n  rose %>%\n    image_resize(g_resize) %>% \n    image_crop(g_crop)\n}\n\nzoom_rose(1)\n\n\n\nzoom_rose(2)\n\n\n\n\nThe function can be applied to the vector of zoom levels by lapply(). Note that, to make the zoom speed looks constant, we need to power the steps.\n\nsteps <- 100\nzooms <- 1 + 9 * (0:steps / steps)^2\nimgs <- lapply(zooms, zoom_rose)\n\nThe list of images can be combined by image_join() and then can be converted to an animation by image_animate()\n\nimgs %>%\n  image_join() %>%\n  image_animate(fps = 50)"
  },
  {
    "objectID": "post/how-to-convert-a-human-to-waves-by-magick-package/index.html#result",
    "href": "post/how-to-convert-a-human-to-waves-by-magick-package/index.html#result",
    "title": "How To Convert A Human To Waves By Magick Package",
    "section": "Result",
    "text": "Result\nAparently, there are a lot of things to explain (expecially about involute of a circle), but it would be a bit too long… Let’s jump to the final version of my code and the result :P\n\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(dplyr, warn.conflicts = FALSE)\n\n# the speed of involute increases uniformly; in order to make the lengths between\n# steps equal, we need to calculate the square root\nresolution_of_involute <- 10000L\nt <- sqrt(seq(0, resolution_of_involute) / resolution_of_involute)\n\ncoil_turns <- 17L\nmax_r <- coil_turns * 2 * pi\n\n# waviness of the coil\nwave_frequency <- 100\nwave_height_base <- pi\n\n# download and read the image\nimg_file <- tempfile(fileext = \".png\")\ndownload.file(\"https://hoxo-m.com/img/team/makiyama.png\", destfile = img_file, mode = \"wb\")\nimg_orig <- image_read(img_file)\n\n# convert to grayscale\nimg_bw <- img_orig %>% \n  image_convert(type = \"grayscale\") %>%\n  image_modulate(brightness = 160) %>% \n  image_contrast(10)\n\n# the width and height of the output\nw <- 320\nh <- 320\n\n# the width and height of the original image\ninfo <- image_info(img_bw)\nw_orig <- info$width\nh_orig <- info$height\n\n# the width and height of the zoomed image\nscale <- 30L\nw_big <- w_orig * scale\nh_big <- h_orig * scale\n\n# zoom image\nimg_bw_big <- image_resize(img_bw, sprintf(\"%dx%d\", w_big, h_big))\n\n# place the small image on the center of the big image\nimg <- image_composite(img_bw_big, img_bw,\n                       offset = sprintf(\"%+d%+d\",\n                                        as.integer((w_big - w_orig) / 2),\n                                        as.integer((h_big - h_orig) / 2)))\n\ndraw_hoxom <- function(rotation = 0, zoom = 1) {\n  # unwavy involute\n  d <- tibble(\n    radius = 2 * pi * t * coil_turns,\n    phi = radius - rotation,\n    .x = cos(phi) + radius * sin(phi),\n    .y = sin(phi) - radius * cos(phi)\n  )\n\n  # crop and resize the image at the specified zoom level\n  g <- sprintf(\"%dx%d%+d%+d\",\n               as.integer(w_big / zoom),\n               as.integer(h_big / zoom),\n               as.integer((w_big - w_big / zoom) / 2),\n               as.integer((h_big - h_big / zoom) / 2))\n  \n  blackness <- img %>%\n    image_crop(g) %>%\n    image_resize(sprintf(\"%dx%d\", w, h)) %>%\n    image_data(\"gray\")\n\n  # calculate which pixel each point falls in\n  x_idx <- as.integer(scales::rescale(d$.x, from = c(-max_r, max_r), to = c(1L, dim(blackness)[2])))\n  y_idx <- as.integer(scales::rescale(d$.y, from = c(-max_r, max_r), to = c(dim(blackness)[3], 1L)))\n  \n  # determine the wave height based on the blackness\n  wave_height <- (255 - as.numeric(blackness[cbind(1, x_idx, y_idx)])) / 256 * wave_height_base\n\n  # wavy involute  \n  d_wavy <- d %>% \n    mutate(\n      x = .x + wave_height * sin(phi * wave_frequency) * sin(phi),\n      y = .y - wave_height * sin(phi * wave_frequency) * cos(phi)\n    )\n  \n  p <- ggplot(d_wavy) +\n    geom_path(aes(x, y)) +\n    theme_minimal() +\n    coord_equal(\n      # 0.85 is for zoom\n      xlim = c(-max_r, max_r) * 0.85,\n      ylim = c(-max_r, max_r) * 0.85\n    ) +\n    theme_void()\n  \n  print(p)\n}\n\n\nimgs <- image_graph(w, h, res = 72)\n\nsteps <- 100\nfor (i in seq_len(steps)) {\n  draw_hoxom(2 * pi * i / steps, 1 + (scale - 1) / steps^2 * i^2)\n}\n\ndev.off()\n\npng \n  2 \n\nimage_animate(imgs, fps = 50)\n\n\n\n\nI’m grad I’ve finally proven that I can live without Mathematica!"
  },
  {
    "objectID": "post/quote-while-the-promise-is-hot/index.html",
    "href": "post/quote-while-the-promise-is-hot/index.html",
    "title": "Quote While the Promise Is Hot!",
    "section": "",
    "text": "quote_x_and_y <- function(x, y) {\n  if (is.null(x)) {\n    stop(\"x is NULL!\", call. = FALSE)\n  }\n  \n  x <- rlang::enquo(x)\n  y <- rlang::enquo(y)\n  \n  list(x, y)\n}\n\nx <- y <- 1\n\nquote_x_and_y(x, y)\n#> [[1]]\n#> <quosure>\n#>   expr: ^1\n#>   env:  empty\n#> \n#> [[2]]\n#> <quosure>\n#>   expr: ^y\n#>   env:  global\nThis is because x is evaluated when is.null() is called before quoting, whereas y is intact. Lionel Henry, the tidyeval super hero, answered my qustion on RStudio Community:\n\nA forced promise can no longer be captured correctly because it no longer carries an environment.\n\nThis means we must not touch arguments before quoting. Instead, quote first and check the expression inside quosure by rlang::quo_is_*().\nquote_x_and_y2 <- function(x, y) {\n  x <- rlang::enquo(x)\n  y <- rlang::enquo(y)\n  \n  if (rlang::quo_is_null(x)) {\n    stop(\"x is NULL!\", call. = FALSE)\n  }\n  \n  list(x, y)\n}\n\nquote_x_and_y2(x, y)\n#> [[1]]\n#> <quosure>\n#>   expr: ^x\n#>   env:  global\n#> \n#> [[2]]\n#> <quosure>\n#>   expr: ^y\n#>   env:  global\nFor more complex checking, we may need to extract the expression from the quosure by rlang::quo_get_expr().\nquote_x_and_y_wont_stop <- function(x, y) {\n  x <- rlang::enquo(x)\n  y <- rlang::enquo(y)\n\n  x_expr <- rlang::quo_get_expr(x)  \n  if (rlang::call_name(x) %in% \"stop\") {\n    message(\"Nothing can stop me!\\n\")\n  }\n  \n  list(x, y)\n}\n\nquote_x_and_y_wont_stop(stop(\"foo\"), \"bar\")\n#> Nothing can stop me!\n#> [[1]]\n#> <quosure>\n#>   expr: ^stop(\"foo\")\n#>   env:  global\n#> \n#> [[2]]\n#> <quosure>\n#>   expr: ^\"bar\"\n#>   env:  empty\nAnyway, keep in mind to use enquo() (or ensym()) at the very beginning of the function. Quote while the promise is hot."
  },
  {
    "objectID": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html",
    "href": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html",
    "title": "Some more notes about using Rust code in R packages",
    "section": "",
    "text": "When I first tried to use Rust code within R package five years ago, it was like crawling in the dark and I wasted several days just to find I didn’t understand anything. But, now we have Using Rust code in R packages, a great presentation by Jeroen Ooms. It taught me almost everything! But still, I needed to learn myself some more things for my purpose. Let me leave some notes about those."
  },
  {
    "objectID": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html#passing-a-string-from-r-to-rust",
    "href": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html#passing-a-string-from-r-to-rust",
    "title": "Some more notes about using Rust code in R packages",
    "section": "Passing a string from R to Rust",
    "text": "Passing a string from R to Rust\nhellorust covers how to pass a string from Rust to R, but not the vice versa. I learned this from the code on clauswilke/sinab.\nFor example, let’s consider an improved version of hellorust::hello() that takes an argument name to say hello to.\n\nR code\nLet’s name it hello2.\nhello2 <- function(name) {\n  .Call(hello_wrapper2, name)\n}\n\n\nC code\nhello_wrapper2 would be like the code below. STRING_ELT(x, i) takes i-th element of a character vector x, and Rf_translateCharUTF8() converts it to a pointer to the string encoded in UTF-8.\nSEXP hello_wrapper2(SEXP name){\n  char* res = string_from_rust2(Rf_translateCharUTF8(STRING_ELT(name, 0)));\n  return Rf_ScalarString(Rf_mkCharCE(res, CE_UTF8));\n}\n\n\napi.h\nThe string is passed as const char *.\nchar * string_from_rust2(const char *);\n\n\nRust code\nThe function takes the string as *const c_char. If we process the string in Rust code, we need to create a String. This is done by std::ffi::CStr::from_ptr(). CStr is a representation of a borrowed C string, and can be converted to String by to_string() or to_string_lossy(). Since this is an unsafe operation, it needs to be wrapped with unsafe.\nuse std;\nuse std::ffi::{CStr, CString};\nuse std::os::raw::c_char;\n\n// Utility function to convert c_char to string\nfn c_char_to_string(c: *const c_char) -> String {\n    unsafe { CStr::from_ptr(c).to_string_lossy().into_owned() }\n}\n\n#[no_mangle]\npub extern fn string_from_rust2(c_name: *const c_char) -> *const c_char {\n    let name = c_char_to_string(c_name);\n\n    let s = CString::new(format!(\"Hello {} !\", name)).unwrap();\n    let p = s.as_ptr();\n    std::mem::forget(s);\n    p\n}\n\n\nResult\nYou can view the diff here:\nhttps://github.com/r-rust/hellorust/commit/a42346c728a408fb1b2e6e7522082e19ec5b8a04"
  },
  {
    "objectID": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html#passing-a-vector-from-rust-to-r-or-vice-versa",
    "href": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html#passing-a-vector-from-rust-to-r-or-vice-versa",
    "title": "Some more notes about using Rust code in R packages",
    "section": "Passing a vector from Rust to R, or vice versa",
    "text": "Passing a vector from Rust to R, or vice versa\n(Update: this code is incomplete, please read the next section as well)\nIt took me some time to figure out how to handle arrays. I’m still not confident if I understand this correctly, but let me try to explain…\nWe cannot simply pass a variable length of vector to FFI because the length is not known. So, what we need to do is obvious; pass the data with the length at the same time. To do this, we need to define the same struct both in C and in Rust.\nSuppose we want to implement a function that takes one double vector and reverse it.\nIn api.h, let’s define a struct named Slice:\ntypedef struct\n{\n  double *data;  // since we want to process `REALSXP` here, the data type is `double`\n  uint32_t len;\n} Slice;\nand in Rust code define the same one. #[repr(C)] means “do what C does.” This is needed to match the alignment of the field with C.\nuse std::os::raw::{c_double, c_uint};\n\n#[repr(C)]\npub struct Slice {\n    data: *mut c_double,\n    len: c_uint,\n}\n\nR code\nThe R code is pretty simple.\nrev <- function(x) {\n  x <- as.double(x)\n  .Call(rev_wrapper, x)\n}\n\n\nC code\nWe need to allocate a REALSXP vector and copy the result into it.\nSEXP rev_wrapper(SEXP x){\n  Slice s = {REAL(x), Rf_length(x)};\n  Slice s_rev = rev_slice(s);\n\n  SEXP out = PROTECT(Rf_allocVector(REALSXP, s_rev.len));\n  for (int i = 0; i < s_rev.len; i++) {\n    SET_REAL_ELT(out, i, s_rev.data[i]);\n  }\n  UNPROTECT(1);\n\n  return out;\n}\n\n\nRust code\nTo convert the Slice into Rust’s slice, we can use std::slice::from_raw_parts_mut. This is unsafe operation, so it needs to be wrapped with unsafe.\nslice and vector can be converted into an unsafe pointer by as_mut_ptr().\n#[no_mangle]\npub extern fn rev_slice(s: Slice) -> Slice {\n    // convert from Slice to Rust slice\n    let s = unsafe { std::slice::from_raw_parts_mut(s.data, s.len as _) };\n\n    let mut v = s.to_vec();\n    v.reverse();\n    let len = v.len();\n\n    let v_ptr = v.as_mut_ptr();\n    std::mem::forget(v);\n\n    Slice {\n        data: v_ptr,\n        len: len as _,\n    }\n}\n\n\nResult\nYou can view the diff here:\nhttps://github.com/r-rust/hellorust/commit/e278d1541301ae18446bf1149a15d7aed868bd51"
  },
  {
    "objectID": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html#update-free-the-rust-allocated-memory",
    "href": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html#update-free-the-rust-allocated-memory",
    "title": "Some more notes about using Rust code in R packages",
    "section": "Update: free the Rust-allocated memory",
    "text": "Update: free the Rust-allocated memory\nThe code above works, but I noticed the memory is never freed. Yes, that’s because I forgot to free it. This was my nice lesson to learn that Rust is not always automatically saving me from doing silly things :P\nOf course we can free it, but it’s a bit tricky. Since Slice is allocated by Rust, it needs to be freed by Rust (c.f. How to return byte array from Rust function to FFI C? - help - The Rust Programming Language Forum). (IIUC, if the length is known in advance, it might be good idea to allocate on C’s side and pass it to the Rust, as the answer on the forum above suggests. rev() is the case, but let me explain the different one for now…)\n\nRust code\nLet’s define a Rust function to free the memory. Box::from_raw() constructs a Box, a pointer for heap allocation, from the raw pointer. After that, the raw pointer is owned by the box, which means it’s now Rust’s role to destruct it and free the memory.\n#[no_mangle]\npub extern \"C\" fn free_slice(s: Slice) {\n    // convert to Rust slice\n    let s = unsafe { std::slice::from_raw_parts_mut(s.data, s.len as _) };\n    let s = s.as_mut_ptr();\n    unsafe {\n        Box::from_raw(s);\n    }\n}\nI still don’t understand how to use Box properly, but it seems Sized structs can be handled simpler using Box in the argument: https://doc.rust-lang.org/std/boxed/index.html#memory-layout\n\n\nC code\nCall the function above from C to free the memory as soon as it’s no longer in use.\n// Need to include to use memcpy()\n#include <string.h>\n\n// ...snip...\n\nSEXP rev_wrapper(SEXP x){\n  Slice s = {REAL(x), Rf_length(x)};\n  Slice s_rev = rev_slice(s);\n\n  SEXP out = PROTECT(Rf_allocVector(REALSXP, s_rev.len));\n  memcpy(REAL(out), s_rev.data, s.len * sizeof(double));\n  free_slice(s_rev); // free!!!\n  UNPROTECT(1);\n\n  return out;\n}\n\n\nResult\nThe full diff is here:\nhttps://github.com/r-rust/hellorust/commit/97b3628b4a66eae9e25898a79ebf20fa59741063\n\n\nCan I do zero-copy?\nCopying memory to memory is not very cool, but it just works. I don’t know any nicer way yet. Apache Arrow seems a overkill for this simple usage, but will I need it in future…? Or flatbuffer? This seems a battle for another day, so I’ll stop here for now."
  },
  {
    "objectID": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html#precompiled-binary-for-windows",
    "href": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html#precompiled-binary-for-windows",
    "title": "Some more notes about using Rust code in R packages",
    "section": "Precompiled binary for Windows",
    "text": "Precompiled binary for Windows\nAs you might already notice, hellorust’s installation instruction for Windows is a bit long. But, do I really need to require the users to install cargo, just to compile my useless package? Now that we have GitHub Actions CI, maybe preparing a precompiled binary is a choice.\nHere’s the YAML I’m using to compile on windows runners and attach the binary on the releases (This creates a two separate releases for x86_64 and i686, which might be improved…).\non:\n  push:\n    tags:\n      - 'windows*'\n\nname: Build Windows\n\njobs:\n  build:\n    strategy:\n      matrix:\n        target:\n          - x86_64\n          - i686\n\n    name: build-${{ matrix.target }}-pc-windows-gnu\n\n    runs-on: windows-latest\n\n    steps:\n      - name: Checkout sources\n        uses: actions/checkout@v2\n\n      - name: Install stable toolchain\n        uses: actions-rs/toolchain@v1\n        with:\n          toolchain: stable\n          target: ${{ matrix.target }}-pc-windows-gnu\n          profile: minimal\n          default: true\n\n      - name: Run cargo build\n        uses: actions-rs/cargo@v1\n        with:\n          command: build\n          args: --release --target=${{ matrix.target }}-pc-windows-gnu --manifest-path=src/string2path/Cargo.toml\n\n      - name: List files\n        run: ls ./src/string2path/target/${{ matrix.target }}-pc-windows-gnu/release/\n        shell: bash\n\n      - name: Create Release\n        id: create_release\n        uses: actions/create-release@v1\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        with:\n          tag_name: ${{ github.ref }}-${{ matrix.target }}\n          release_name: Release ${{ github.ref }}-${{ matrix.target }}\n          draft: false\n          prerelease: true\n      - name: Upload Release Asset\n        id: upload-release-asset\n        uses: actions/upload-release-asset@v1\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        with:\n          upload_url: ${{ steps.create_release.outputs.upload_url }}\n          asset_path: ./src/string2path/target/${{ matrix.target }}-pc-windows-gnu/release/libstring2path.a\n          asset_name: libstring2path.a\n          asset_content_type: application/octet-stream\nIf there’s a precompiled binary, we can skip the compilation by tweaking Makevars.win like this:\nCRATE = string2path\n\n# Change this when created a new tag\nBASE_TAG = windows7\n\nTARGET = $(subst 64,x86_64,$(subst 32,i686,$(WIN)))\nLIBDIR = windows/$(TARGET)\nSTATLIB = $(LIBDIR)/lib$(CRATE).a\nPKG_LIBS = -L$(LIBDIR) -l$(CRATE) -lws2_32 -ladvapi32 -luserenv\n\nall: clean\n\n$(SHLIB): $(STATLIB)\n\n$(STATLIB):\n    mkdir -p $(LIBDIR)\n    # Not sure, but $@ doesn't seem to work here...\n    curl -L -o $(STATLIB) https://github.com/yutannihilation/$(CRATE)/releases/download/$(BASE_TAG)-$(TARGET)/lib$(CRATE).a\n\nclean:\n    rm -Rf $(SHLIB) $(STATLIB) $(OBJECTS)\nBy the way, at the time when hellorust was created, the extension of staticlib was .lib on Windows (MinGW), but recently (as of v1.44) this is changed to .a. Be careful."
  },
  {
    "objectID": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html#why-rust",
    "href": "post/some-more-notes-about-using-rust-code-in-r-packages/index.html#why-rust",
    "title": "Some more notes about using Rust code in R packages",
    "section": "Why Rust?",
    "text": "Why Rust?\nLastly, let me answer to what some of you might wonder. I know you want me to say something like “memory safe” or “fast,” but…, it was just I was more familiar with Rust than C/C++.\nI just happened to learn Rust. I was searching for some alternative of Processing, a great creative coding framework, and I found nannou. At first, I didn’t expect I needed to learn Rust seriously, as the framework wraps the things very nicely. But, since nannou is still maturing, I found I needed to dive a bit deeper into the world of Rust to make things work on my environment. I’m now learning wgpu, a Rust implementation of WebGPU. If you are interested in, here’s some resources:\n\nLearn WGPU\nA Taste of WebGPU in Firefox - Mozilla Hacks - the Web developer blog"
  }
]